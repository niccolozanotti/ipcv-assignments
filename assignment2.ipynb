{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "32852243"
   },
   "source": [
    "# Project members\n",
    "\n",
    "**Sali Raffaele**:\n",
    "- raffaele.sali@studio.unibo.it\n",
    "- 000\n",
    "\n",
    "**Zanotti Niccolò**:\n",
    "- niccolo.zanotti@studio.unibo.it\n",
    "- 000\n",
    "\n",
    "**Zocco Ramazzo Marco**:\n",
    "- marco.zoccoramazzo@studio.unibo.it\n",
    "- 000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "d41535fd"
   },
   "source": [
    "# Assignment Module 2: Pet Classification\n",
    "\n",
    "The goal of this assignment is to implement a neural network that classifies images of 37 breeds of cats and dogs from the [Oxford-IIIT-Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The assignment is divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "b1476550"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The following cells contain the code to download and access the dataset you will be using in this assignment. Note that, although this dataset features each and every image from [Oxford-IIIT-Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/), it uses a different train-val-test split than the original authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91101a0d",
    "outputId": "a9db6786-a66e-49d3-f158-d4c6e28e8faa"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/CVLAB-Unibo/ipcv-assignment-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "0d8fb0d2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "b99c9929"
   },
   "outputs": [],
   "source": [
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, split: str, transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = Path(\"ipcv-assignment-2\") / \"dataset\"\n",
    "        self.split = split\n",
    "        self.names, self.labels = self._get_names_and_labels()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
    "        img_path = self.root / \"images\" / f\"{self.names[idx]}.jpg\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return max(self.labels) + 1\n",
    "\n",
    "    def _get_names_and_labels(self) -> Tuple[List[str], List[int]]:\n",
    "        names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(self.root / \"annotations\" / f\"{self.split}.txt\") as f:\n",
    "            for line in f:\n",
    "                name, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                names.append(name),\n",
    "                labels.append(int(label) - 1)\n",
    "\n",
    "        return names, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XR1xyGG16n2l",
    "outputId": "6bc3b30b-fc4f-48d5-b7b1-ef6f0a04b9ab"
   },
   "outputs": [],
   "source": [
    "train_dataset1 = OxfordPetDataset(split=\"train\")\n",
    "print(len(train_dataset1))\n",
    "img, label = train_dataset1[0]\n",
    "print(img.size, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "ifMoNKZErJeP"
   },
   "outputs": [],
   "source": [
    "def breed_from_name(name):\n",
    "    return \"_\".join(name.split(\"_\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "emkFPsc9KCvX",
    "outputId": "9f6f371a-0dc4-4b1a-f607-a4a840c822c3"
   },
   "outputs": [],
   "source": [
    "# Images if we don't apply transformations\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(6):\n",
    "    img, _ = train_dataset1[i]\n",
    "    name = train_dataset1.names[i]\n",
    "\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(breed_from_name(name))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BstT-9u9KO-5",
    "outputId": "4e483ff0-6cb2-4a11-ea15-9815cd9db341"
   },
   "outputs": [],
   "source": [
    "# Images if we apply transformations\n",
    "import torchvision.transforms as T\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomCrop(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = OxfordPetDataset(split=\"train\", transform=train_transform)\n",
    "test_dataset = OxfordPetDataset(split=\"test\", transform=val_transform)\n",
    "validation_dataset = OxfordPetDataset(split=\"val\", transform=val_transform)\n",
    "print(\"Number of samples - train:\", len(train_dataset))\n",
    "print(\"Number of classes - train:\", train_dataset.get_num_classes())\n",
    "print(\"Number of samples - test:\", len(test_dataset))\n",
    "print(\"Number of classes - test:\", test_dataset.get_num_classes())\n",
    "print(\"Number of samples - validation:\", len(validation_dataset))\n",
    "print(\"Number of classes - validation:\", validation_dataset.get_num_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "N32ErS8zLLuz",
    "outputId": "7715abf1-3914-401a-a408-0cf1979ccbad"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR)\n",
    "\n",
    "def show_samples(dataset, n=6):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(n):\n",
    "        img, _ = dataset[i]\n",
    "        names = dataset.names[i]\n",
    "        # label = dataset.labels[i]\n",
    "        img = img.permute(1, 2, 0)\n",
    "\n",
    "        plt.subplot(2, n//2, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{breed_from_name(names)}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "WDJiLukJWBmO"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xz1YpyeRYSEZ",
    "outputId": "a8723cdc-9dc2-4b28-dd82-dfb4af163302"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(images.min(), images.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "b4e655bd"
   },
   "source": [
    "## Part 1: design your own network\n",
    "\n",
    "Your goal is to implement a convolutional neural network for image classification and train it from scratch on `OxfordPetDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~60%. You are free to achieve this however you want, except for a few rules you must follow:\n",
    "\n",
    "- Compile this notebook by displaying the results obtained by the best model you found throughout your experimentation; then show how, by removing some of its components, its performance drops. In other words, do an *ablation study* to prove that your design choices have a positive impact on the final result.\n",
    "\n",
    "- Do not instantiate an off-the-self PyTorch network. Instead, construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you cannot use e.g. `torchvision.models.alexnet`.\n",
    "\n",
    "- Show your results and ablations with plots, tables, images, etc. — the clearer, the better.\n",
    "\n",
    "Don't be too concerned with your model performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded more points than a poorly experimentally validated model with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "ZOmvZnRXe8zd"
   },
   "source": [
    "## **NOTE:**\n",
    "- Several strategies and network architectures were explored, with the main objective of exploiting the components and architectures presented during the course. More advanced networks, such as EfficientNet, were also developed, achieving results very close to those of a simpler ResNet-inspired network. For this reason, the network that better leverages the course topics was ultimately chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "pplftQBVy43G"
   },
   "source": [
    "## **Common Pipeline**\n",
    "\n",
    "**Includes:**\n",
    "- **Accuracy function:** takes into account different output structure when MixUp is employed or not\n",
    "- **Evaluation function**\n",
    "- **Residual block class:** defines the structure of the residual blocks, taking into account the presence or not of the Batch Normalization layers\n",
    "- **Convolutional block class:** defines the structure of the convolutional blocks, which are employed when trying to evidence the impact of Residual blocks\n",
    "- **Network class:** defines the structure of the CNN, taking into account the use of Stem layers, Batch Normalization layers, Residual layers, Pooling layers and Dropout, to measure the impact of these components\n",
    "- **TrainConfig class:** defines some core parameters for the training process (like number of epochs, starting learning rate, MixUp parameter, label smoothing factor) and takes into account the use of Label Smoothing, Learning Rate Scheduler and MixUp data augmentation technique\n",
    "- **build_training_components function:** defines the structure of criterion, optimizer, scheduler and mixup according to boolean and values set in the TrainConfig class\n",
    "- **Train process function:** defines the pipeline for training the model, minimizing the loss function and updating weights, storing the model with the highest accuracy obtained in validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "m1VDS3wNURdy"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    preds = outputs.argmax(dim=1)\n",
    "\n",
    "    # If labels are one-hot / soft (MixUp case)\n",
    "    if labels.ndim == 2:\n",
    "        labels = labels.argmax(dim=1)\n",
    "\n",
    "    return (preds == labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "O_MbfrYVohUm"
   },
   "outputs": [],
   "source": [
    "def compute_precision_recall_f1(preds, labels, num_classes):\n",
    "    eps = 1e-8\n",
    "\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        tp = ((preds == cls) & (labels == cls)).sum().float()\n",
    "        fp = ((preds == cls) & (labels != cls)).sum().float()\n",
    "        fn = ((preds != cls) & (labels == cls)).sum().float()\n",
    "\n",
    "        precision = tp / (tp + fp + eps)\n",
    "        recall = tp / (tp + fn + eps)\n",
    "        f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    return (\n",
    "        torch.mean(torch.stack(precision_list)).item(),\n",
    "        torch.mean(torch.stack(recall_list)).item(),\n",
    "        torch.mean(torch.stack(f1_list)).item(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "Ay6XhTZ1URdz"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            # Handle one-hot / soft labels\n",
    "            if labels.ndim == 2:\n",
    "                labels_hard = labels.argmax(dim=1)\n",
    "            else:\n",
    "                labels_hard = labels\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += accuracy(outputs, labels)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels_hard)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct.float() / total_samples\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    precision, recall, f1 = compute_precision_recall_f1(\n",
    "        all_preds,\n",
    "        all_labels,\n",
    "        num_classes=outputs.size(1)\n",
    "    )\n",
    "\n",
    "    return avg_loss, avg_acc.item(), precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "7xXNM7hMIzo1"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_batchnorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        norm = nn.BatchNorm2d if use_batchnorm else nn.Identity\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = norm(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels,\n",
    "            kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = norm(out_channels)\n",
    "\n",
    "        # Skip connection adjustment if shape changes\n",
    "        self.skip = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels,\n",
    "                    kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                norm(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.skip is not None:\n",
    "            identity = self.skip(identity)\n",
    "\n",
    "        out += identity\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "ih1tQX4Xf-aJ"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "zsjgN192DYUA"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes,\n",
    "        use_stem=True,\n",
    "        use_residuals=True,\n",
    "        use_batchnorm=True,\n",
    "        use_pooling=True,\n",
    "        use_dropout=True\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_residuals = use_residuals\n",
    "\n",
    "        # Stem layers\n",
    "        if use_stem:\n",
    "          self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "          )\n",
    "        else:\n",
    "          # Minimal stem: only channel lifting, no downsampling or pooling\n",
    "          self.stem = nn.Sequential(\n",
    "              nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "              nn.BatchNorm2d(64) if use_batchnorm else nn.Identity(),\n",
    "              nn.ReLU()\n",
    "          )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.stage1 = self._make_stage(64, 64, num_blocks=2, stride=1, use_batchnorm=use_batchnorm)\n",
    "        self.stage2 = self._make_stage(64, 128, num_blocks=2, stride=2, use_batchnorm=use_batchnorm)\n",
    "        self.stage3 = self._make_stage(128, 256, num_blocks=2, stride=2, use_batchnorm=use_batchnorm)\n",
    "        self.stage4 = self._make_stage(256, 512, num_blocks=2, stride=2, use_batchnorm=use_batchnorm)\n",
    "\n",
    "        # Classifier and Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1)) if use_pooling else None\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2) if use_dropout else nn.Identity(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def _make_stage(self, in_channels, out_channels, num_blocks, stride, use_batchnorm):\n",
    "        if not self.use_residuals:\n",
    "            # Plain conv blocks instead of residual blocks\n",
    "            layers = [ConvBlock(in_channels, out_channels, stride, use_batchnorm)]\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(ConvBlock(out_channels, out_channels, 1, use_batchnorm))\n",
    "            return nn.Sequential(*layers)\n",
    "        else:\n",
    "          layers = [ResidualBlock(in_channels, out_channels, stride, use_batchnorm)]\n",
    "          for _ in range(1, num_blocks):\n",
    "              layers.append(ResidualBlock(out_channels, out_channels, use_batchnorm=use_batchnorm))\n",
    "          return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        if self.pool is not None:\n",
    "          x = self.pool(x)\n",
    "        else:\n",
    "          x = x.mean(dim=(2, 3), keepdim=True)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "DwIJFDnggkHm"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    num_epochs: int = 100\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-2\n",
    "\n",
    "    use_scheduler: bool = False\n",
    "    use_label_smoothing: bool = False\n",
    "    label_smoothing: float = 0.1\n",
    "\n",
    "    use_mixup: bool = False\n",
    "    mixup_alpha: float = 0.2\n",
    "\n",
    "    save_path : str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "yxmzP0_JfKXp"
   },
   "outputs": [],
   "source": [
    "class NoMixUp:\n",
    "  def __call__(self, x, y):\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "tEpb3xtwfUNa"
   },
   "outputs": [],
   "source": [
    "def build_training_components(model, train_loader, train_dataset, config: TrainConfig):\n",
    "  criterion = nn.CrossEntropyLoss(\n",
    "        label_smoothing=config.label_smoothing\n",
    "        if config.use_label_smoothing\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "  optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "\n",
    "  scheduler = None\n",
    "  if config.use_scheduler:\n",
    "      scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "          optimizer,\n",
    "          max_lr=config.lr,\n",
    "          epochs=config.num_epochs,\n",
    "          steps_per_epoch=len(train_loader),\n",
    "          pct_start=0.1,\n",
    "          anneal_strategy=\"cos\",\n",
    "      )\n",
    "\n",
    "  if config.use_mixup:\n",
    "      mixup = v2.MixUp(\n",
    "          alpha=config.mixup_alpha,\n",
    "          num_classes=train_dataset.get_num_classes(),\n",
    "      )\n",
    "  else:\n",
    "      mixup = NoMixUp()\n",
    "\n",
    "  return criterion, optimizer, scheduler, mixup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "UQ3b98vlDhRf"
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    scheduler=None,\n",
    "    mixup=None,\n",
    "    grad_clip=1.0,\n",
    "    save_path=None,\n",
    "):\n",
    "  best_val_acc = 0.0\n",
    "  history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_precision\": [],\n",
    "    \"val_recall\": [],\n",
    "    \"val_f1\": [],\n",
    "    \"lr\": [],\n",
    "  }\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      model.train()\n",
    "\n",
    "      total_loss = 0.0\n",
    "      total_correct = 0\n",
    "      total_samples = 0\n",
    "\n",
    "      for images, labels in train_loader:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          # Apply MixUp augmentation\n",
    "          images, labels = mixup(images, labels)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          loss.backward()\n",
    "\n",
    "          # Gradient clipping to prevent exploding gradients\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "          optimizer.step()\n",
    "          if scheduler is not None:\n",
    "            scheduler.step()  # OneCycleLR steps per batch\n",
    "\n",
    "          total_loss += loss.item() * images.size(0)\n",
    "\n",
    "          total_correct += accuracy(outputs, labels)\n",
    "          total_samples += images.size(0)\n",
    "\n",
    "      train_loss = total_loss / total_samples\n",
    "      train_acc = total_correct.float() / total_samples\n",
    "\n",
    "      val_loss, val_acc, val_precision, val_recall, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "      # Track metrics\n",
    "      history[\"train_loss\"].append(train_loss)\n",
    "      history[\"val_loss\"].append(val_loss)\n",
    "      history[\"train_acc\"].append(train_acc.item())\n",
    "      history[\"val_acc\"].append(val_acc)\n",
    "      history[\"val_precision\"].append(val_precision)\n",
    "      history[\"val_recall\"].append(val_recall)\n",
    "      history[\"val_f1\"].append(val_f1)\n",
    "      history[\"lr\"].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "      if save_path is not None and val_acc > best_val_acc:\n",
    "          best_val_acc = val_acc\n",
    "          torch.save(model.state_dict(), save_path)\n",
    "\n",
    "      print(\n",
    "          f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "          f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f} | \"\n",
    "          f\"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f} | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "      )\n",
    "\n",
    "  print(f\"\\nBest Validation Accuracy: {best_val_acc:.3f}\")\n",
    "  return best_val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "7fG4AXRd76yB"
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_acc\"])\n",
    "    plt.plot(history[\"val_acc\"])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.legend([\"Train\", \"Validation\"])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_loss\"])\n",
    "    plt.plot(history[\"val_loss\"])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend([\"Train\", \"Validation\"])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_rate(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"lr\"])\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"ablation_test_results.csv\"\n",
    "JSON_PATH = \"ablation_histories.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous CSV results if available\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df_results = pd.read_csv(CSV_PATH)\n",
    "    results = df_results.to_dict(orient=\"records\")\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "# Load previous histories if available\n",
    "if os.path.exists(JSON_PATH):\n",
    "    with open(JSON_PATH, \"r\") as f:\n",
    "        all_histories = json.load(f)\n",
    "else:\n",
    "    all_histories = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "LZZEX5bf-yo3"
   },
   "source": [
    "## **Model variant: Full model (baseline)**\n",
    "\n",
    "**Architecture**\n",
    "- Convolutional neural network implementing architectures and strategies faced during the course.\n",
    "- A convolutional stem composed of a three 3×3 convolution (1st with stride=2 and others with stride=1), followed by Batch Normalization, ReLU activation, and max pooling.\n",
    "- Four sequential stages operating at increasing feature dimensions (64 → 128 → 256 → 512).\n",
    "- Each stage consists of two residual blocks with identity skip connections.\n",
    "- When spatial resolution or channel dimensions change, skip connections are adapted using a 1×1 convolution followed by Batch Normalization.\n",
    "- Global feature aggregation is performed using adaptive average pooling.\n",
    "- The classifier head consists of two fully connected layers with ReLU activation and dropout.\n",
    "\n",
    "**Normalization and regularization**\n",
    "- Batch Normalization is applied after every convolution, including within residual branches and skip connections.\n",
    "- Dropout is applied in the classifier to reduce overfitting.\n",
    "\n",
    "**Training setup**\n",
    "- Optimized using AdamW with weight decay.\n",
    "- Learning rate scheduling is enabled via OneCycleLR with cosine annealing.\n",
    "- Cross-entropy loss with label smoothing is used.\n",
    "- MixUp data augmentation is applied during training.\n",
    "- Gradient norm clipping is used to improve training stability.\n",
    "\n",
    "**Purpose**\n",
    "- This configuration serves as the baseline model against which all ablation studies are compared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "6SYwWCuEIUSf"
   },
   "source": [
    "**NOTE:** All ablation variants modify a single component at a time while keeping the remaining architecture and training configuration identical to the baseline model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4dbgOCyJDBNH",
    "outputId": "f0802c86-10d9-4461-9c73-33f01a24ce50"
   },
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "dot_code = r\"\"\"\n",
    "digraph ImprovedNet {\n",
    "    rankdir=TB;\n",
    "    fontname=\"Helvetica\";\n",
    "    node [shape=record, fontname=\"Helvetica\"];\n",
    "\n",
    "    Input [\n",
    "        label=\"Input\\n3×224×224\"\n",
    "    ];\n",
    "\n",
    "    Stem [\n",
    "        label=\"{Stem|\n",
    "        Conv3×3, s=2, p=1|\n",
    "        BN + ReLU|\n",
    "        Conv3×3, s=1, p=1|\n",
    "        BN + ReLU|\n",
    "        Conv3×3, s=1, p=1|\n",
    "        BN + ReLU|\n",
    "        Output: 64×112×112}\"\n",
    "    ];\n",
    "\n",
    "    Stage1 [\n",
    "        label=\"{Stage 1:\n",
    "        2×ResidualBlock|\n",
    "        RB1:\n",
    "        Conv3×3 → BN → ReLU +\n",
    "        Conv3×3 → BN\n",
    "        + Identity|\n",
    "        RB2:\n",
    "        Conv3×3 → BN → ReLU +\n",
    "        Conv3×3 → BN\n",
    "        + Identity|\n",
    "        Output: 64x56x56}\"\n",
    "    ];\n",
    "\n",
    "    Stage2 [\n",
    "        label=\"{Stage 2:\n",
    "        2×ResidualBlock|\n",
    "        RB1:\n",
    "        Conv3×3, s=2 + BN + ReLU +\n",
    "        Conv3×3 + BN\n",
    "        + Skip Conv1×1, s=2 + BN|\n",
    "        Output: 128x28x28|\n",
    "        RB2:\n",
    "        Conv3×3 + BN + ReLU +\n",
    "        Conv3×3 + BN|\n",
    "        Output: 128x28x28}\"\n",
    "    ];\n",
    "\n",
    "    Stage3 [\n",
    "        label=\"{Stage 3:\n",
    "        2×ResidualBlock|\n",
    "        RB1:\n",
    "        Conv3×3, s=2 + BN + ReLU +\n",
    "        Conv3×3 + BN\n",
    "        + Skip Conv1×1, s=2 + BN|\n",
    "        Output: 256x14x14|\n",
    "        RB2:\n",
    "        Conv3×3 + BN + ReLU +\n",
    "        Conv3×3 + BN|\n",
    "        Output: 256x14x14}\"\n",
    "    ];\n",
    "\n",
    "    Stage4 [\n",
    "        label=\"{Stage 4:\n",
    "        2×ResidualBlock|\n",
    "        RB1:\n",
    "        Conv3×3, s=2 + BN + ReLU +\n",
    "        Conv3×3 + BN\n",
    "        + Skip Conv1×1, s=2 + BN|\n",
    "        Output: 512x7x7|\n",
    "        RB2:\n",
    "        Conv3×3 + BN + ReLU +\n",
    "        Conv3×3 + BN|\n",
    "        Output: 512x7x7}\"\n",
    "    ];\n",
    "\n",
    "    Pool [\n",
    "        label=\"AdaptiveAvgPool\\n512×1×1\"\n",
    "    ];\n",
    "\n",
    "    FC [\n",
    "        label=\"{Classifier|\n",
    "        Linear 512→256 + ReLU|\n",
    "        Dropout p=0.2|\n",
    "        Linear 256→N classes}\"\n",
    "    ];\n",
    "\n",
    "    Output [\n",
    "        label=\"Output\\nN classes\"\n",
    "    ];\n",
    "\n",
    "    Input -> Stem -> Stage1 -> Stage2 -> Stage3 -> Stage4 -> Pool -> FC -> Output;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "Source(dot_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdixFoK0DhRd",
    "outputId": "60725686-f0bc-4fab-c599-d48aa6e00b00"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_1 = Net(n_classes=train_dataset.get_num_classes()).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_1.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_1.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFNcunWRL-vc",
    "outputId": "4c532949-6dac-449a-d3c6-778c8c9d70f1"
   },
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_full_net.pth\",\n",
    ")\n",
    "\n",
    "criterion, optimizer, scheduler, mixup = build_training_components(\n",
    "    model = Model_1,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "best_acc, history = train_model(\n",
    "    model=Model_1,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    mixup=mixup,\n",
    "    device=device,\n",
    "    num_epochs=config.num_epochs,\n",
    "    save_path=config.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqjGAzRzDhRf",
    "outputId": "6593f8e6-d906-4fdb-f753-eee42739e060"
   },
   "outputs": [],
   "source": [
    "best_model1 = Model_1\n",
    "\n",
    "best_model1.load_state_dict(\n",
    "    torch.load(config.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model1.to(device)\n",
    "\n",
    "test_loss, test_acc, test_prec, test_recall, test_f1 = evaluate(best_model1, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "a9mCvS_lF2qG"
   },
   "outputs": [],
   "source": [
    "model_name = \"Baseline\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name]\n",
    "\n",
    "# Append new results\n",
    "results.append({\n",
    "    \"Model\": model_name,\n",
    "    \"Test Loss\": test_loss,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test Precision\": test_prec,\n",
    "    \"Test Recall\": test_recall,\n",
    "    \"Test F1\": test_f1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "Y-Tb7nNTqwWy"
   },
   "outputs": [],
   "source": [
    "# Update history\n",
    "all_histories[model_name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aOeuU2EQEyw7",
    "outputId": "74b3175e-7a24-47e1-d294-163253b73d7a"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)\n",
    "plot_loss(history)\n",
    "plot_learning_rate(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "i4Um0-dky8PE"
   },
   "source": [
    "## **Model variant: No Batch Normalization**\n",
    "\n",
    "**Change**\n",
    "- Removed all Batch Normalization layers from the network, including those in residual skip connections.\n",
    "\n",
    "**Purpose**\n",
    "- To assess the contribution of Batch Normalization to training stability and final performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXoAIDAcIyJS",
    "outputId": "28cc7d57-2edb-4ccf-c265-e75d94844942"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_2 = Net(n_classes=train_dataset.get_num_classes(), use_batchnorm=False).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_2.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_2.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbLP7OorIyJT",
    "outputId": "3fc87ddf-88c6-4451-f336-dec61a50ffe8"
   },
   "outputs": [],
   "source": [
    "config2 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_noBN_net.pth\",\n",
    ")\n",
    "\n",
    "criterion2, optimizer2, scheduler2, mixup2 = build_training_components(\n",
    "    model = Model_2,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config2,\n",
    ")\n",
    "\n",
    "best_acc2, history2 = train_model(\n",
    "    model=Model_2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion2,\n",
    "    optimizer=optimizer2,\n",
    "    scheduler=scheduler2,\n",
    "    mixup=mixup2,\n",
    "    device=device,\n",
    "    num_epochs=config2.num_epochs,\n",
    "    save_path=config2.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppZVwFODIyJU",
    "outputId": "c7903e7e-13c8-41c2-ff13-e9250e4fc20a"
   },
   "outputs": [],
   "source": [
    "best_model2 = Model_2\n",
    "\n",
    "best_model2.load_state_dict(\n",
    "    torch.load(config2.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model2.to(device)\n",
    "\n",
    "test_loss2, test_acc2, test_prec2, test_recall2, test_f12 = evaluate(best_model2, test_loader, criterion2, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss2:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "ktuufiNEF-U_"
   },
   "outputs": [],
   "source": [
    "model_name2 = \"No BatchNorm\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name2]\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name2,\n",
    "    \"Test Loss\": test_loss2,\n",
    "    \"Test Accuracy\": test_acc2,\n",
    "    \"Test Precision\": test_prec2,\n",
    "    \"Test Recall\": test_recall2,\n",
    "    \"Test F1\": test_f12\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "pJ1At0igrHdq"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name2] = history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N_96-2tKE0rN",
    "outputId": "521a7076-0d97-46f9-dbde-718aff704777"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history2)\n",
    "plot_loss(history2)\n",
    "plot_learning_rate(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "zh5O8p4gzRw4"
   },
   "source": [
    "## **Model variant: No Pooling**\n",
    "\n",
    "**Change**\n",
    "- Removed all pooling operations from the network, including max pooling in the stem and global average pooling before the classifier.\n",
    "\n",
    "**Purpose**\n",
    "- To evaluate the role of spatial downsampling and global feature aggregation in representation learning and classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwpxNT0qI0tS",
    "outputId": "f52df25e-59b1-47a7-a807-6a55568b51ab"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_3 = Net(n_classes=train_dataset.get_num_classes(), use_pooling=False).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_3.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_3.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPjkYW48I0tT",
    "outputId": "37ffb2ac-1b5e-47de-e8e1-a6d66625fba1"
   },
   "outputs": [],
   "source": [
    "config3 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_noPOOL_net.pth\",\n",
    ")\n",
    "\n",
    "criterion3, optimizer3, scheduler3, mixup3 = build_training_components(\n",
    "    model = Model_3,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config3,\n",
    ")\n",
    "\n",
    "best_acc3, history3 = train_model(\n",
    "    model=Model_3,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion3,\n",
    "    optimizer=optimizer3,\n",
    "    scheduler=scheduler3,\n",
    "    mixup=mixup3,\n",
    "    device=device,\n",
    "    num_epochs=config3.num_epochs,\n",
    "    save_path=config3.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xZEx_TaI0tU",
    "outputId": "3c1e9bee-1cc1-4feb-ee02-b07aebb70681"
   },
   "outputs": [],
   "source": [
    "best_model3 = Model_3\n",
    "\n",
    "best_model3.load_state_dict(\n",
    "    torch.load(config3.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model3.to(device)\n",
    "\n",
    "test_loss3, test_acc3, test_prec3, test_recall3, test_f13 = evaluate(best_model3, test_loader, criterion3, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss3:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc3:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "id": "Swt3DVBMGEk_"
   },
   "outputs": [],
   "source": [
    "model_name3 = \"No Pooling\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name3]\n",
    "\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name3,\n",
    "    \"Test Loss\": test_loss3,\n",
    "    \"Test Accuracy\": test_acc3,\n",
    "    \"Test Precision\": test_prec3,\n",
    "    \"Test Recall\": test_recall3,\n",
    "    \"Test F1\": test_f13\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "2TUUJtzArNNK"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name3] = history3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ISPCsrHWE3Yf",
    "outputId": "68f1c309-bb4f-4b7d-e086-1cb456f6a7a6"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history3)\n",
    "plot_loss(history3)\n",
    "plot_learning_rate(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "AXFXA796Dl8a"
   },
   "source": [
    "## **Model variant: No Dropout**\n",
    "\n",
    "**Change**\n",
    "- Removed dropout from the classifier head while keeping the rest of the architecture unchanged.\n",
    "\n",
    "**Purpose**\n",
    "- To assess the impact of dropout-based regularization on overfitting and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwUx-nGdI1j2",
    "outputId": "cf714d34-2ecc-439e-f342-b0963d621ae1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_4 = Net(n_classes=train_dataset.get_num_classes(), use_dropout=False).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_4.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_4.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOKpIyLiI1j3",
    "outputId": "222ec135-6ba8-436c-f45f-cee599d8140e"
   },
   "outputs": [],
   "source": [
    "config4 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_noDOUT_net.pth\",\n",
    ")\n",
    "\n",
    "criterion4, optimizer4, scheduler4, mixup4 = build_training_components(\n",
    "    model = Model_4,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config4,\n",
    ")\n",
    "\n",
    "best_acc4, history4 = train_model(\n",
    "    model=Model_4,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion4,\n",
    "    optimizer=optimizer4,\n",
    "    scheduler=scheduler4,\n",
    "    mixup=mixup4,\n",
    "    device=device,\n",
    "    num_epochs=config4.num_epochs,\n",
    "    save_path=config4.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BqPhJcZI1j4",
    "outputId": "ec47fd4a-8222-4ade-f8cd-db97a5f9a785"
   },
   "outputs": [],
   "source": [
    "best_model4 = Model_4\n",
    "\n",
    "best_model4.load_state_dict(\n",
    "    torch.load(config4.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model4.to(device)\n",
    "\n",
    "test_loss4, test_acc4, test_prec4, test_recall4, test_f14 = evaluate(best_model4, test_loader, criterion4, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss4:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc4:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "id": "BlHr8SDxGOMm"
   },
   "outputs": [],
   "source": [
    "model_name4 = \"No Dropout\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name4]\n",
    "\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name4,\n",
    "    \"Test Loss\": test_loss4,\n",
    "    \"Test Accuracy\": test_acc4,\n",
    "    \"Test Precision\": test_prec4,\n",
    "    \"Test Recall\": test_recall4,\n",
    "    \"Test F1\": test_f14\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "id": "Awg2Db1UrTtE"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name4] = history4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "22cpQX-RFiBx",
    "outputId": "abbb50b7-9f7f-49c9-84be-11d22bee3476"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history4)\n",
    "plot_loss(history4)\n",
    "plot_learning_rate(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "id": "Z3_-LcAoy-lC"
   },
   "source": [
    "## **Model variant: No MixUp**\n",
    "\n",
    "**Change**\n",
    "- Disabled MixUp data augmentation during training, using only standard input–label pairs.\n",
    "\n",
    "**Purpose**\n",
    "- To measure the effect of MixUp on model robustness and generalization compared to standard supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPchXB_3JGZW",
    "outputId": "adc8d08d-438e-457e-ab61-453583e43bc0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_5 = Net(n_classes=train_dataset.get_num_classes()).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_5.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_5.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHYX_ddgJGZX",
    "outputId": "0d8de295-262d-45e1-998a-7316ee99147d"
   },
   "outputs": [],
   "source": [
    "config5 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=False,\n",
    "    save_path=\"best_noMIXUP_net.pth\",\n",
    ")\n",
    "\n",
    "criterion5, optimizer5, scheduler5, mixup5 = build_training_components(\n",
    "    model = Model_5,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config5,\n",
    ")\n",
    "\n",
    "best_acc5, history5 = train_model(\n",
    "    model=Model_5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion5,\n",
    "    optimizer=optimizer5,\n",
    "    scheduler=scheduler5,\n",
    "    mixup=mixup5,\n",
    "    device=device,\n",
    "    num_epochs=config5.num_epochs,\n",
    "    save_path=config5.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-l_fTDyJGZX",
    "outputId": "6bcf3ee7-2487-4780-9d82-b0a063022c51"
   },
   "outputs": [],
   "source": [
    "best_model5 = Model_5\n",
    "\n",
    "best_model5.load_state_dict(\n",
    "    torch.load(config5.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model5.to(device)\n",
    "\n",
    "test_loss5, test_acc5, test_prec5, test_recall5, test_f15 = evaluate(best_model5, test_loader, criterion5, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss5:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "id": "_sf0iKJRGVA6"
   },
   "outputs": [],
   "source": [
    "model_name5 = \"No MixUp\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name5]\n",
    "\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name5,\n",
    "    \"Test Loss\": test_loss5,\n",
    "    \"Test Accuracy\": test_acc5,\n",
    "    \"Test Precision\": test_prec5,\n",
    "    \"Test Recall\": test_recall5,\n",
    "    \"Test F1\": test_f15\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "id": "U4PMXa8krXMR"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name5] = history5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Pdcd4iQqHKnL",
    "outputId": "0bec8fcf-ecfc-4691-9fec-9b6ea553fab7"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history5)\n",
    "plot_loss(history5)\n",
    "plot_learning_rate(history5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "id": "yxBAwp_9zBUU"
   },
   "source": [
    "## **Model variant: No Data Augmentation**\n",
    "\n",
    "**Change**\n",
    "- Disabled all data augmentation techniques during training, including MixUp and any other stochastic input transformations.\n",
    "\n",
    "**Purpose**\n",
    "- To isolate the contribution of data augmentation to model generalization and performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "id": "EShR0cuK_OCW"
   },
   "outputs": [],
   "source": [
    "train_transform2 = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset2 = OxfordPetDataset(split=\"train\", transform=train_transform2)\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "    train_dataset2,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALqHsA_jJG5v",
    "outputId": "fc5005b8-376d-4b36-fa3e-58b91b4563c1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_6 = Net(n_classes=train_dataset.get_num_classes()).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_6.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_6.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj3UNQXfJG5w",
    "outputId": "6b6a3724-c335-4398-a50d-420dad91c74d"
   },
   "outputs": [],
   "source": [
    "config6 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=False,\n",
    "    save_path=\"best_noAUGM_net.pth\",\n",
    ")\n",
    "\n",
    "criterion6, optimizer6, scheduler6, mixup6 = build_training_components(\n",
    "    model = Model_6,\n",
    "    train_loader=train_loader2,\n",
    "    train_dataset=train_dataset2,\n",
    "    config=config6,\n",
    ")\n",
    "\n",
    "best_acc6, history6 = train_model(\n",
    "    model=Model_6,\n",
    "    train_loader=train_loader2,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion6,\n",
    "    optimizer=optimizer6,\n",
    "    scheduler=scheduler6,\n",
    "    mixup=mixup6,\n",
    "    device=device,\n",
    "    num_epochs=config6.num_epochs,\n",
    "    save_path=config6.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00TAmCfbJG5w",
    "outputId": "ef6a64e1-9a97-4052-a198-540e89251b43"
   },
   "outputs": [],
   "source": [
    "best_model6 = Model_6\n",
    "\n",
    "best_model6.load_state_dict(\n",
    "    torch.load(config6.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model6.to(device)\n",
    "\n",
    "test_loss6, test_acc6, test_prec6, test_recall6, test_f16 = evaluate(best_model6, test_loader, criterion6, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss6:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc6:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "id": "t3rkQiHjGdTg"
   },
   "outputs": [],
   "source": [
    "model_name6 = \"No Augmentation\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name6]\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name6,\n",
    "    \"Test Loss\": test_loss6,\n",
    "    \"Test Accuracy\": test_acc6,\n",
    "    \"Test Precision\": test_prec6,\n",
    "    \"Test Recall\": test_recall6,\n",
    "    \"Test F1\": test_f16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "id": "qlvbAscMrbto"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name6] = history6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8W6LUZgDG0Uw",
    "outputId": "f15e8d07-2bf9-4204-ae21-101f65da3205"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history6)\n",
    "plot_loss(history6)\n",
    "plot_learning_rate(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {
    "id": "95a7DbvjzDwC"
   },
   "source": [
    "## **Model variant: No Stem Layer**\n",
    "\n",
    "**Change**\n",
    "- Removed the convolutional stem (7×7 convolution, Batch Normalization, ReLU, and max pooling), feeding inputs directly into the first stage of the network.\n",
    "\n",
    "**Purpose**\n",
    "- To investigate the importance of early-stage feature extraction and aggressive spatial downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3C8qpU3iJHaf",
    "outputId": "daa3368a-6864-4bf4-b67d-81b20bd5f432"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_7 = Net(n_classes=train_dataset.get_num_classes(), use_stem=False).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_7.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_7.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6UtBzZpJHag",
    "outputId": "87355cf5-0d31-4aeb-de80-0e1e8bd286ef"
   },
   "outputs": [],
   "source": [
    "config7 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_noSTEM_net.pth\",\n",
    ")\n",
    "\n",
    "criterion7, optimizer7, scheduler7, mixup7 = build_training_components(\n",
    "    model = Model_7,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config7,\n",
    ")\n",
    "\n",
    "best_acc7, history7 = train_model(\n",
    "    model=Model_7,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion7,\n",
    "    optimizer=optimizer7,\n",
    "    scheduler=scheduler7,\n",
    "    mixup=mixup7,\n",
    "    device=device,\n",
    "    num_epochs=config7.num_epochs,\n",
    "    save_path=config7.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqWdBkAmJHag",
    "outputId": "ce9a2543-4e3d-4b4b-e7f4-381c7a0f59a8"
   },
   "outputs": [],
   "source": [
    "best_model7 = Model_7\n",
    "\n",
    "best_model7.load_state_dict(\n",
    "    torch.load(config7.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model7.to(device)\n",
    "\n",
    "test_loss7, test_acc7, test_prec7, test_recall7, test_f17 = evaluate(best_model7, test_loader, criterion7, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss7:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc7:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "id": "2qd6T5cUGjaR"
   },
   "outputs": [],
   "source": [
    "model_name7 = \"No StemLayer\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name7]\n",
    "\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name7,\n",
    "    \"Test Loss\": test_loss7,\n",
    "    \"Test Accuracy\": test_acc7,\n",
    "    \"Test Precision\": test_prec7,\n",
    "    \"Test Recall\": test_recall7,\n",
    "    \"Test F1\": test_f17\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "id": "tU74y4qMrhEQ"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name7] = history7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H47Fdz_iH7qv",
    "outputId": "7e19c666-2260-4589-89de-3876a56b20ca"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history7)\n",
    "plot_loss(history7)\n",
    "plot_learning_rate(history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {
    "id": "iUnS-uhgzGRZ"
   },
   "source": [
    "## **Model variant: No Residual Blocks**\n",
    "\n",
    "**Change**\n",
    "- Replaced all residual blocks with plain convolutional blocks, removing skip connections while preserving depth and channel dimensions.\n",
    "\n",
    "**Purpose**\n",
    "- To evaluate the contribution of residual connections to optimization stability and final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3j5wQiRRJH42",
    "outputId": "1f717dfc-6fc8-4020-a359-294c122bccf4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_8 = Net(n_classes=train_dataset.get_num_classes(), use_residuals=False).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_8.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_8.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAjCxOwZJH43",
    "outputId": "54622d63-8ea0-40c9-b5e2-5f2e41af6e46"
   },
   "outputs": [],
   "source": [
    "config8 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_noRES_net.pth\",\n",
    ")\n",
    "\n",
    "criterion8, optimizer8, scheduler8, mixup8 = build_training_components(\n",
    "    model = Model_8,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config8,\n",
    ")\n",
    "\n",
    "best_acc8, history8 = train_model(\n",
    "    model=Model_8,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion8,\n",
    "    optimizer=optimizer8,\n",
    "    scheduler=scheduler8,\n",
    "    mixup=mixup8,\n",
    "    device=device,\n",
    "    num_epochs=config8.num_epochs,\n",
    "    save_path=config8.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQJVkgtIJH44",
    "outputId": "f5ccb7e8-7efe-4eaf-f019-447e3c98d802"
   },
   "outputs": [],
   "source": [
    "best_model8 = Model_8\n",
    "\n",
    "best_model8.load_state_dict(\n",
    "    torch.load(config8.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model8.to(device)\n",
    "\n",
    "test_loss8, test_acc8, test_prec8, test_recall8, test_f18 = evaluate(best_model8, test_loader, criterion8, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss8:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc8:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "id": "iLpFQAj_GqOI"
   },
   "outputs": [],
   "source": [
    "model_name8 = \"No Residual\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name8]\n",
    "\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name8,\n",
    "    \"Test Loss\": test_loss8,\n",
    "    \"Test Accuracy\": test_acc8,\n",
    "    \"Test Precision\": test_prec8,\n",
    "    \"Test Recall\": test_recall8,\n",
    "    \"Test F1\": test_f18\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {
    "id": "oUwUdAgIrlSQ"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name8] = history8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AedDOLoZH_aI",
    "outputId": "620f74b8-e419-488a-bc2a-1a8685c62735"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history8)\n",
    "plot_loss(history8)\n",
    "plot_learning_rate(history8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {
    "id": "lk6V2lsqzaXD"
   },
   "source": [
    "## **Model variant: No Label Smoothing**\n",
    "\n",
    "**Change**\n",
    "- Disabled label smoothing in the cross-entropy loss, using hard one-hot target labels during training.\n",
    "\n",
    "**Purpose**\n",
    "- To assess the effect of label smoothing on model calibration and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3JDEOnqJIYP",
    "outputId": "cae6335e-7dd7-418e-cfd9-0b8d8adbfab6"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_9 = Net(n_classes=train_dataset.get_num_classes()).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_9.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_9.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDxrBi0VJIYQ",
    "outputId": "c7534bbd-b98b-41e8-d1bc-5c08b91a8026"
   },
   "outputs": [],
   "source": [
    "config9 = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=False,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_noSMOOTH_net.pth\",\n",
    ")\n",
    "\n",
    "criterion9, optimizer9, scheduler9, mixup9 = build_training_components(\n",
    "    model = Model_9,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config9,\n",
    ")\n",
    "\n",
    "best_acc9, history9 = train_model(\n",
    "    model=Model_9,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion9,\n",
    "    optimizer=optimizer9,\n",
    "    scheduler=scheduler9,\n",
    "    mixup=mixup9,\n",
    "    device=device,\n",
    "    num_epochs=config9.num_epochs,\n",
    "    save_path=config9.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUXj7b2dJIYQ",
    "outputId": "2d18146c-0795-4654-ba15-264d139e113e"
   },
   "outputs": [],
   "source": [
    "best_model9 = Model_9\n",
    "\n",
    "best_model9.load_state_dict(\n",
    "    torch.load(config9.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model9.to(device)\n",
    "\n",
    "test_loss9, test_acc9, test_prec9, test_recall9, test_f19 = evaluate(best_model9, test_loader, criterion9, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss9:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc9:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {
    "id": "94HtyDiPGwyx"
   },
   "outputs": [],
   "source": [
    "model_name9 = \"No LabelSmoothing\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name9]\n",
    "\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name9,\n",
    "    \"Test Loss\": test_loss9,\n",
    "    \"Test Accuracy\": test_acc9,\n",
    "    \"Test Precision\": test_prec9,\n",
    "    \"Test Recall\": test_recall9,\n",
    "    \"Test F1\": test_f19\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {
    "id": "yVVSdpiLroxT"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name9] = history9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W6PELVnKICZ_",
    "outputId": "2181f6eb-4509-4278-b9af-e79e3bac5935"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history9)\n",
    "plot_loss(history9)\n",
    "plot_learning_rate(history9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {
    "id": "CJA79QmBzcam"
   },
   "source": [
    "## **Model variant: No Learning Rate Scheduler**\n",
    "\n",
    "**Change**\n",
    "- Disabled the learning rate scheduler, training the model with a constant learning rate throughout all epochs.\n",
    "\n",
    "**Purpose**\n",
    "- To evaluate the impact of dynamic learning rate scheduling on convergence speed and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ocdej3RJI5S",
    "outputId": "49672e82-944b-4f3e-bd5f-d4273779e5c6"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Re-initialize model with fresh weights\n",
    "Model_10 = Net(n_classes=train_dataset.get_num_classes()).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in Model_10.parameters())\n",
    "trainable_params = sum(p.numel() for p in Model_10.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQqfmSPFJI5T",
    "outputId": "f8ff6851-5726-45dc-f61e-1a8f52ec738d"
   },
   "outputs": [],
   "source": [
    "config10 = TrainConfig(\n",
    "    use_scheduler=False,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_noLRSCHED_net.pth\",\n",
    ")\n",
    "\n",
    "criterion10, optimizer10, scheduler10, mixup10 = build_training_components(\n",
    "    model = Model_10,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config10,\n",
    ")\n",
    "\n",
    "best_acc10, history10 = train_model(\n",
    "    model=Model_10,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion10,\n",
    "    optimizer=optimizer10,\n",
    "    scheduler=scheduler10,\n",
    "    mixup=mixup10,\n",
    "    device=device,\n",
    "    num_epochs=config10.num_epochs,\n",
    "    save_path=config10.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jW1hNY0LJI5U",
    "outputId": "db1836d0-9c77-4d08-896a-0889aadb2691"
   },
   "outputs": [],
   "source": [
    "best_model10 = Model_10\n",
    "\n",
    "best_model10.load_state_dict(\n",
    "    torch.load(config10.save_path, map_location=device)\n",
    ")\n",
    "\n",
    "best_model10.to(device)\n",
    "\n",
    "test_loss10, test_acc10, test_prec10, test_recall10, test_f110 = evaluate(best_model10, test_loader, criterion10, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss10:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc10:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "id": "ATtaVLuhG1vC"
   },
   "outputs": [],
   "source": [
    "model_name10 = \"No LR Scheduler\"\n",
    "\n",
    "# Remove existing entry if re-training same model\n",
    "results = [r for r in results if r[\"Model\"] != model_name10]\n",
    "\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name10,\n",
    "    \"Test Loss\": test_loss10,\n",
    "    \"Test Accuracy\": test_acc10,\n",
    "    \"Test Precision\": test_prec10,\n",
    "    \"Test Recall\": test_recall10,\n",
    "    \"Test F1\": test_f110\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {
    "id": "sRJW1Z_1rs8I"
   },
   "outputs": [],
   "source": [
    "all_histories[model_name10] = history10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Save JSON\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Strx_k6sIFcu",
    "outputId": "8abfb0b4-0d95-407b-c6ac-68e6a9b5c211"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history10)\n",
    "plot_loss(history10)\n",
    "plot_learning_rate(history10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {
    "id": "CZ_KSPucY2PD"
   },
   "source": [
    "## **RECAP**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\n",
    "    \"Baseline\",\n",
    "    \"No BatchNorm\",\n",
    "    \"No Pooling\",\n",
    "    \"No Dropout\",\n",
    "    \"No MixUp\",\n",
    "    \"No Augmentation\",\n",
    "    \"No StemLayer\",\n",
    "    \"No Residual\",\n",
    "    \"No LabelSmoothing\",\n",
    "    \"No LR Scheduler\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for model in model_order:\n",
    "    if model in all_histories:\n",
    "        plt.plot(all_histories[model][\"val_acc\"])\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy - Ablation Study\")\n",
    "plt.legend([m for m in model_order if m in all_histories], loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for model in model_order:\n",
    "    if model in all_histories:\n",
    "        plt.plot(all_histories[model][\"train_loss\"])\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss - Ablation Study\")\n",
    "plt.legend([m for m in model_order if m in all_histories], loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {
    "id": "192fb2ce"
   },
   "source": [
    "## Part 2: fine-tune an existing network\n",
    "\n",
    "Your goal is to fine-tune a pretrained ResNet-18 model on `OxfordPetDataset`. Use the implementation provided by PyTorch, i.e. the opposite of part 1. Specifically, use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
    "\n",
    "2A. First, fine-tune the ResNet-18 with the same training hyperparameters you used for your best model in part 1.\n",
    "\n",
    "2B. Then, tweak the training hyperparameters in order to increase the accuracy on the test split. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions — papers, blog posts, YouTube videos, or whatever else you may find useful. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afhrZ1_oYrAk",
    "outputId": "b8c07595-71dd-4a3b-dbd7-aedf6954aa29"
   },
   "outputs": [],
   "source": [
    "# Part 2A: Fine-tune ResNet-18 with the same hyperparameters as Part 1\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Load pretrained ResNet-18 (ImageNet-1K V1 weights)\n",
    "resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace the final classifier layer for 37 pet classes\n",
    "num_classes = train_dataset.get_num_classes()\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in resnet18.parameters())\n",
    "trainable_params = sum(p.numel() for p in resnet18.parameters() if p.requires_grad)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bec421b",
    "outputId": "1f266d24-0e8d-431a-9e39-dda1728699d3"
   },
   "outputs": [],
   "source": [
    "# Train ResNet-18 with the same config as Part 1 best model (full baseline)\n",
    "\n",
    "config_2a = TrainConfig(\n",
    "    use_scheduler=True,\n",
    "    use_label_smoothing=True,\n",
    "    use_mixup=True,\n",
    "    save_path=\"best_resnet18_2a.pth\",\n",
    ")\n",
    "\n",
    "criterion_2a, optimizer_2a, scheduler_2a, mixup_2a = build_training_components(\n",
    "    model=resnet18,\n",
    "    train_loader=train_loader,\n",
    "    train_dataset=train_dataset,\n",
    "    config=config_2a,\n",
    ")\n",
    "\n",
    "best_acc_2a, history_2a = train_model(\n",
    "    model=resnet18,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion_2a,\n",
    "    optimizer=optimizer_2a,\n",
    "    scheduler=scheduler_2a,\n",
    "    mixup=mixup_2a,\n",
    "    device=device,\n",
    "    num_epochs=config_2a.num_epochs,\n",
    "    save_path=config_2a.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35453374",
    "outputId": "ca195b94-7705-4a13-caba-cf37bc5ccb2d"
   },
   "outputs": [],
   "source": [
    "# Evaluate ResNet-18 Part 2A on test set\n",
    "\n",
    "resnet18.load_state_dict(\n",
    "    torch.load(config_2a.save_path, map_location=device)\n",
    ")\n",
    "resnet18.to(device)\n",
    "\n",
    "test_loss_2a, test_acc_2a, test_prec_2a, test_recall_2a, test_f1_2a = evaluate(resnet18, test_loader, criterion_2a, device)\n",
    "\n",
    "print(f\"Part 2A - ResNet-18 Fine-tuned (same hyperparameters as Part 1)\")\n",
    "print(f\"Test Loss: {test_loss_2a:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc_2a:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b84520a4",
    "outputId": "90e06672-37f1-495f-ca0c-b92de2976f15"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history_2a)\n",
    "plot_loss(history_2a)\n",
    "plot_learning_rate(history_2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {
    "id": "fcf86e41"
   },
   "source": [
    "## Part 2B: Fine-tune ResNet-18 with optimized hyperparameters\n",
    "\n",
    "**Changes from Part 2A:**\n",
    "- **Lower learning rate**: Pretrained models benefit from smaller learning rates (1e-4 to 3e-4) to avoid destroying pretrained features\n",
    "- **Layer-wise learning rate decay**: Lower learning rate for earlier layers, higher for classifier (discriminative fine-tuning)\n",
    "- **Longer warmup**: Gradual warmup helps stabilize training when fine-tuning\n",
    "- **Freeze backbone initially** (optional): Train only the classifier first, then unfreeze the backbone\n",
    "\n",
    "**Justification:**\n",
    "- Transfer learning best practices suggest using smaller learning rates for pretrained layers (Yosinski et al., \"How transferable are features in deep neural networks?\", NeurIPS 2014)\n",
    "- The pretrained features are already well-optimized for image classification; aggressive updates can degrade them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5d80bdd",
    "outputId": "20027e75-bc05-47c0-b43c-6fa101c9fef4"
   },
   "outputs": [],
   "source": [
    "# Part 2B: Fine-tune ResNet-18 with optimized hyperparameters\n",
    "\n",
    "# Re-initialize pretrained ResNet-18\n",
    "resnet18_2b = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_classes = train_dataset.get_num_classes()\n",
    "resnet18_2b.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.4, inplace=False),\n",
    "    nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    ")\n",
    "\n",
    "resnet18_2b = resnet18_2b.to(device)\n",
    "\n",
    "# Training only the head\n",
    "resnet18_2b.requires_grad_ = False\n",
    "resnet18_2b.fc.requires_grad_ = True\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "total_params = sum(p.numel() for p in resnet18_2b.parameters())\n",
    "trainable_params = sum(p.numel() for p in resnet18_2b.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {
    "id": "faeaf13d"
   },
   "outputs": [],
   "source": [
    "# Train with optimized hyperparameters for transfer learning\n",
    "base_lr = 1e-3\n",
    "classifier_lr = 1e-5\n",
    "num_epochs_2b = 100\n",
    "\n",
    "optimizer_fc = torch.optim.AdamW(resnet18_2b.fc.parameters(), lr=base_lr, weight_decay=1e-3)\n",
    "optimizer_full = torch.optim.AdamW(resnet18_2b.parameters(), lr=classifier_lr, weight_decay=1e-3)\n",
    "\n",
    "scheduler_fc = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_fc, T_max=num_epochs_2b)\n",
    "scheduler_full = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_full, T_max=num_epochs_2b)\n",
    "\n",
    "# Use label smoothing for regularization\n",
    "criterion_2b = nn.CrossEntropyLoss(label_smoothing=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f417a08",
    "outputId": "72545a66-f15c-4b5a-ac54-f4191801d191"
   },
   "outputs": [],
   "source": [
    "# Train the model - only the head\n",
    "best_acc_2b, history_2b = train_model(\n",
    "    model=resnet18_2b,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion_2b,\n",
    "    optimizer=optimizer_fc,\n",
    "    scheduler=scheduler_fc,\n",
    "    mixup=NoMixUp(),\n",
    "    device=device,\n",
    "    num_epochs=num_epochs_2b,\n",
    "    save_path=\"best_resnet18_2b.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21f60908",
    "outputId": "e13cb66d-3094-4e11-9d71-ecc151fe041c"
   },
   "outputs": [],
   "source": [
    "# Evaluate ResNet-18 Part 2B on test set\n",
    "\n",
    "resnet18_2b.load_state_dict(\n",
    "    torch.load(\"best_resnet18_2b.pth\", map_location=device)\n",
    ")\n",
    "resnet18_2b.to(device)\n",
    "\n",
    "test_loss_2b, test_acc_2b, test_prec_2b, test_recall_2b, test_f1_2b = evaluate(resnet18_2b, test_loader, criterion_2b, device)\n",
    "\n",
    "print(f\"Part 2B - ResNet-18 Fine-tuned - only the head (optimized hyperparameters)\")\n",
    "print(f\"Test Loss: {test_loss_2b:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc_2b:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "120b3370",
    "outputId": "903f8373-5068-4b6e-970d-265488c189a8"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history_2b)\n",
    "plot_loss(history_2b)\n",
    "plot_learning_rate(history_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training all\n",
    "resnet18_2b.requires_grad_ = True\n",
    "best_acc_2b2, history_2b2 = train_model(\n",
    "    model=resnet18_2b,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion_2b,\n",
    "    optimizer=optimizer_full,\n",
    "    scheduler=scheduler_full,\n",
    "    mixup=NoMixUp(),\n",
    "    device=device,\n",
    "    num_epochs=num_epochs_2b,\n",
    "    save_path=\"best_resnet18_2b_full.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_2b.load_state_dict(\n",
    "    torch.load(\"best_resnet18_2b_full.pth\", map_location=device)\n",
    ")\n",
    "resnet18_2b.to(device)\n",
    "\n",
    "test_loss_2b2, test_acc_2b2, test_prec_2b2, test_recall_2b2, test_f1_2b2 = evaluate(resnet18_2b, test_loader, criterion_2b, device)\n",
    "\n",
    "print(f\"Part 2B - ResNet-18 Fine-tuned (optimized hyperparameters)\")\n",
    "print(f\"Test Loss: {test_loss_2b2:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc_2b2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {
    "id": "32182eb3"
   },
   "outputs": [],
   "source": [
    "# Summary comparison: Part 2A vs Part 2B\n",
    "print(\"Part 2 Summary: ResNet-18 Fine-tuning Results\")\n",
    "print(f\"\\nPart 2A (same hyperparameters as Part 1):\")\n",
    "print(f\"  Test Accuracy: {test_acc_2a:.3f}\")\n",
    "print(f\"\\nPart 2B (optimized hyperparameters for transfer learning):\")\n",
    "print(f\"  Test Accuracy: {test_acc_2b2:.3f}\")\n",
    "print(f\"\\nImprovement: {(test_acc_2b2 - test_acc_2a) * 100:.2f} percentage points\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
