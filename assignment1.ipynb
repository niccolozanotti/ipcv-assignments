{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project members\n",
    "\n",
    "**Sali Raffaele**:\n",
    "- ðŸ“§ [raffaele.sali@studio.unibo.it](mailto:raffaele.sali@studio.unibo.it)\n",
    "- Student Number: `0001167817`\n",
    "\n",
    "**Zanotti NiccolÃ²**:\n",
    "- ðŸ“§ [niccolo.zanotti@studio.unibo.it](mailto:niccolo.zanotti@studio.unibo.it)\n",
    "- Student Number: `0001121646`\n",
    "\n",
    "**Zocco Ramazzo Marco**:\n",
    "- ðŸ“§ [marco.zoccoramazzo@studio.unibo.it](mailto:marco.zoccoramazzo@studio.unibo.it)\n",
    "- Student Number: `0001198289`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yOM8fRGa43Y"
   },
   "source": [
    "# **Product Recognition of Books**\n",
    "\n",
    "## Image Processing and Computer Vision - Assignment Module \\#1\n",
    "\n",
    "\n",
    "Contacts:\n",
    "\n",
    "- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n",
    "- Prof. Samuele Salti -> samuele.salti@unibo.it\n",
    "- Alex Costanzino -> alex.costanzino@unibo.it\n",
    "- Francesco Ballerini -> francesco.ballerini4@unibo.it\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Computer vision-based object detection techniques can be applied in library or bookstore settings to build a system that identifies books on shelves.\n",
    "\n",
    "Such a system could assist in:\n",
    "* Helping visually impaired users locate books by title/author;\n",
    "* Automating inventory management (e.g., detecting misplaced or out-of-stock books);\n",
    "* Enabling faster book retrieval by recognizing spine text or cover designs.\n",
    "\n",
    "## Task\n",
    "Develop a computer vision system that, given a reference image for each book, is able to identify such book from one picture of a shelf.\n",
    "\n",
    "<figure>\n",
    "<a href=\"https://ibb.co/pvLVjbM5\"><img src=\"https://i.ibb.co/svVx9bNz/example.png\" alt=\"example\" border=\"0\"></a>\n",
    "</figure>\n",
    "\n",
    "For each type of product displayed on the shelf, the system should compute a bounding box aligned with the book spine or cover and report:\n",
    "1. Number of instances;\n",
    "1. Dimension of each instance (area in pixel of the bounding box that encloses each one of them);\n",
    "1. Position in the image reference system of each instance (four corners of the bounding box that enclose them);\n",
    "1. Overlay of the bounding boxes on the scene images.\n",
    "\n",
    "<font color=\"red\"><b>Each step of this assignment must be solved using traditional computer vision techniques.</b></font>\n",
    "\n",
    "#### Example of expected output\n",
    "```\n",
    "Book 0 - 2 instance(s) found:\n",
    "  Instance 1 {top_left: (100,200), top_right: (110, 220), bottom_left: (10, 202), bottom_right: (10, 208), area: 230px}\n",
    "  Instance 2 {top_left: (90,310), top_right: (95, 340), bottom_left: (24, 205), bottom_right: (23, 234), area: 205px}\n",
    "Book 1 â€“ 1 instance(s) found:\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach Overview\n",
    "\n",
    "This solution implements a **traditional computer vision pipeline** for detecting books on shelves.\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "After extensive experimentation, we adopted a **simplified pipeline** based on the following observations about our dataset:\n",
    "\n",
    "1. **Books are nearly planar**: Shelf images have minimal perspective distortion\n",
    "2. **Books are upright or horizontal**: Spines can be vertical or horizontal (stacked shelves)\n",
    "3. **Scale is consistent**: Model and scene images have similar scale\n",
    "4. **Multiple copies are adjacent**: Same book appears side-by-side on shelves\n",
    "\n",
    "These characteristics led us to choose:\n",
    "\n",
    "| Component | Choice | Justification |\n",
    "|-----------|--------|---------------|\n",
    "| Preprocessing | Clahe | Enhance local contras and reveal texture |\n",
    "| Features | RootSIFT | ~15-30% better matching than standard SIFT |\n",
    "| Matching | BF 5NN consecutive ratio test | Captures good matches at any rank position, fondamental to detect multiple instances in the same scene |\n",
    "| Geometric model | **Similarity transform** (4 DOF) | Uniform scale + rotation + translation; strongest inductive bias for books |\n",
    "| Multi-instance | Iterative detection with keypoint index exclusion | Handles multiple copies efficiently |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9SjPDVSa43k"
   },
   "source": [
    "## Setup and dependencies installation\n",
    "\n",
    "In the following, we will assume that you have\n",
    "- created a local python virtual environment - either with python [venv](https://docs.python.org/3/library/venv.html) module or via [uv](https://github.com/astral-sh/uv) (preferred) - with the `ipykernel` or `jupyter` packages pre-installed to start the jupyter kernel;\n",
    "- have `git` installed on your machine;\n",
    "- have a working internet connection\n",
    "\n",
    "We will now download the `pyproject.toml` file specifying the project dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_project_root() -> Path:\n",
    "    \"\"\"Return the root directory of the project.\"\"\"\n",
    "    start_dir = Path.cwd()\n",
    "\n",
    "    markers = [\"assignment1.ipynb\"]\n",
    "\n",
    "    for path in [start_dir, *list(start_dir.parents)]:\n",
    "        for marker in markers:\n",
    "            if (path / marker).exists():\n",
    "                return path\n",
    "\n",
    "    return start_dir\n",
    "\n",
    "\n",
    "PROJECT_ROOT: Path = get_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "PROJECT_REPO: str = \"niccolozanotti/ipcv-assignments\"\n",
    "COMMIT_HASH: str = \"9f1f600af59401673e2e816b12d1ae740dc4386b\"\n",
    "\n",
    "pyproject_url = (\n",
    "    f\"https://raw.githubusercontent.com/{PROJECT_REPO}/{COMMIT_HASH}/pyproject.toml\"\n",
    ")\n",
    "lockfile_url = f\"https://raw.githubusercontent.com/{PROJECT_REPO}/{COMMIT_HASH}/uv.lock\"\n",
    "\n",
    "urllib.request.urlretrieve(pyproject_url, PROJECT_ROOT / \"pyproject.toml\")\n",
    "urllib.request.urlretrieve(lockfile_url, PROJECT_ROOT / \"uv.lock\");\n",
    "pyproject_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using [uv](https://github.com/astral-sh/uv) (recommended) you can now install the dependencies to a local virtual environment at `.venv` simply via\n",
    "```sh\n",
    "uv sync --extra assignment1\n",
    "```\n",
    "\n",
    "If not, the same can be achieved with the usual python [venv](https://docs.python.org/3/library/venv.html):\n",
    "```sh\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "(.venv) pip install \".[assignment1]\"\n",
    "```\n",
    "\n",
    "Make sure to do the above and *restart the kernel* if necessary before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMpN7gHAa43m",
    "outputId": "b8eb097f-1669-4145-f985-2ff7d3c73aa1"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Set, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9calCjqa43p"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "The following class contains all the parameters used throughout the notebook. The meaning of each parameter is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFtUesnda43q"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration parameters for the detection pipeline.\n",
    "\n",
    "    Each parameter is justified based on dataset characteristics\n",
    "    and experimental results.\n",
    "    \"\"\"\n",
    "\n",
    "    # === FEATURE DETECTION ===\n",
    "    # SIFT parameters (used via RootSIFT)\n",
    "    SIFT_FEATURES = 0                # 0 = detect all features\n",
    "    SIFT_N_OCTAVE_LAYERS = 7         # Increased from default value to capture more details\n",
    "    SIFT_CONTRAST_THRESHOLD = 0.02   # Lowered form default value to detect more keypoints\n",
    "    SIFT_EDGE_THRESHOLD = 10         # Default value\n",
    "    SIFT_SIGMA = 1.2                 # Slightly lower than default 1.6 to detect more keypoints\n",
    "\n",
    "    # === PREPROCESSING ===\n",
    "    # Clahe + histogram equalization\n",
    "    CLAHE_CLIP_LIMIT = 2.0           # Moderate contrast enhancement\n",
    "    CLAHE_GRID_SIZE = (4, 4)         # 4x4 provides finer local adaptation than 8x8\n",
    "    APPLY_PREPROCESSING = True       # Able/disable Clahe\n",
    "\n",
    "    # === FEATURE MATCHING ===\n",
    "    # BF 5NN consecutive ratio test:\n",
    "    KNN_K = 5                        # Number of nearest neighbors considerd\n",
    "    KNN_CONSECUTIVE_RATIO = 0.6      # Consecutive neighbor ratio threshold\n",
    "\n",
    "    # === GEOMETRIC VERIFICATION ===\n",
    "    # Similarity transformation (4 DOF) with RANSAC\n",
    "    MIN_MATCH_COUNT = 3                  # Minimum matches to attempt estimation\n",
    "    RANSAC_REPROJ_THRESHOLD = 4.0        # Pixels â€” allows small localization errors\n",
    "\n",
    "    # === DETECTION VALIDATION ===\n",
    "    MIN_INLIERS = 3                  # Minimum inliers for valid detection\n",
    "    MIN_INLIERS_RATIO = 1 / 3        # At least 1/3 of matches should be inliers\n",
    "    MIN_AREA = 1000                  # Minimum bounding box area (pixels)\n",
    "    MAX_AREA = 100000                # Maximum bounding box area (pixels)\n",
    "    MIN_EXTENT = 0.5                 # Min ratio: contour area / bounding rect area\n",
    "    LOW_EXTENT_THRESHOLD = 0.65      # Below this extent, require extra inlier support\n",
    "    MIN_INLIERS_LOW_EXTENT = 5       # Minimum inliers when extent is below LOW_EXTENT_THRESHOLD\n",
    "    TOLERANCE_on_IMAGE_LIMITS = 4    # Tolerance in pixels of the bounding box to exeed the frame.\n",
    "                                     # Fondamental for the detection of model_14 in scene_4, a correct\n",
    "                                     # detection had a bounding box out of the frame by 3 pixels.\n",
    "\n",
    "    # === MULTI-INSTANCE DETECTION ===\n",
    "    MAX_INSTANCES_PER_BOOK = 10      # Safety limit\n",
    "    SAME_BOOK_IOU_THRESHOLD = 0.30   # Reject same-book detection if >30% overlap with previous instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuZXSNTJa43s"
   },
   "source": [
    "## Data Classes\n",
    "\n",
    "Structured representation of detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8m-8H7la43t"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoundingBox:\n",
    "    \"\"\"\n",
    "    Represents a detected book instance.\n",
    "\n",
    "    Stores the four corners of the bounding quadrilateral,\n",
    "    which may not be axis-aligned due to the affine transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    top_left: Tuple[int, int]\n",
    "    top_right: Tuple[int, int]\n",
    "    bottom_right: Tuple[int, int]\n",
    "    bottom_left: Tuple[int, int]\n",
    "    area: int\n",
    "    n_inliers: int\n",
    "    inlier_ratio: float\n",
    "\n",
    "    def get_polygon(self) -> np.ndarray:\n",
    "        \"\"\"Return corners as numpy array for geometric operations.\"\"\"\n",
    "        return np.array(\n",
    "            [self.top_left, self.top_right, self.bottom_right, self.bottom_left],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BookDetection:\n",
    "    \"\"\"All detections for a single book model in a scene.\"\"\"\n",
    "\n",
    "    book_id: int\n",
    "    model_path: str\n",
    "    instances: List[BoundingBox]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whh1bh7Ra43u"
   },
   "source": [
    "## RootSIFT Feature Extractor\n",
    "\n",
    "### Why RootSIFT?\n",
    "\n",
    "Standard SIFT descriptors are compared using Euclidean distance. However, SIFT descriptors are histograms of gradient orientations, and the **Hellinger kernel** (Bhattacharyya distance) is more appropriate for histogram comparison.\n",
    "\n",
    "RootSIFT achieves this by:\n",
    "1. L1-normalizing the SIFT descriptor\n",
    "2. Taking the element-wise square root\n",
    "\n",
    "This allows Euclidean distance to implicitly compute Hellinger distance, providing **~15-30% better matching accuracy**.\n",
    "\n",
    "**Reference**: ArandjeloviÄ‡ & Zisserman (2012), *Three things everyone should know to improve object retrieval*, IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, DOI: [10.1109/CVPR.2012.6248018](https://doi.org/10.1109/CVPR.2012.6248018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U94_JJAKa43v"
   },
   "outputs": [],
   "source": [
    "class RootSIFT:\n",
    "    \"\"\"\n",
    "    RootSIFT feature extractor.\n",
    "\n",
    "    Enhances SIFT descriptors for better matching performance\n",
    "    by applying L1 normalization followed by square root.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sift = cv2.SIFT_create(\n",
    "            nfeatures=Config.SIFT_FEATURES,\n",
    "            nOctaveLayers=Config.SIFT_N_OCTAVE_LAYERS,\n",
    "            contrastThreshold=Config.SIFT_CONTRAST_THRESHOLD,\n",
    "            edgeThreshold=Config.SIFT_EDGE_THRESHOLD,\n",
    "            sigma=Config.SIFT_SIGMA,\n",
    "        )\n",
    "\n",
    "    def detect_and_compute(self, image: np.ndarray):\n",
    "        \"\"\"\n",
    "        Detect keypoints and compute RootSIFT descriptors.\n",
    "\n",
    "        Args:\n",
    "            image: Grayscale input image\n",
    "\n",
    "        Returns:\n",
    "            keypoints: List of cv2.KeyPoint\n",
    "            descriptors: RootSIFT descriptors (Nx128 float32 array)\n",
    "        \"\"\"\n",
    "        keypoints, descriptors = self.sift.detectAndCompute(image, None)\n",
    "\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            return keypoints, None\n",
    "\n",
    "        # Convert to RootSIFT\n",
    "        eps = 1e-7\n",
    "\n",
    "        # Step 1: L1 normalize\n",
    "        descriptors = descriptors / (np.sum(descriptors, axis=1, keepdims=True) + eps)\n",
    "\n",
    "        # Step 2: Square root (Hellinger kernel)\n",
    "        descriptors = np.sqrt(descriptors)\n",
    "\n",
    "        return keypoints, descriptors.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFDYBvQ7a43w"
   },
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "### Grayscale conversion + Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "\n",
    "Clahe was applied to enhance local contrast. We initially used a default dimension for the kernel (`8x8`), but we noticed how that could be problematic on some noisy images of the dataset.\n",
    "After some tweaking we decided to lower it to `4x4`. This guarantees a more global behavior, smoother results, less local contrast exaggeration and more important less noise amplification.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKzBryfSa43x"
   },
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline: Gaussian smoothing + histogram equalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.clahe = cv2.createCLAHE(\n",
    "            clipLimit=Config.CLAHE_CLIP_LIMIT, tileGridSize=Config.CLAHE_GRID_SIZE\n",
    "        )\n",
    "\n",
    "    def preprocess(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert to grayscale, apply Gaussian blur, and equalize histogram.\n",
    "\n",
    "        Args:\n",
    "            image: BGR input image\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed grayscale image\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image.copy()\n",
    "        if Config.APPLY_PREPROCESSING:\n",
    "            return self.clahe.apply(gray)\n",
    "        else:\n",
    "            return gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYs29G24a43y"
   },
   "source": [
    "## Feature Matching\n",
    "\n",
    "### BF 5NN Consecutive Ratio Test\n",
    "\n",
    "For each model keypoint's 5 nearest neighbors we compare consecutive distances:   \n",
    "if `match[i].distance < 0.6 * match[i+1].distance` keep `match[i]`.\n",
    "\n",
    "Unlike the standard Lowe's ratio test that only checks rank 1 vs rank 2, this captures good matches at **any rank position**. This is particularly valuable for multi-instance detection, in which a model keypoint may match multiple scene locations at different ranks.\n",
    "We decided to use a kNN with `k=5` knowing there are no more than 4 instances of the same book in a single scene of the dataset. `k` from the class `Config` could be increased for robustness and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMEz38Pla43z"
   },
   "outputs": [],
   "source": [
    "class FeatureMatcher:\n",
    "    \"\"\"\n",
    "    Feature matcher using BF 5NN consecutive ratio test.\n",
    "\n",
    "    For each model keypoint, finds 5 nearest neighbors in the scene\n",
    "    and keeps match[i] if match[i].distance < ratio * match[i+1].distance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "\n",
    "    def match(\n",
    "        self,\n",
    "        des_model: np.ndarray,\n",
    "        des_scene: np.ndarray,\n",
    "        excluded_indices: Set[int] = None,\n",
    "    ) -> List[cv2.DMatch]:\n",
    "        \"\"\"\n",
    "        Match model descriptors to scene descriptors.\n",
    "\n",
    "        Args:\n",
    "            des_model: Model descriptors\n",
    "            des_scene: Scene descriptors\n",
    "            excluded_indices: Scene keypoint indices to exclude (already matched)\n",
    "\n",
    "        Returns:\n",
    "            List of good matches\n",
    "        \"\"\"\n",
    "        if des_model is None or des_scene is None:\n",
    "            return []\n",
    "        if len(des_model) < 2 or len(des_scene) < Config.KNN_K:\n",
    "            return []\n",
    "\n",
    "        if excluded_indices is None:\n",
    "            excluded_indices = set()\n",
    "\n",
    "        try:\n",
    "            knn_matches = self.bf.knnMatch(des_model, des_scene, k=Config.KNN_K)\n",
    "        except cv2.error:\n",
    "            return []\n",
    "\n",
    "        good_matches = []\n",
    "        seen = set()\n",
    "\n",
    "        for match_group in knn_matches:\n",
    "            for i in range(len(match_group) - 1):\n",
    "                m = match_group[i]\n",
    "                m_next = match_group[i + 1]\n",
    "\n",
    "                if m.trainIdx in excluded_indices:\n",
    "                    continue\n",
    "\n",
    "                if m.distance < Config.KNN_CONSECUTIVE_RATIO * m_next.distance:\n",
    "                    key = (m.queryIdx, m.trainIdx)\n",
    "                    if key not in seen:\n",
    "                        seen.add(key)\n",
    "                        good_matches.append(m)\n",
    "\n",
    "        return good_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHUWRcera431"
   },
   "source": [
    "\n",
    "## Similarity Transform Estimator\n",
    "\n",
    "### Similarity Transform (Partial Affine) instead of Full Affine or Homography\n",
    "\n",
    "We use `cv2.estimateAffinePartial2D` which estimates a **similarity transform** (4 DOF):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x' \\\\\n",
    "y'\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "s \\cos\\theta & -s \\sin\\theta \\\\\n",
    "s \\sin\\theta & s \\cos\\theta\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "t_x \\\\\n",
    "t_y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This allows only:\n",
    "- **Uniform scaling** (single scale factor $s$)\n",
    "- **Rotation** (angle $\\theta$)\n",
    "- **Translation** ($t_x$, $t_y$)\n",
    "\n",
    "### Why Similarity Transform (4 DOF) instead of Full Affine (6 DOF) or Homography (8 DOF)?\n",
    "\n",
    "| Property | Homography | Full Affine | Similarity (Partial Affine) |\n",
    "|----------|------------|-------------|----------------------------|\n",
    "| Degrees of freedom | 8 | 6 | **4** |\n",
    "| Minimum points | 4 | 3 | **2** |\n",
    "| Handles perspective | Yes | No | No |\n",
    "| Preserves aspect ratio | No | No | **Yes** |\n",
    "| Allows shear | Yes | Yes | **No** |\n",
    "| Stability with few points | Lowest | Medium | **Highest** |\n",
    "\n",
    "Books on shelves do not undergo perspective distortion, shear, or non-uniform scaling. The similarity transform (`cv2.estimateAffinePartial2D`) encodes exactly this **inductive bias**: it only allows uniform scaling, rotation, and translation. This makes RANSAC converge faster and more reliably, especially with the limited matches (~8-15) typical of narrow book spines.\n",
    "\n",
    "The resulting 2Ã—3 matrix is converted to a 3Ã—3 homography matrix (by appending `[0, 0, 1]`) for convenient corner projection via `cv2.perspectiveTransform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uEz7JuWa431"
   },
   "outputs": [],
   "source": [
    "class AffineEstimator:\n",
    "    \"\"\"\n",
    "    Similarity transform estimator using RANSAC.\n",
    "    \"\"\"\n",
    "\n",
    "    def estimate(\n",
    "        self, src_pts: np.ndarray, dst_pts: np.ndarray\n",
    "    ) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], int]:\n",
    "        \"\"\"\n",
    "        Estimate similarity transformation using RANSAC.\n",
    "\n",
    "        Args:\n",
    "            src_pts: Source points (model) - Nx2 array\n",
    "            dst_pts: Destination points (scene) - Nx2 array\n",
    "\n",
    "        Returns:\n",
    "            (homography_matrix, inlier_mask, n_inliers)\n",
    "            homography_matrix is 3x3 (similarity embedded in homography form)\n",
    "        \"\"\"\n",
    "        if len(src_pts) < 2:\n",
    "            return None, None, 0\n",
    "\n",
    "        try:\n",
    "            M, inliers = cv2.estimateAffinePartial2D(\n",
    "                src_pts.reshape(-1, 1, 2),\n",
    "                dst_pts.reshape(-1, 1, 2),\n",
    "                method=cv2.RANSAC,\n",
    "                ransacReprojThreshold=Config.RANSAC_REPROJ_THRESHOLD,\n",
    "            )\n",
    "\n",
    "            if M is None or inliers is None:\n",
    "                return None, None, 0\n",
    "\n",
    "            # Convert 2x3 affine to 3x3 homography matrix\n",
    "            H = np.vstack([M, [0, 0, 1]])\n",
    "\n",
    "            n_inliers = int(np.sum(inliers))\n",
    "            return H, inliers, n_inliers\n",
    "\n",
    "        except cv2.error:\n",
    "            return None, None, 0\n",
    "\n",
    "    def transform_corners(\n",
    "        self, H: np.ndarray, model_shape: Tuple[int, int]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Project model corners to scene using the homography matrix.\n",
    "\n",
    "        Args:\n",
    "            H: 3x3 homography matrix\n",
    "            model_shape: (height, width) of model image\n",
    "\n",
    "        Returns:\n",
    "            Projected corners as 4x2 array: [TL, TR, BR, BL]\n",
    "        \"\"\"\n",
    "        h, w = model_shape\n",
    "        corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
    "        projected = cv2.perspectiveTransform(corners, H)\n",
    "        return projected.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ppXDRYea432"
   },
   "source": [
    "## Detection Validation\n",
    "\n",
    "### Validation Criteria\n",
    "\n",
    "A detected rectangle is considered geometrically valid if the following 4 conditions hold:\n",
    "1. **4 points**: The projected contour has exactly 4 corners\n",
    "2. **Reasonable area**: Between 1000 and 100000 pixels\n",
    "3. **Roughly rectangular**: The ratio of contour area to bounding rect area (extent) is above 0.5\n",
    "4. **Within image bounds**: All corners lie within the image dimensions with a tolerance of 4 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL8MKmVSa433"
   },
   "outputs": [],
   "source": [
    "def is_rectangle_valid(\n",
    "    rectangle: np.ndarray, image_shape: Tuple[int, ...]\n",
    ") -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate whether a detected rectangle is geometrically reasonable.\n",
    "\n",
    "    Args:\n",
    "        rectangle: 4x1x2 or 4x2 array of corner points\n",
    "        image_shape: (height, width, ...) of the scene image\n",
    "\n",
    "    Returns:\n",
    "        (is_valid, reason_string)\n",
    "    \"\"\"\n",
    "    # Ensure correct shape for cv2 functions\n",
    "    rect = np.int32(rectangle)\n",
    "    if rect.ndim == 2:\n",
    "        rect = rect.reshape(-1, 1, 2)\n",
    "\n",
    "    if len(rect) != 4:\n",
    "        return False, \"Not 4 points\"\n",
    "\n",
    "    # Check area\n",
    "    area = cv2.contourArea(rect)\n",
    "    if area < Config.MIN_AREA or area > Config.MAX_AREA:\n",
    "        return False, f\"Invalid area: {area:.0f}\"\n",
    "\n",
    "    # Check ratio of contour area to bounding box area (extent)\n",
    "    x, y, w, h = cv2.boundingRect(rect)\n",
    "    bounding_box_area = w * h\n",
    "    if bounding_box_area == 0:\n",
    "        return False, \"Zero bounding box area\"\n",
    "    extent = area / bounding_box_area\n",
    "\n",
    "    if extent < Config.MIN_EXTENT:\n",
    "        return False, f\"Not rectangular enough: extent={extent:.2f}\"\n",
    "\n",
    "    # Check if the rectangle is within the image bounds\n",
    "    for p in rect:\n",
    "        px, py = p[0]\n",
    "        if (\n",
    "            px + Config.TOLERANCE_on_IMAGE_LIMITS < 0\n",
    "            or px - Config.TOLERANCE_on_IMAGE_LIMITS >= image_shape[1]\n",
    "            or py + Config.TOLERANCE_on_IMAGE_LIMITS < 0\n",
    "            or py - Config.TOLERANCE_on_IMAGE_LIMITS >= image_shape[0]\n",
    "        ):\n",
    "            return False, \"Point out of bounds\"\n",
    "\n",
    "    return True, \"Valid rectangle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNAIDtrwa433"
   },
   "source": [
    "## Book Detector\n",
    "\n",
    "### Multi-Instance Detection: Iterative Keypoint Exclusion\n",
    "\n",
    "1. Match all model keypoints to scene (excluding previously used indices)\n",
    "2. Estimate similarity transformation (RANSAC)\n",
    "3. Validate the detection (number of inlier, inlier ratio, rectangle validity)\n",
    "4. If valid: save detection, add inlier scene keypoint indices to exclusion set\n",
    "5. Re-match with excluded indices, repeat until no valid detections remain\n",
    "\n",
    "This approach extracts features only once and allows multiple RANSAC\n",
    "attempts even when match quality degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeometryUtils:\n",
    "    \"\"\"Geometric utility functions.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def polygon_area(points: np.ndarray) -> float:\n",
    "        \"\"\"Compute polygon area using Shoelace formula.\"\"\"\n",
    "        n = len(points)\n",
    "        area = 0.0\n",
    "        for i in range(n):\n",
    "            j = (i + 1) % n\n",
    "            area += points[i][0] * points[j][1]\n",
    "            area -= points[j][0] * points[i][1]\n",
    "        return abs(area) / 2.0\n",
    "\n",
    "    @staticmethod\n",
    "    def polygon_iou(poly1: np.ndarray, poly2: np.ndarray) -> float:\n",
    "        \"\"\"Compute Intersection over Union for two polygons.\"\"\"\n",
    "        all_pts = np.vstack([poly1, poly2])\n",
    "        x_min = max(0, int(np.floor(all_pts[:, 0].min())) - 5)\n",
    "        y_min = max(0, int(np.floor(all_pts[:, 1].min())) - 5)\n",
    "        x_max = int(np.ceil(all_pts[:, 0].max())) + 5\n",
    "        y_max = int(np.ceil(all_pts[:, 1].max())) + 5\n",
    "\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        if w <= 0 or h <= 0:\n",
    "            return 0.0\n",
    "\n",
    "        offset = np.array([x_min, y_min])\n",
    "        mask1 = np.zeros((h, w), dtype=np.uint8)\n",
    "        mask2 = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(mask1, [(poly1 - offset).astype(np.int32)], 1)\n",
    "        cv2.fillPoly(mask2, [(poly2 - offset).astype(np.int32)], 1)\n",
    "\n",
    "        intersection = np.sum(mask1 & mask2)\n",
    "        union = np.sum(mask1 | mask2)\n",
    "        return intersection / union if union > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwNAmpvBa434"
   },
   "outputs": [],
   "source": [
    "class BookDetector:\n",
    "    \"\"\"\n",
    "    Main book detection pipeline.\n",
    "\n",
    "    Pipeline:\n",
    "    1. Preprocess images (Gaussian blur + histogram equalization)\n",
    "    2. Extract RootSIFT features\n",
    "    3. Match features (BF 5NN consecutive ratio test)\n",
    "    4. Iterative detection with keypoint index exclusion\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.preprocessor = ImagePreprocessor()\n",
    "        self.feature_extractor = RootSIFT()\n",
    "        self.matcher = FeatureMatcher()\n",
    "        self.affine_estimator = AffineEstimator()\n",
    "        self.model_cache = {}\n",
    "\n",
    "    def load_model(self, model_path: str):\n",
    "        \"\"\"Load and cache model image features.\"\"\"\n",
    "        if model_path in self.model_cache:\n",
    "            return self.model_cache[model_path]\n",
    "\n",
    "        img = cv2.imread(model_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load: {model_path}\")\n",
    "\n",
    "        gray = self.preprocessor.preprocess(img)\n",
    "        kp, des = self.feature_extractor.detect_and_compute(gray)\n",
    "\n",
    "        self.model_cache[model_path] = (img, kp, des)\n",
    "        return img, kp, des\n",
    "\n",
    "    def detect_in_scene(\n",
    "        self, scene_path: str, model_paths: List[str], verbose: bool = False\n",
    "    ) -> Tuple[List[BookDetection], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Detect all books in a scene image.\n",
    "        \"\"\"\n",
    "        scene_img = cv2.imread(scene_path)\n",
    "        if scene_img is None:\n",
    "            raise ValueError(f\"Could not load: {scene_path}\")\n",
    "\n",
    "        scene_gray = self.preprocessor.preprocess(scene_img)\n",
    "        scene_kp, scene_des = self.feature_extractor.detect_and_compute(scene_gray)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Scene: {Path(scene_path).name} - {len(scene_kp)} keypoints\")\n",
    "\n",
    "        detections = []\n",
    "        result_img = scene_img.copy()\n",
    "\n",
    "        for book_id, model_path in enumerate(model_paths):\n",
    "            model_img, model_kp, model_des = self.load_model(model_path)\n",
    "\n",
    "            if model_des is None or len(model_kp) < Config.MIN_MATCH_COUNT:\n",
    "                detections.append(BookDetection(book_id, model_path, []))\n",
    "                continue\n",
    "\n",
    "            instances = self._detect_instances(\n",
    "                model_img,\n",
    "                model_kp,\n",
    "                model_des,\n",
    "                scene_img,\n",
    "                scene_kp,\n",
    "                scene_des,\n",
    "            )\n",
    "\n",
    "            detection = BookDetection(book_id, model_path, instances)\n",
    "            detections.append(detection)\n",
    "            self._draw_detection(result_img, detection)\n",
    "\n",
    "            if verbose and len(instances) > 0:\n",
    "                print(f\"  Book {book_id}: {len(instances)} instance(s)\")\n",
    "\n",
    "        return detections, result_img\n",
    "\n",
    "    def _detect_instances(\n",
    "        self, model_img, model_kp, model_des, scene_img, scene_kp, scene_des\n",
    "    ) -> List[BoundingBox]:\n",
    "        \"\"\"\n",
    "        Detect all instances using iterative similarity estimation\n",
    "        with keypoint index exclusion.\n",
    "        \"\"\"\n",
    "        instances = []\n",
    "        excluded_indices: Set[int] = set()\n",
    "\n",
    "        for iteration in range(Config.MAX_INSTANCES_PER_BOOK):\n",
    "            matches = self.matcher.match(model_des, scene_des, excluded_indices)\n",
    "\n",
    "            if len(matches) < Config.MIN_MATCH_COUNT:\n",
    "                break\n",
    "\n",
    "            src_pts = np.float32([model_kp[m.queryIdx].pt for m in matches])\n",
    "            dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in matches])\n",
    "            match_indices = [m.trainIdx for m in matches]\n",
    "\n",
    "            # Estimate similarity transformation\n",
    "            H, inlier_mask, n_inliers = self.affine_estimator.estimate(src_pts, dst_pts)\n",
    "\n",
    "            if H is None:\n",
    "                break\n",
    "\n",
    "            # Check inlier quality\n",
    "            inlier_ratio = n_inliers / len(matches)\n",
    "            if (\n",
    "                n_inliers < Config.MIN_INLIERS\n",
    "                or inlier_ratio < Config.MIN_INLIERS_RATIO\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            # Project model corners to scene\n",
    "            projected = self.affine_estimator.transform_corners(H, model_img.shape[:2])\n",
    "\n",
    "            is_valid, reason = is_rectangle_valid(projected, scene_img.shape)\n",
    "            if not is_valid:\n",
    "                break\n",
    "\n",
    "            # Combined extent + inlier check: low-extent detections need\n",
    "            # stronger inlier support to be trusted\n",
    "            rect_check = np.int32(projected).reshape(-1, 1, 2)\n",
    "            contour_area = cv2.contourArea(rect_check)\n",
    "            x_r, y_r, w_r, h_r = cv2.boundingRect(rect_check)\n",
    "            bb_area = w_r * h_r\n",
    "            extent = contour_area / bb_area if bb_area > 0 else 0\n",
    "            if (\n",
    "                extent < Config.LOW_EXTENT_THRESHOLD\n",
    "                and n_inliers < Config.MIN_INLIERS_LOW_EXTENT\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            # Compute area\n",
    "            area = GeometryUtils.polygon_area(projected)\n",
    "\n",
    "            bbox = BoundingBox(\n",
    "                top_left=tuple(map(int, projected[0])),\n",
    "                top_right=tuple(map(int, projected[1])),\n",
    "                bottom_right=tuple(map(int, projected[2])),\n",
    "                bottom_left=tuple(map(int, projected[3])),\n",
    "                area=int(area),\n",
    "                n_inliers=n_inliers,\n",
    "                inlier_ratio=inlier_ratio,\n",
    "            )\n",
    "\n",
    "            # Same-book overlap check: reject if >30% IoU with any\n",
    "            # previously detected instance of this same book\n",
    "            new_poly = bbox.get_polygon()\n",
    "            is_same_book_dup = any(\n",
    "                GeometryUtils.polygon_iou(new_poly, prev.get_polygon())\n",
    "                > Config.SAME_BOOK_IOU_THRESHOLD\n",
    "                for prev in instances\n",
    "            )\n",
    "            if is_same_book_dup:\n",
    "                # Exclude inliers so iteration can continue past this region\n",
    "                polygon = projected.astype(np.float32).reshape(-1, 1, 2)\n",
    "                for i in range(len(match_indices)):\n",
    "                    if inlier_mask[i]:\n",
    "                        kp_idx = match_indices[i]\n",
    "                        pt = scene_kp[kp_idx].pt\n",
    "                        if cv2.pointPolygonTest(polygon, pt, measureDist=True) >= 0:\n",
    "                            excluded_indices.add(kp_idx)\n",
    "                continue\n",
    "\n",
    "            instances.append(bbox)\n",
    "\n",
    "            # --- Spatial inlier filtering ---\n",
    "            # Only exclude inliers whose scene keypoint falls INSIDE\n",
    "            # the detected bounding box. This prevents RANSAC from claiming\n",
    "            # keypoints that physically sit on adjacent book copies.\n",
    "            polygon = projected.astype(np.float32).reshape(-1, 1, 2)\n",
    "            for i in range(len(match_indices)):\n",
    "                if inlier_mask[i]:\n",
    "                    kp_idx = match_indices[i]\n",
    "                    pt = scene_kp[kp_idx].pt\n",
    "                    # pointPolygonTest returns positive if inside, 0 on edge, negative if outside\n",
    "                    # Check to exclude only the inliers belonging to the instance just detected\n",
    "                    if cv2.pointPolygonTest(polygon, pt, measureDist=True) >= 0:\n",
    "                        excluded_indices.add(kp_idx)\n",
    "\n",
    "        return instances\n",
    "\n",
    "    def _draw_detection(self, img: np.ndarray, detection: BookDetection):\n",
    "        \"\"\"Draw bounding boxes on image.\"\"\"\n",
    "        colors = [\n",
    "            (0, 255, 0),\n",
    "            (255, 0, 0),\n",
    "            (0, 0, 255),\n",
    "            (255, 255, 0),\n",
    "            (255, 0, 255),\n",
    "            (0, 255, 255),\n",
    "            (128, 0, 255),\n",
    "            (255, 128, 0),\n",
    "        ]\n",
    "        color = colors[detection.book_id % len(colors)]\n",
    "\n",
    "        for inst in detection.instances:\n",
    "            pts = np.array(\n",
    "                [inst.top_left, inst.top_right, inst.bottom_right, inst.bottom_left],\n",
    "                dtype=np.int32,\n",
    "            )\n",
    "            cv2.polylines(img, [pts], True, color, 3)\n",
    "\n",
    "            label = f\"Book {detection.book_id}\"\n",
    "            pos = (inst.top_left[0], max(inst.top_left[1] - 10, 20))\n",
    "            cv2.putText(img, label, pos, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdInq2TIS5Zs"
   },
   "source": [
    "### Detection/Rejection Criteria\n",
    "\n",
    "Each candidate detection goes through a sequence of checks before being accepted. If any check fails, the iteration stops (`break`) or the detection is skipped (`continue`). The checks are applied in order, and each serves a distinct purpose:\n",
    "\n",
    "**1. Minimum match count** (`MIN_MATCH_COUNT = 3`) â€” (`break`)   \n",
    "Before attempting RANSAC, at least 3 matches are required. A similarity transform needs a minimum of 2 points, but 3 provides a basic sanity margin. Below this, there is not enough evidence to attempt geometric verification.\n",
    "\n",
    "**2. RANSAC failure** â€” (`break`)        \n",
    "If `estimateAffinePartial2D` returns `None`, the point correspondences are too noisy or contradictory for any consistent transformation. This typically means the remaining matches are scattered across unrelated scene regions.\n",
    "\n",
    "**3. Inlier count and ratio** (`MIN_INLIERS = 3`, `MIN_INLIERS_RATIO = 1/3`) â€” (`break`)    \n",
    "The number of RANSAC inliers must reach at least 3, and the inlier ratio (inliers / total matches) must be at least 33%. These ensure that the estimated transformation has sufficient geometric consensus. A low inlier ratio indicates the matches are dominated by outliers.\n",
    "\n",
    "**4. Rectangle validity** (`is_rectangle_valid`) â€” (`break`)  \n",
    "Function `is_rectangle_valid` previously presented in detail.\n",
    "\n",
    "**5. Combined extent + inlier check** (`LOW_EXTENT_THRESHOLD = 0.65`, `MIN_INLIERS_LOW_EXTENT = 5`) â€” (`break`)  \n",
    "A detection with moderately low extent (between 0.5 and 0.65) is ambiguous: it could be a slightly rotated true detection or a poorly constrained false positive. To distinguish between these cases, we require stronger inlier support. If extent < 0.65 **and** inliers < 5, the detection is rejected. This allows well-supported detections with slight skew to pass while filtering out weakly-supported ones (see next section).\n",
    "\n",
    "**6. Same-book IoU overlap** (`SAME_BOOK_IOU_THRESHOLD = 0.30`) â€” (`continue`)\n",
    "After all geometric checks pass, the new bounding box is compared against all previously accepted instances **of the same book model** using Intersection over Union (IoU). If the overlap exceeds 30%, the detection is a duplicate â€” RANSAC converged on the same physical book again. Unlike the previous checks, this uses `continue` instead of `break`: the inliers are excluded (via spatial filtering) and the loop tries the next iteration, since valid non-overlapping instances may still exist.\n",
    "\n",
    "### Combined Extent + Inlier Check\n",
    "\n",
    "A simple `MIN_EXTENT` threshold creates a dilemma:\n",
    "\n",
    "| Case | Extent | Inliers | Verdict |\n",
    "|------|--------|---------|--------|\n",
    "| Scene 4, Book 15 iter 2 (false positive) | 0.620 | 3 | Should reject |\n",
    "| Scene 16, Book 12 iter 2 (true positive) | 0.601 | 10 (100%) | Should accept |\n",
    "\n",
    "Both have similar extent, so no single threshold can separate them. However, the key difference is **inlier support**: the false positive has only 3 inliers (the minimum), while the true positive has 10 with 100% inlier ratio â€” strong geometric consensus.\n",
    "\n",
    "The solution is a **combined check**: detections with low extent (below `LOW_EXTENT_THRESHOLD = 0.65`) are only accepted if they have sufficient inlier support (`MIN_INLIERS_LOW_EXTENT = 5`). This rejects poorly-supported skewed detections while preserving well-supported ones that happen to be slightly rotated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqRqlTPda436"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of two folders of images:\n",
    "* **Models**: contains one reference image for each product that the system should be able to identify;\n",
    "* **Scenes**: contains different shelve pictures to test the developed algorithm in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_NAME: str = \"dataset/assignment1\"\n",
    "REPO_URL: str = f\"https://github.com/{PROJECT_REPO}.git\"\n",
    "\n",
    "temp_dir: Path = PROJECT_ROOT / \"temp_repo\"\n",
    "dataset_path: Path = PROJECT_ROOT / \"dataset\"\n",
    "\n",
    "if dataset_path.exists():\n",
    "    print(f\"'{dataset_path.name}' folder already exists locally. Skipping download.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\n",
    "            f\"Downloading dataset at {PROJECT_REPO}/{BRANCH_NAME} via git sparse checkout...\"\n",
    "        )\n",
    "\n",
    "        # Clone the repo tree\n",
    "        clone_cmd = [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"--filter=blob:none\",\n",
    "            \"--sparse\",\n",
    "            \"--depth\",\n",
    "            \"1\",\n",
    "            \"--branch\",\n",
    "            BRANCH_NAME,\n",
    "            REPO_URL,\n",
    "            str(temp_dir),\n",
    "        ]\n",
    "        subprocess.run(clone_cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "        # Fetch the contents of the 'dataset' folder\n",
    "        sparse_cmd = [\"git\", \"-C\", str(temp_dir), \"sparse-checkout\", \"set\", \"dataset\"]\n",
    "        subprocess.run(sparse_cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "        source_dataset_path: Path = temp_dir / \"dataset\"\n",
    "\n",
    "        if source_dataset_path.exists():\n",
    "            shutil.move(source_dataset_path, dataset_path)\n",
    "            print(\"Dataset successfully downloaded.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Error: Could not find the 'dataset' folder inside the cloned repo at '{temp_dir}'.\"\n",
    "            )\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Git command failed: {e.stderr}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if temp_dir.exists():\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH: Path = dataset_path / \"models\"\n",
    "SCENES_PATH: Path = dataset_path / \"scenes\"\n",
    "\n",
    "model_files = sorted(\n",
    "    MODELS_PATH.glob(\"model_*.png\"), key=lambda x: int(x.stem.split(\"_\")[1])\n",
    ")\n",
    "scene_files = sorted(\n",
    "    SCENES_PATH.glob(\"scene_*.jpg\"), key=lambda x: int(x.stem.split(\"_\")[1])\n",
    ")\n",
    "\n",
    "model_paths = [str(f) for f in model_files]\n",
    "scene_paths = [str(f) for f in scene_files]\n",
    "\n",
    "print(f\"Found {len(model_paths)} models and {len(scene_paths)} scenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwschBcQa436"
   },
   "source": [
    "## Run Full Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-DiVBzTa437",
    "outputId": "3eae762e-f05d-461a-9583-cdf1bd052d20"
   },
   "outputs": [],
   "source": [
    "# Initialize detector\n",
    "detector = BookDetector()\n",
    "\n",
    "# Process all scenes\n",
    "all_results = {}\n",
    "all_images = {}\n",
    "\n",
    "for idx, scene_path in enumerate(scene_paths):\n",
    "    print(f\"\\nProcessing scene {idx}: {Path(scene_path).name}\")\n",
    "\n",
    "    detections, result_img = detector.detect_in_scene(\n",
    "        scene_path, model_paths, verbose=False\n",
    "    )\n",
    "\n",
    "    all_results[idx] = detections\n",
    "    all_images[idx] = result_img\n",
    "\n",
    "    total = sum(len(d.instances) for d in detections)\n",
    "    books = sum(1 for d in detections if len(d.instances) > 0)\n",
    "    print(f\"  Found {total} instances of {books} books\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqeY2C4Ua437"
   },
   "source": [
    "## Formatted Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(detections: List[BookDetection]) -> str:\n",
    "    \"\"\"\n",
    "    Format detection results as specified in the assignment.\n",
    "\n",
    "    Output format:\n",
    "    Book X - N instance(s) found:\n",
    "      Instance 1 {top_left: (x,y), top_right: (x,y), ...}\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for det in detections:\n",
    "        if len(det.instances) > 0:\n",
    "            lines.append(\n",
    "                f\"Book {det.book_id} - {len(det.instances)} instance(s) found:\"\n",
    "            )\n",
    "\n",
    "            for i, inst in enumerate(det.instances, 1):\n",
    "                lines.append(\n",
    "                    f\"  Instance {i} {{\"\n",
    "                    f\"top_left: {inst.top_left}, \"\n",
    "                    f\"top_right: {inst.top_right}, \"\n",
    "                    f\"bottom_left: {inst.bottom_left}, \"\n",
    "                    f\"bottom_right: {inst.bottom_right}, \"\n",
    "                    f\"area: {inst.area}px}}\"\n",
    "                )\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeaioN5Ga437",
    "outputId": "e62b7c08-c1bc-48a8-b695-bee16eb37693"
   },
   "outputs": [],
   "source": [
    "# Print formatted results\n",
    "for idx, detections in all_results.items():\n",
    "    detected = [d for d in detections if len(d.instances) > 0]\n",
    "    if detected:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"SCENE: {Path(scene_paths[idx]).name}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(format_output(detected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBu8HcWXa438"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scene_with_models(scene_idx, scene_path, detections, model_paths):\n",
    "    \"\"\"\n",
    "    Visualize detection results: scene with bounding boxes on the left,\n",
    "    detected model images with matching colored borders on the right.\n",
    "    One model image per book, with instance count in the label.\n",
    "\n",
    "    Args:\n",
    "        scene_idx: Scene index (for title)\n",
    "        scene_path: Path to scene image\n",
    "        detections: List of BookDetection for this scene\n",
    "        model_paths: List of all model image paths\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    colors_bgr = [\n",
    "        (0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0),\n",
    "        (255, 0, 255), (0, 255, 255), (128, 0, 255), (255, 128, 0)\n",
    "    ]\n",
    "\n",
    "    def bgr_to_rgb_norm(bgr):\n",
    "        return (bgr[2] / 255, bgr[1] / 255, bgr[0] / 255)\n",
    "\n",
    "    # Collect detected books (those with at least one instance)\n",
    "    detected = [(d, colors_bgr[d.book_id % len(colors_bgr)])\n",
    "                for d in detections if len(d.instances) > 0]\n",
    "\n",
    "    # One entry per book (not per instance)\n",
    "    model_entries = [(d.book_id, d.model_path, color, len(d.instances))\n",
    "                     for d, color in detected]\n",
    "\n",
    "    n_models = len(model_entries)\n",
    "    if n_models == 0:\n",
    "        scene_img = cv2.cvtColor(cv2.imread(scene_path), cv2.COLOR_BGR2RGB)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax.imshow(scene_img)\n",
    "        ax.set_title(f\"Scene {scene_idx} â€” No detections\", fontsize=14)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    # Build the scene image with bounding boxes\n",
    "    scene_img = cv2.imread(scene_path)\n",
    "    result_img = scene_img.copy()\n",
    "    for d, color in detected:\n",
    "        for inst in d.instances:\n",
    "            pts = np.array([inst.top_left, inst.top_right,\n",
    "                           inst.bottom_right, inst.bottom_left], dtype=np.int32)\n",
    "            cv2.polylines(result_img, [pts], True, color, 3)\n",
    "            label = f\"Book {d.book_id}\"\n",
    "            pos = (inst.top_left[0], max(inst.top_left[1] - 10, 20))\n",
    "            cv2.putText(result_img, label, pos, cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.6, color, 2)\n",
    "    result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Layout: scene on left, one model per detected book on right\n",
    "    n_cols = 1 + n_models\n",
    "    width_ratios = [4] + [1] * n_models\n",
    "    fig_width = 6 + 1.5 * n_models\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(fig_width, 6),\n",
    "                              gridspec_kw={'width_ratios': width_ratios})\n",
    "    if n_cols == 2:\n",
    "        axes = [axes[0], axes[1]]\n",
    "\n",
    "    # Scene\n",
    "    axes[0].imshow(result_img)\n",
    "    axes[0].set_title(f\"Scene {scene_idx}\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Model images\n",
    "    for idx, (book_id, model_path, color, n_instances) in enumerate(model_entries):\n",
    "        ax = axes[1 + idx]\n",
    "        model_img = cv2.cvtColor(cv2.imread(model_path), cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(model_img)\n",
    "\n",
    "        rgb_color = bgr_to_rgb_norm(color)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(rgb_color)\n",
    "            spine.set_linewidth(4)\n",
    "            spine.set_visible(True)\n",
    "\n",
    "        label = f\"Model {book_id}\"\n",
    "        if n_instances > 1:\n",
    "            label += f\"\\n({n_instances} inst.)\"\n",
    "        ax.set_title(label, fontsize=10, color=rgb_color, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all scenes\n",
    "for scene_idx, scene_path in enumerate(scene_paths):\n",
    "    visualize_scene_with_models(\n",
    "        scene_idx, scene_path,\n",
    "        all_results[scene_idx], model_paths\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If9_jABCa438"
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIILjqOia438",
    "outputId": "8157c593-f3cd-410c-d434-844b6cb72811"
   },
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "total_detections = 0\n",
    "detections_per_scene = []\n",
    "detections_per_book = defaultdict(int)\n",
    "\n",
    "for detections in all_results.values():\n",
    "    scene_total = sum(len(d.instances) for d in detections)\n",
    "    detections_per_scene.append(scene_total)\n",
    "    total_detections += scene_total\n",
    "\n",
    "    for d in detections:\n",
    "        if len(d.instances) > 0:\n",
    "            detections_per_book[d.book_id] += len(d.instances)\n",
    "\n",
    "print(\"DETECTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total detections: {total_detections}\")\n",
    "print(f\"Average per scene: {np.mean(detections_per_scene):.2f}\")\n",
    "print(f\"Max in one scene: {max(detections_per_scene)}\")\n",
    "print(f\"\\nUnique books detected: {len(detections_per_book)}\")\n",
    "\n",
    "print(\"\\nTop 10 most detected books:\")\n",
    "for book_id, count in sorted(detections_per_book.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  Book {book_id}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeoyS9KOa439"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This pipeline implements book detection using traditional computer vision:\n",
    "\n",
    "1. **RootSIFT features** for robust descriptor matching\n",
    "2. **BF 5NN consecutive ratio test** for feature matching\n",
    "3. **Similarity transform** (4 DOF via `estimateAffinePartial2D`) for geometric verification\n",
    "4. **Iterative keypoint exclusion** for multi-instance detection"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nico_assignment_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
