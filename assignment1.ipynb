{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzfT6d2t_hcE"
   },
   "source": [
    "# **Product Recognition of Books**\n",
    "\n",
    "## Image Processing and Computer Vision - Assignment Module #1\n",
    "\n",
    "---\n",
    "\n",
    "## Approach Overview\n",
    "\n",
    "This solution implements a **traditional computer vision pipeline** for detecting books on shelves.\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "After extensive experimentation, we adopted a **simplified pipeline** based on the following observations about our dataset:\n",
    "\n",
    "1. **Books are nearly planar**: Shelf images have minimal perspective distortion\n",
    "2. **Books are upright**: Spines are vertical with minimal rotation\n",
    "3. **Scale is consistent**: Model and scene images have similar scale\n",
    "4. **Multiple copies are adjacent**: Same book appears side-by-side on shelves\n",
    "\n",
    "These characteristics led us to choose:\n",
    "\n",
    "| Component | Choice | Justification |\n",
    "|-----------|--------|---------------|\n",
    "| Features | RootSIFT | ~15-30% better matching than standard SIFT |\n",
    "| Matching | 5NN | Allows one model keypoint to match multiple scene locations |\n",
    "| Geometric model | **Affine** (not Homography) | 6 DOF sufficient for upright books, more stable with fewer points |\n",
    "| Multi-instance | Iterative masking | Simpler and more effective than clustering approaches |\n",
    "\n",
    "### Why Affine Instead of Homography?\n",
    "\n",
    "- **Homography**: 8 degrees of freedom, requires 4+ point correspondences, typically needs 8+ for robustness\n",
    "- **Affine**: 6 degrees of freedom, requires only 3 point correspondences, more stable with limited matches\n",
    "\n",
    "For upright books with minimal perspective distortion, the extra 2 DOF of homography add noise without benefit. Our experiments showed that with ~8-10 matches per book instance, affine transformation provides more reliable detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXZzVgAL_hcK"
   },
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mak8OeZ1_hcL"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Set\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r01GuR_3_hcO"
   },
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All parameters are centralized here with justifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj6NYy2n_hcO"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration parameters for the detection pipeline.\n",
    "\n",
    "    Each parameter is justified based on dataset characteristics\n",
    "    and experimental results.\n",
    "    \"\"\"\n",
    "\n",
    "    # === PREPROCESSING ===\n",
    "    # CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    # Normalizes lighting variations across shelf images\n",
    "    CLAHE_CLIP_LIMIT = 2.0   # Moderate contrast enhancement\n",
    "    CLAHE_GRID_SIZE = (4, 4) # 4x4 provides finer local adaptation than 8x8\n",
    "    # Justification: Experiments showed fixed CLAHE outperformed adaptive selection\n",
    "\n",
    "    # === FEATURE DETECTION ===\n",
    "    # SIFT parameters (used via RootSIFT)\n",
    "    SIFT_FEATURES = 0                # 0 = detect all features\n",
    "    SIFT_CONTRAST_THRESHOLD = 0.015  # 0.04  # Default value, works well for book spines\n",
    "    SIFT_EDGE_THRESHOLD = 15         # 10       # Default value\n",
    "\n",
    "    # === FEATURE MATCHING ===\n",
    "    # 5NN matching strategy for multi-instance detection\n",
    "    # Justification: Unlike 2NN ratio test, 5NN allows one model keypoint\n",
    "    # to match multiple scene locations (essential for detecting book copies)\n",
    "    DISTANCE_THRESHOLD = 0.7  # Relative quality threshold\n",
    "    OUTLIER_RATIO = 1.5       # Filter matches far from the group\n",
    "\n",
    "    # === GEOMETRIC VERIFICATION ===\n",
    "    # Affine transformation with RANSAC\n",
    "    # Justification: Affine (6 DOF) is sufficient for upright books\n",
    "    # and more stable than homography (8 DOF) with limited matches\n",
    "    MIN_MATCH_COUNT = 6             # Minimum matches to attempt affine estimation\n",
    "                                    # (affine needs 3 points, we require 6 for robustness)\n",
    "    RANSAC_REPROJ_THRESHOLD = 4.0   # 5.0  # Pixels - allows for small localization errors\n",
    "\n",
    "    # === DETECTION VALIDATION ===\n",
    "    MIN_INLIERS = 6           # Minimum inliers for valid detection\n",
    "    MIN_INLIERS_RATIO = 0.25  # At least 25% of matches should be inliers\n",
    "    MIN_AREA = 1000           # Minimum bounding box area (pixels)\n",
    "    MAX_AREA_RATIO = 4        #  20       # Max ratio: detected_area / model_area\n",
    "    MIN_AREA_RATIO = 0.2      #  0.05     # Min ratio: detected_area / model_area\n",
    "\n",
    "    # === MULTI-INSTANCE DETECTION ===\n",
    "    MAX_INSTANCES_PER_BOOK = 10  # Safety limit\n",
    "    IOU_THRESHOLD = 0.3          # Overlap threshold for duplicate removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YWnhLAG_hcP"
   },
   "source": [
    "## 3. Data Classes\n",
    "\n",
    "Structured representation of detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FELmAZp0_hcP"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoundingBox:\n",
    "    \"\"\"\n",
    "    Represents a detected book instance.\n",
    "\n",
    "    Stores the four corners of the bounding quadrilateral,\n",
    "    which may not be axis-aligned due to the affine transformation.\n",
    "    \"\"\"\n",
    "    top_left: Tuple[int, int]\n",
    "    top_right: Tuple[int, int]\n",
    "    bottom_right: Tuple[int, int]\n",
    "    bottom_left: Tuple[int, int]\n",
    "    area: int\n",
    "    n_inliers: int\n",
    "    inlier_ratio: float\n",
    "\n",
    "    def get_polygon(self) -> np.ndarray:\n",
    "        \"\"\"Return corners as numpy array for geometric operations.\"\"\"\n",
    "        return np.array([self.top_left, self.top_right,\n",
    "                        self.bottom_right, self.bottom_left], dtype=np.float32)\n",
    "\n",
    "@dataclass\n",
    "class BookDetection:\n",
    "    \"\"\"All detections for a single book model in a scene.\"\"\"\n",
    "    book_id: int\n",
    "    model_path: str\n",
    "    instances: List[BoundingBox]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPps9Xt4_hcQ"
   },
   "source": [
    "## 4. RootSIFT Feature Extractor\n",
    "\n",
    "### Why RootSIFT?\n",
    "\n",
    "Standard SIFT descriptors are compared using Euclidean distance. However, SIFT descriptors are histograms of gradient orientations, and the **Hellinger kernel** (Bhattacharyya distance) is more appropriate for histogram comparison.\n",
    "\n",
    "RootSIFT achieves this by:\n",
    "1. L1-normalizing the SIFT descriptor\n",
    "2. Taking the element-wise square root\n",
    "\n",
    "This allows Euclidean distance to implicitly compute Hellinger distance, providing **~15-30% better matching accuracy**.\n",
    "\n",
    "**Reference**: Arandjelović & Zisserman, \"Three things everyone should know to improve object retrieval\" (CVPR 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdy0nb3s_hcR"
   },
   "outputs": [],
   "source": [
    "class RootSIFT:\n",
    "    \"\"\"\n",
    "    RootSIFT feature extractor.\n",
    "\n",
    "    Enhances SIFT descriptors for better matching performance\n",
    "    by applying L1 normalization followed by square root.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sift = cv2.SIFT_create(\n",
    "            nfeatures=Config.SIFT_FEATURES,\n",
    "            contrastThreshold=Config.SIFT_CONTRAST_THRESHOLD,\n",
    "            edgeThreshold=Config.SIFT_EDGE_THRESHOLD\n",
    "        )\n",
    "\n",
    "    def detect_and_compute(self, image: np.ndarray):\n",
    "        \"\"\"\n",
    "        Detect keypoints and compute RootSIFT descriptors.\n",
    "\n",
    "        Args:\n",
    "            image: Grayscale input image\n",
    "\n",
    "        Returns:\n",
    "            keypoints: List of cv2.KeyPoint\n",
    "            descriptors: RootSIFT descriptors (Nx128 float32 array)\n",
    "        \"\"\"\n",
    "        keypoints, descriptors = self.sift.detectAndCompute(image, None)\n",
    "\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            return keypoints, None\n",
    "\n",
    "        # Convert to RootSIFT\n",
    "        eps = 1e-7\n",
    "\n",
    "        # Step 1: L1 normalize\n",
    "        descriptors = descriptors / (np.sum(descriptors, axis=1, keepdims=True) + eps)\n",
    "\n",
    "        # Step 2: Square root (Hellinger kernel)\n",
    "        descriptors = np.sqrt(descriptors)\n",
    "\n",
    "        return keypoints, descriptors.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhSgnmpL_hcS"
   },
   "source": [
    "## 5. Image Preprocessing\n",
    "\n",
    "### Why CLAHE?\n",
    "\n",
    "Shelf images have varying lighting conditions (shadows, reflections, uneven illumination). **CLAHE** (Contrast Limited Adaptive Histogram Equalization) normalizes local contrast while preventing over-amplification of noise.\n",
    "\n",
    "### Parameter Choices\n",
    "\n",
    "- **clipLimit = 2.0**: Moderate enhancement; higher values can amplify noise\n",
    "- **tileGridSize = (4, 4)**: Finer grid provides better local adaptation for book spines\n",
    "\n",
    "We use **fixed parameters** rather than adaptive selection because experiments showed that adaptive approaches (varying CLAHE based on image brightness) performed worse overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aSWkBNo_hcS"
   },
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline for lighting normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.clahe = cv2.createCLAHE(\n",
    "            clipLimit=Config.CLAHE_CLIP_LIMIT,\n",
    "            tileGridSize=Config.CLAHE_GRID_SIZE\n",
    "        )\n",
    "\n",
    "    def preprocess(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert to grayscale and apply CLAHE.\n",
    "\n",
    "        Args:\n",
    "            image: BGR input image\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed grayscale image\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image.copy()\n",
    "\n",
    "        return self.clahe.apply(gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XEy3ne9_hcT"
   },
   "source": [
    "## 6. Feature Matching with 5NN\n",
    "\n",
    "### Why 5NN Instead of Standard Ratio Test?\n",
    "\n",
    "The standard approach (Lowe's ratio test with k=2) assumes each model keypoint matches **at most one** scene location. This fails for **multi-instance detection** where the same book appears multiple times.\n",
    "\n",
    "**5NN strategy**:\n",
    "1. For each model keypoint, find 5 nearest neighbors in the scene\n",
    "2. Keep matches that are significantly better than the worst in the group\n",
    "3. This allows one model keypoint to match multiple scene locations\n",
    "\n",
    "### Filtering Criteria\n",
    "\n",
    "- **DISTANCE_THRESHOLD = 0.7**: Best match must be within 70% of median distance\n",
    "- **OUTLIER_RATIO = 1.5**: Keep matches within (max_distance / 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B--b2RsJ_hcT"
   },
   "outputs": [],
   "source": [
    "class FeatureMatcher:\n",
    "    \"\"\"\n",
    "    5NN feature matcher for multi-instance detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # FLANN matcher for efficient approximate nearest neighbor search\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=100)\n",
    "        self.flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    def match(self, des_model: np.ndarray, des_scene: np.ndarray,\n",
    "              excluded_indices: Set[int] = None) -> List[cv2.DMatch]:\n",
    "        \"\"\"\n",
    "        Match model descriptors to scene descriptors using 5NN.\n",
    "\n",
    "        Args:\n",
    "            des_model: Model descriptors\n",
    "            des_scene: Scene descriptors\n",
    "            excluded_indices: Scene keypoint indices to exclude (already matched)\n",
    "\n",
    "        Returns:\n",
    "            List of good matches\n",
    "        \"\"\"\n",
    "        if des_model is None or des_scene is None:\n",
    "            return []\n",
    "        if len(des_model) < 2 or len(des_scene) < 5:\n",
    "            return []\n",
    "\n",
    "        if excluded_indices is None:\n",
    "            excluded_indices = set()\n",
    "\n",
    "        try:\n",
    "            matches = self.flann.knnMatch(des_model, des_scene, k=5)\n",
    "        except cv2.error:\n",
    "            return []\n",
    "\n",
    "        good_matches = []\n",
    "\n",
    "        for match_group in matches:\n",
    "            # Filter excluded indices\n",
    "            valid = [m for m in match_group if m.trainIdx not in excluded_indices]\n",
    "\n",
    "            if len(valid) < 2:\n",
    "                continue\n",
    "\n",
    "            distances = [m.distance for m in valid]\n",
    "            min_dist = distances[0]\n",
    "            median_dist = distances[min(2, len(distances) - 1)]\n",
    "            max_dist = distances[-1]\n",
    "\n",
    "            # Quality check: best match should be good relative to median\n",
    "            if median_dist > 0 and min_dist > Config.DISTANCE_THRESHOLD * median_dist:\n",
    "                continue\n",
    "\n",
    "            # Keep matches significantly better than worst\n",
    "            threshold = max_dist / Config.OUTLIER_RATIO\n",
    "            for m in valid:\n",
    "                if m.distance <= threshold:\n",
    "                    good_matches.append(m)\n",
    "\n",
    "        return good_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA27HZ93_hcU"
   },
   "source": [
    "## 7. Geometry Utilities\n",
    "\n",
    "Helper functions for geometric validation and IoU computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_vFoEQH_hcU"
   },
   "outputs": [],
   "source": [
    "class GeometryUtils:\n",
    "    \"\"\"Geometric utility functions.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def polygon_area(points: np.ndarray) -> float:\n",
    "        \"\"\"Compute polygon area using Shoelace formula.\"\"\"\n",
    "        n = len(points)\n",
    "        area = 0.0\n",
    "        for i in range(n):\n",
    "            j = (i + 1) % n\n",
    "            area += points[i][0] * points[j][1]\n",
    "            area -= points[j][0] * points[i][1]\n",
    "        return abs(area) / 2.0\n",
    "\n",
    "    @staticmethod\n",
    "    def is_convex(points: np.ndarray) -> bool:\n",
    "        \"\"\"Check if quadrilateral is convex using cross product signs.\"\"\"\n",
    "        n = len(points)\n",
    "        if n != 4:\n",
    "            return False\n",
    "\n",
    "        sign = None\n",
    "        for i in range(n):\n",
    "            p1, p2, p3 = points[i], points[(i+1)%n], points[(i+2)%n]\n",
    "            cross = (p2[0]-p1[0]) * (p3[1]-p2[1]) - (p2[1]-p1[1]) * (p3[0]-p2[0])\n",
    "\n",
    "            if abs(cross) < 1e-6:\n",
    "                continue\n",
    "            if sign is None:\n",
    "                sign = cross > 0\n",
    "            elif (cross > 0) != sign:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def polygon_iou(poly1: np.ndarray, poly2: np.ndarray) -> float:\n",
    "        \"\"\"Compute Intersection over Union for two polygons.\"\"\"\n",
    "        # Find bounding region\n",
    "        all_pts = np.vstack([poly1, poly2])\n",
    "        x_min, y_min = np.floor(all_pts.min(axis=0)).astype(int) - 10\n",
    "        x_max, y_max = np.ceil(all_pts.max(axis=0)).astype(int) + 10\n",
    "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
    "\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        if w <= 0 or h <= 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Create masks\n",
    "        offset = np.array([x_min, y_min])\n",
    "        mask1 = np.zeros((h, w), dtype=np.uint8)\n",
    "        mask2 = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(mask1, [(poly1 - offset).astype(np.int32)], 1)\n",
    "        cv2.fillPoly(mask2, [(poly2 - offset).astype(np.int32)], 1)\n",
    "\n",
    "        intersection = np.sum(mask1 & mask2)\n",
    "        union = np.sum(mask1 | mask2)\n",
    "\n",
    "        return intersection / union if union > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqALpA4a_hcU"
   },
   "source": [
    "## 8. Affine Transformation Estimator\n",
    "\n",
    "### Why Affine Instead of Homography?\n",
    "\n",
    "| Property | Homography | Affine |\n",
    "|----------|------------|--------|\n",
    "| Degrees of freedom | 8 | 6 |\n",
    "| Minimum points | 4 | 3 |\n",
    "| Handles perspective | Yes | No |\n",
    "| Stability with few points | Lower | Higher |\n",
    "\n",
    "For our dataset:\n",
    "- Books are **upright** on shelves (minimal rotation)\n",
    "- Camera view is **nearly frontal** (minimal perspective)\n",
    "- We often have only **6-10 matches** per instance\n",
    "\n",
    "Affine transformation preserves:\n",
    "- Parallel lines (book edges remain parallel)\n",
    "- Ratios of distances along lines\n",
    "\n",
    "This is sufficient for our use case and provides more stable estimation with limited correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPmtYqfg_hcU"
   },
   "outputs": [],
   "source": [
    "class AffineEstimator:\n",
    "    \"\"\"\n",
    "    Affine transformation estimator using RANSAC.\n",
    "\n",
    "    More stable than homography for our nearly-planar book images.\n",
    "    \"\"\"\n",
    "\n",
    "    def estimate(self, src_pts: np.ndarray, dst_pts: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], int]:\n",
    "        \"\"\"\n",
    "        Estimate affine transformation using RANSAC.\n",
    "\n",
    "        Args:\n",
    "            src_pts: Source points (model) - Nx2 array\n",
    "            dst_pts: Destination points (scene) - Nx2 array\n",
    "\n",
    "        Returns:\n",
    "            (affine_matrix, inlier_mask, n_inliers)\n",
    "            affine_matrix is 2x3 for cv2.warpAffine\n",
    "        \"\"\"\n",
    "        if len(src_pts) < 3:\n",
    "            return None, None, 0\n",
    "\n",
    "        try:\n",
    "            # cv2.estimateAffine2D returns 2x3 matrix and inlier mask\n",
    "            M, inliers = cv2.estimateAffine2D(\n",
    "                src_pts.reshape(-1, 1, 2),\n",
    "                dst_pts.reshape(-1, 1, 2),\n",
    "                method=cv2.RANSAC,\n",
    "                ransacReprojThreshold=Config.RANSAC_REPROJ_THRESHOLD,\n",
    "                maxIters=2000,\n",
    "                confidence=0.99\n",
    "            )\n",
    "\n",
    "            if M is None or inliers is None:\n",
    "                return None, None, 0\n",
    "\n",
    "            n_inliers = int(np.sum(inliers))\n",
    "            return M, inliers, n_inliers\n",
    "\n",
    "        except cv2.error:\n",
    "            return None, None, 0\n",
    "\n",
    "    def transform_points(self, M: np.ndarray, points: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply affine transformation to points.\n",
    "\n",
    "        Args:\n",
    "            M: 2x3 affine matrix\n",
    "            points: Nx2 array of points\n",
    "\n",
    "        Returns:\n",
    "            Transformed points (Nx2)\n",
    "        \"\"\"\n",
    "        # Convert to homogeneous coordinates\n",
    "        ones = np.ones((len(points), 1))\n",
    "        pts_h = np.hstack([points, ones])  # Nx3\n",
    "\n",
    "        # Apply transformation: [x', y'] = M @ [x, y, 1]^T\n",
    "        transformed = pts_h @ M.T  # Nx2\n",
    "\n",
    "        return transformed\n",
    "\n",
    "    def validate(self, M: np.ndarray, model_shape: Tuple[int, int],\n",
    "                 scene_shape: Tuple[int, int]) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if the affine transformation produces a reasonable bounding box.\n",
    "\n",
    "        Checks:\n",
    "        1. Result is convex quadrilateral\n",
    "        2. Area is within reasonable bounds\n",
    "        3. Box is within scene (with margin)\n",
    "        \"\"\"\n",
    "        if M is None:\n",
    "            return False\n",
    "\n",
    "        # Get model corners\n",
    "        h, w = model_shape\n",
    "        corners = np.array([[0, 0], [w, 0], [w, h], [0, h]], dtype=np.float32)\n",
    "\n",
    "        # Transform to scene\n",
    "        projected = self.transform_points(M, corners)\n",
    "\n",
    "        # Check convexity\n",
    "        if not GeometryUtils.is_convex(projected):\n",
    "            return False\n",
    "\n",
    "        # Check bounds (allow some margin outside scene)\n",
    "        scene_h, scene_w = scene_shape\n",
    "        margin = max(scene_h, scene_w) * 0.2\n",
    "\n",
    "        if (np.any(projected < -margin) or\n",
    "            np.any(projected[:, 0] > scene_w + margin) or\n",
    "            np.any(projected[:, 1] > scene_h + margin)):\n",
    "            return False\n",
    "\n",
    "        # Check area ratio\n",
    "        model_area = h * w\n",
    "        detected_area = GeometryUtils.polygon_area(projected)\n",
    "\n",
    "        if detected_area < Config.MIN_AREA:\n",
    "            return False\n",
    "\n",
    "        ratio = detected_area / model_area\n",
    "        if ratio < Config.MIN_AREA_RATIO or ratio > Config.MAX_AREA_RATIO:\n",
    "            return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d21FqDVN_hcV"
   },
   "source": [
    "## 9. Book Detector\n",
    "\n",
    "### Multi-Instance Detection Strategy\n",
    "\n",
    "We use **iterative detection with masking**:\n",
    "\n",
    "1. Match all model keypoints to scene\n",
    "2. Estimate affine transformation (RANSAC)\n",
    "3. If valid: save detection, mark inlier keypoints as used\n",
    "4. Repeat with remaining keypoints until no more valid detections\n",
    "\n",
    "### Why Not Clustering?\n",
    "\n",
    "We experimented with clustering approaches:\n",
    "- **Scene-location DBSCAN**: Failed because adjacent books have nearby keypoints\n",
    "- **Translation-voting DBSCAN**: Fragmented instances into too-small clusters\n",
    "\n",
    "Simple iterative masking proved most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdhPuzsS_hcV"
   },
   "outputs": [],
   "source": [
    "class BookDetector:\n",
    "    \"\"\"\n",
    "    Main book detection pipeline.\n",
    "\n",
    "    Pipeline:\n",
    "    1. Preprocess images (CLAHE)\n",
    "    2. Extract RootSIFT features\n",
    "    3. Match features (5NN)\n",
    "    4. Iteratively detect instances (Affine + RANSAC + Masking)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.preprocessor = ImagePreprocessor()\n",
    "        self.feature_extractor = RootSIFT()\n",
    "        self.matcher = FeatureMatcher()\n",
    "        self.affine_estimator = AffineEstimator()\n",
    "        self.model_cache = {}  # Cache model features\n",
    "\n",
    "    def load_model(self, model_path: str):\n",
    "        \"\"\"Load and cache model image features.\"\"\"\n",
    "        if model_path in self.model_cache:\n",
    "            return self.model_cache[model_path]\n",
    "\n",
    "        img = cv2.imread(model_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load: {model_path}\")\n",
    "\n",
    "        gray = self.preprocessor.preprocess(img)\n",
    "        kp, des = self.feature_extractor.detect_and_compute(gray)\n",
    "\n",
    "        self.model_cache[model_path] = (img, kp, des)\n",
    "        return img, kp, des\n",
    "\n",
    "    def detect_in_scene(self, scene_path: str, model_paths: List[str],\n",
    "                        verbose: bool = False) -> Tuple[List[BookDetection], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Detect all books in a scene image.\n",
    "\n",
    "        Args:\n",
    "            scene_path: Path to scene image\n",
    "            model_paths: List of paths to model images\n",
    "            verbose: Print progress information\n",
    "\n",
    "        Returns:\n",
    "            (list of BookDetection, annotated image)\n",
    "        \"\"\"\n",
    "        # Load and preprocess scene\n",
    "        scene_img = cv2.imread(scene_path)\n",
    "        if scene_img is None:\n",
    "            raise ValueError(f\"Could not load: {scene_path}\")\n",
    "\n",
    "        scene_gray = self.preprocessor.preprocess(scene_img)\n",
    "        scene_kp, scene_des = self.feature_extractor.detect_and_compute(scene_gray)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Scene: {Path(scene_path).name} - {len(scene_kp)} keypoints\")\n",
    "\n",
    "        detections = []\n",
    "        result_img = scene_img.copy()\n",
    "\n",
    "        # Process each model\n",
    "        for book_id, model_path in enumerate(model_paths):\n",
    "            model_img, model_kp, model_des = self.load_model(model_path)\n",
    "\n",
    "            if model_des is None or len(model_kp) < Config.MIN_MATCH_COUNT:\n",
    "                detections.append(BookDetection(book_id, model_path, []))\n",
    "                continue\n",
    "\n",
    "            # Detect all instances of this book\n",
    "            instances = self._detect_instances(\n",
    "                model_img, model_kp, model_des,\n",
    "                scene_img, scene_kp, scene_des\n",
    "            )\n",
    "\n",
    "            detection = BookDetection(book_id, model_path, instances)\n",
    "            detections.append(detection)\n",
    "\n",
    "            # Draw on result image\n",
    "            self._draw_detection(result_img, detection)\n",
    "\n",
    "            if verbose and len(instances) > 0:\n",
    "                print(f\"  Book {book_id}: {len(instances)} instance(s)\")\n",
    "\n",
    "        return detections, result_img\n",
    "\n",
    "    def _detect_instances(self, model_img, model_kp, model_des,\n",
    "                          scene_img, scene_kp, scene_des) -> List[BoundingBox]:\n",
    "        \"\"\"\n",
    "        Detect all instances of a book using iterative affine estimation.\n",
    "\n",
    "        Strategy:\n",
    "        1. Match features\n",
    "        2. Estimate affine transformation\n",
    "        3. Validate and save detection\n",
    "        4. Mask inlier keypoints\n",
    "        5. Repeat until no more valid detections\n",
    "        \"\"\"\n",
    "        instances = []\n",
    "        excluded_indices: Set[int] = set()\n",
    "\n",
    "        for _ in range(Config.MAX_INSTANCES_PER_BOOK):\n",
    "            # Match with exclusion\n",
    "            matches = self.matcher.match(model_des, scene_des, excluded_indices)\n",
    "\n",
    "            if len(matches) < Config.MIN_MATCH_COUNT:\n",
    "                break\n",
    "\n",
    "            # Extract point coordinates\n",
    "            src_pts = np.float32([model_kp[m.queryIdx].pt for m in matches])\n",
    "            dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in matches])\n",
    "            match_indices = [m.trainIdx for m in matches]\n",
    "\n",
    "            # Estimate affine transformation\n",
    "            M, inlier_mask, n_inliers = self.affine_estimator.estimate(src_pts, dst_pts)\n",
    "\n",
    "            if M is None:\n",
    "                break\n",
    "\n",
    "            # Check inlier quality\n",
    "            inlier_ratio = n_inliers / len(matches)\n",
    "            if n_inliers < Config.MIN_INLIERS or inlier_ratio < Config.MIN_INLIERS_RATIO:\n",
    "                # Mask these points and try again\n",
    "                inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
    "                                  if inlier_mask[i]]\n",
    "                excluded_indices.update(inlier_indices)\n",
    "                continue\n",
    "\n",
    "            # Validate transformation\n",
    "            if not self.affine_estimator.validate(M, model_img.shape[:2], scene_img.shape[:2]):\n",
    "                inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
    "                                  if inlier_mask[i]]\n",
    "                excluded_indices.update(inlier_indices)\n",
    "                continue\n",
    "\n",
    "            # Create bounding box\n",
    "            h, w = model_img.shape[:2]\n",
    "            corners = np.array([[0, 0], [w, 0], [w, h], [0, h]], dtype=np.float32)\n",
    "            projected = self.affine_estimator.transform_points(M, corners)\n",
    "            area = GeometryUtils.polygon_area(projected)\n",
    "\n",
    "            bbox = BoundingBox(\n",
    "                top_left=tuple(map(int, projected[0])),\n",
    "                top_right=tuple(map(int, projected[1])),\n",
    "                bottom_right=tuple(map(int, projected[2])),\n",
    "                bottom_left=tuple(map(int, projected[3])),\n",
    "                area=int(area),\n",
    "                n_inliers=n_inliers,\n",
    "                inlier_ratio=inlier_ratio\n",
    "            )\n",
    "\n",
    "            # Check for duplicates (NMS)\n",
    "            is_duplicate = False\n",
    "            for existing in instances:\n",
    "                if GeometryUtils.polygon_iou(bbox.get_polygon(), existing.get_polygon()) > Config.IOU_THRESHOLD:\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "\n",
    "            if is_duplicate:\n",
    "                inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
    "                                  if inlier_mask[i]]\n",
    "                excluded_indices.update(inlier_indices)\n",
    "                continue\n",
    "\n",
    "            # Valid new instance!\n",
    "            instances.append(bbox)\n",
    "\n",
    "            # Mask inlier keypoints\n",
    "            inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
    "                              if inlier_mask[i]]\n",
    "            excluded_indices.update(inlier_indices)\n",
    "\n",
    "        return instances\n",
    "\n",
    "    def _draw_detection(self, img: np.ndarray, detection: BookDetection):\n",
    "        \"\"\"Draw bounding boxes on image.\"\"\"\n",
    "        colors = [\n",
    "            (0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0),\n",
    "            (255, 0, 255), (0, 255, 255), (128, 0, 255), (255, 128, 0)\n",
    "        ]\n",
    "        color = colors[detection.book_id % len(colors)]\n",
    "\n",
    "        for inst in detection.instances:\n",
    "            pts = np.array([inst.top_left, inst.top_right,\n",
    "                           inst.bottom_right, inst.bottom_left], dtype=np.int32)\n",
    "            cv2.polylines(img, [pts], True, color, 3)\n",
    "\n",
    "            label = f\"Book {detection.book_id}\"\n",
    "            pos = (inst.top_left[0], max(inst.top_left[1] - 10, 20))\n",
    "            cv2.putText(img, label, pos, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXPJLn4b_hcW"
   },
   "source": [
    "## 10. Output Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yClnTb1V_hcW"
   },
   "outputs": [],
   "source": [
    "def format_output(detections: List[BookDetection]) -> str:\n",
    "    \"\"\"\n",
    "    Format detection results as specified in the assignment.\n",
    "\n",
    "    Output format:\n",
    "    Book X - N instance(s) found:\n",
    "      Instance 1 {top_left: (x,y), top_right: (x,y), ...}\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for det in detections:\n",
    "        if len(det.instances) > 0:\n",
    "            lines.append(f\"Book {det.book_id} - {len(det.instances)} instance(s) found:\")\n",
    "\n",
    "            for i, inst in enumerate(det.instances, 1):\n",
    "                lines.append(\n",
    "                    f\"  Instance {i} {{\"\n",
    "                    f\"top_left: {inst.top_left}, \"\n",
    "                    f\"top_right: {inst.top_right}, \"\n",
    "                    f\"bottom_left: {inst.bottom_left}, \"\n",
    "                    f\"bottom_right: {inst.bottom_right}, \"\n",
    "                    f\"area: {inst.area}px}}\"\n",
    "                )\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7R9Mmaw_hcW"
   },
   "source": [
    "## 11. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5nJwekg_hcW"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODELS_PATH = \"/content/drive/MyDrive/Colab Notebooks/IPCV_1/dataset/models\"\n",
    "SCENES_PATH = \"/content/drive/MyDrive/Colab Notebooks/IPCV_1/dataset/scenes\"\n",
    "\n",
    "# Load file lists\n",
    "model_files = sorted(Path(MODELS_PATH).glob(\"model_*.png\"), key=lambda x: int(x.stem.split('_')[1]))\n",
    "scene_files = sorted(Path(SCENES_PATH).glob(\"scene_*.jpg\"), key=lambda x: int(x.stem.split('_')[1]))\n",
    "\n",
    "model_paths = [str(f) for f in model_files]\n",
    "scene_paths = [str(f) for f in scene_files]\n",
    "\n",
    "print(f\"Found {len(model_paths)} models and {len(scene_paths)} scenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LCqnDeh_hcX"
   },
   "source": [
    "## 12. Run Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxp60LU4_hcX"
   },
   "outputs": [],
   "source": [
    "# Initialize detector\n",
    "detector = BookDetector()\n",
    "\n",
    "# Process all scenes\n",
    "all_results = {}\n",
    "all_images = {}\n",
    "\n",
    "for idx, scene_path in enumerate(scene_paths):\n",
    "    print(f\"\\nProcessing scene {idx}: {Path(scene_path).name}\")\n",
    "\n",
    "    detections, result_img = detector.detect_in_scene(scene_path, model_paths, verbose=False)\n",
    "\n",
    "    all_results[idx] = detections\n",
    "    all_images[idx] = result_img\n",
    "\n",
    "    total = sum(len(d.instances) for d in detections)\n",
    "    books = sum(1 for d in detections if len(d.instances) > 0)\n",
    "    print(f\"  Found {total} instances of {books} books\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPe8xovu_hcX"
   },
   "source": [
    "## 13. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0JU6HLN_hcX"
   },
   "outputs": [],
   "source": [
    "# Print formatted results\n",
    "for idx, detections in all_results.items():\n",
    "    detected = [d for d in detections if len(d.instances) > 0]\n",
    "    if detected:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SCENE: {Path(scene_paths[idx]).name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(format_output(detected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uZrTZlL_hcX"
   },
   "source": [
    "## 14. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BT6XPnhp_hcX"
   },
   "outputs": [],
   "source": [
    "# Visualize all results\n",
    "for idx in range(len(scene_paths)):\n",
    "    if idx not in all_images:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(all_images[idx], cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Scene {idx}: {Path(scene_paths[idx]).name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmBtfEP7ngfU"
   },
   "outputs": [],
   "source": [
    "def diagnose_detection(detector, scene_path, model_path, book_id, scene_idx):\n",
    "    \"\"\"\n",
    "    Diagnose the affine detection pipeline for a specific scene/model pair.\n",
    "\n",
    "    Usage:\n",
    "        diagnose_detection(detector, scene_paths[19], model_paths[6], book_id=6, scene_idx=19)\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(f\"DIAGNOSTIC: Scene {scene_idx} / Book {book_id}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 1: Load and preprocess images\n",
    "    # =========================================================================\n",
    "    print(\"\\n[STEP 1] IMAGE LOADING & PREPROCESSING\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    scene_img = cv2.imread(scene_path)\n",
    "    model_img, model_kp, model_des = detector.load_model(model_path)\n",
    "\n",
    "    print(f\"Scene path: {Path(scene_path).name}\")\n",
    "    print(f\"Model path: {Path(model_path).name}\")\n",
    "    print(f\"Scene image shape: {scene_img.shape}\")\n",
    "    print(f\"Model image shape: {model_img.shape}\")\n",
    "\n",
    "    scene_gray = detector.preprocessor.preprocess(scene_img)\n",
    "    model_gray = detector.preprocessor.preprocess(model_img)\n",
    "\n",
    "    print(f\"Scene mean intensity (after CLAHE): {scene_gray.mean():.1f}\")\n",
    "    print(f\"Model mean intensity (after CLAHE): {model_gray.mean():.1f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 2: Feature extraction\n",
    "    # =========================================================================\n",
    "    print(\"\\n[STEP 2] FEATURE EXTRACTION (RootSIFT)\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    scene_kp, scene_des = detector.feature_extractor.detect_and_compute(scene_gray)\n",
    "\n",
    "    print(f\"Model keypoints: {len(model_kp)}\")\n",
    "    print(f\"Scene keypoints: {len(scene_kp)}\")\n",
    "\n",
    "    if model_des is not None:\n",
    "        print(f\"Model descriptor shape: {model_des.shape}\")\n",
    "    if scene_des is not None:\n",
    "        print(f\"Scene descriptor shape: {scene_des.shape}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 3: Feature matching (5NN)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[STEP 3] FEATURE MATCHING (5NN)\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    matches = detector.matcher.match(model_des, scene_des, excluded_indices=None)\n",
    "\n",
    "    print(f\"Matches after 5NN + filtering: {len(matches)}\")\n",
    "    print(f\"Match ratio: {len(matches)/len(model_kp)*100:.1f}% of model keypoints matched\")\n",
    "\n",
    "    if len(matches) < Config.MIN_MATCH_COUNT:\n",
    "        print(f\"⚠️  Not enough matches (need {Config.MIN_MATCH_COUNT}, have {len(matches)})\")\n",
    "        return\n",
    "\n",
    "    # Extract coordinates\n",
    "    src_pts = np.float32([model_kp[m.queryIdx].pt for m in matches])\n",
    "    dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in matches])\n",
    "    match_indices = [m.trainIdx for m in matches]\n",
    "\n",
    "    print(f\"\\nScene match locations:\")\n",
    "    print(f\"  X range: {dst_pts[:,0].min():.0f} - {dst_pts[:,0].max():.0f}\")\n",
    "    print(f\"  Y range: {dst_pts[:,1].min():.0f} - {dst_pts[:,1].max():.0f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 4: Affine estimation (single pass on all matches)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[STEP 4] AFFINE ESTIMATION (all matches)\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    M, inlier_mask, n_inliers = detector.affine_estimator.estimate(src_pts, dst_pts)\n",
    "\n",
    "    if M is None:\n",
    "        print(\"⚠️  Affine estimation failed!\")\n",
    "        return\n",
    "\n",
    "    inlier_ratio = n_inliers / len(matches)\n",
    "    print(f\"Inliers: {n_inliers}/{len(matches)} ({inlier_ratio*100:.1f}%)\")\n",
    "    print(f\"MIN_INLIERS threshold: {Config.MIN_INLIERS}\")\n",
    "    print(f\"MIN_INLIERS_RATIO threshold: {Config.MIN_INLIERS_RATIO*100:.0f}%\")\n",
    "\n",
    "    passes_inlier_count = n_inliers >= Config.MIN_INLIERS\n",
    "    passes_inlier_ratio = inlier_ratio >= Config.MIN_INLIERS_RATIO\n",
    "    print(f\"Passes inlier count: {passes_inlier_count}\")\n",
    "    print(f\"Passes inlier ratio: {passes_inlier_ratio}\")\n",
    "\n",
    "    # Show affine matrix\n",
    "    print(f\"\\nAffine matrix:\")\n",
    "    print(f\"  [{M[0,0]:.3f}  {M[0,1]:.3f}  {M[0,2]:.1f}]\")\n",
    "    print(f\"  [{M[1,0]:.3f}  {M[1,1]:.3f}  {M[1,2]:.1f}]\")\n",
    "\n",
    "    # Decompose affine into components\n",
    "    scale_x = np.sqrt(M[0,0]**2 + M[1,0]**2)\n",
    "    scale_y = np.sqrt(M[0,1]**2 + M[1,1]**2)\n",
    "    rotation = np.arctan2(M[1,0], M[0,0]) * 180 / np.pi\n",
    "    tx, ty = M[0,2], M[1,2]\n",
    "\n",
    "    print(f\"\\nAffine decomposition:\")\n",
    "    print(f\"  Scale X: {scale_x:.3f}\")\n",
    "    print(f\"  Scale Y: {scale_y:.3f}\")\n",
    "    print(f\"  Rotation: {rotation:.1f}°\")\n",
    "    print(f\"  Translation: ({tx:.1f}, {ty:.1f})\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 5: Validation\n",
    "    # =========================================================================\n",
    "    print(\"\\n[STEP 5] VALIDATION\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    is_valid = detector.affine_estimator.validate(M, model_img.shape[:2], scene_img.shape[:2])\n",
    "    print(f\"Validation result: {'✓ VALID' if is_valid else '✗ INVALID'}\")\n",
    "\n",
    "    # Show projected corners\n",
    "    h, w = model_img.shape[:2]\n",
    "    corners = np.array([[0, 0], [w, 0], [w, h], [0, h]], dtype=np.float32)\n",
    "    projected = detector.affine_estimator.transform_points(M, corners)\n",
    "\n",
    "    print(f\"\\nProjected corners:\")\n",
    "    print(f\"  Top-left:     ({projected[0,0]:.0f}, {projected[0,1]:.0f})\")\n",
    "    print(f\"  Top-right:    ({projected[1,0]:.0f}, {projected[1,1]:.0f})\")\n",
    "    print(f\"  Bottom-right: ({projected[2,0]:.0f}, {projected[2,1]:.0f})\")\n",
    "    print(f\"  Bottom-left:  ({projected[3,0]:.0f}, {projected[3,1]:.0f})\")\n",
    "\n",
    "    # Area check\n",
    "    model_area = h * w\n",
    "    detected_area = GeometryUtils.polygon_area(projected)\n",
    "    area_ratio = detected_area / model_area\n",
    "\n",
    "    print(f\"\\nArea analysis:\")\n",
    "    print(f\"  Model area: {model_area} px\")\n",
    "    print(f\"  Detected area: {detected_area:.0f} px\")\n",
    "    print(f\"  Area ratio: {area_ratio:.3f}\")\n",
    "    print(f\"  Valid range: [{Config.MIN_AREA_RATIO}, {Config.MAX_AREA_RATIO}]\")\n",
    "    print(f\"  Min area threshold: {Config.MIN_AREA}\")\n",
    "\n",
    "    # Convexity check\n",
    "    is_convex = GeometryUtils.is_convex(projected)\n",
    "    print(f\"\\nConvexity: {'✓ Convex' if is_convex else '✗ Not convex'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 6: Inlier distribution\n",
    "    # =========================================================================\n",
    "    print(\"\\n[STEP 6] INLIER DISTRIBUTION\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    if inlier_mask is not None:\n",
    "        inlier_dst = dst_pts[inlier_mask.ravel() == 1]\n",
    "        outlier_dst = dst_pts[inlier_mask.ravel() == 0]\n",
    "\n",
    "        print(f\"Inlier locations:\")\n",
    "        print(f\"  X range: {inlier_dst[:,0].min():.0f} - {inlier_dst[:,0].max():.0f}\")\n",
    "        print(f\"  Y range: {inlier_dst[:,1].min():.0f} - {inlier_dst[:,1].max():.0f}\")\n",
    "\n",
    "        if len(outlier_dst) > 0:\n",
    "            print(f\"\\nOutlier locations:\")\n",
    "            print(f\"  X range: {outlier_dst[:,0].min():.0f} - {outlier_dst[:,0].max():.0f}\")\n",
    "            print(f\"  Y range: {outlier_dst[:,1].min():.0f} - {outlier_dst[:,1].max():.0f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 7: Simulate iterative detection\n",
    "    # =========================================================================\n",
    "    print(\"\\n[STEP 7] ITERATIVE DETECTION SIMULATION\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    excluded = set()\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < Config.MAX_INSTANCES_PER_BOOK:\n",
    "        iteration += 1\n",
    "\n",
    "        # Match with exclusion\n",
    "        iter_matches = detector.matcher.match(model_des, scene_des, excluded)\n",
    "\n",
    "        if len(iter_matches) < Config.MIN_MATCH_COUNT:\n",
    "            print(f\"\\nIteration {iteration}: Only {len(iter_matches)} matches remaining - STOP\")\n",
    "            break\n",
    "\n",
    "        iter_src = np.float32([model_kp[m.queryIdx].pt for m in iter_matches])\n",
    "        iter_dst = np.float32([scene_kp[m.trainIdx].pt for m in iter_matches])\n",
    "        iter_indices = [m.trainIdx for m in iter_matches]\n",
    "\n",
    "        M_iter, mask_iter, n_inliers_iter = detector.affine_estimator.estimate(iter_src, iter_dst)\n",
    "\n",
    "        if M_iter is None:\n",
    "            print(f\"\\nIteration {iteration}: Affine estimation failed - STOP\")\n",
    "            break\n",
    "\n",
    "        inlier_ratio_iter = n_inliers_iter / len(iter_matches)\n",
    "        is_valid_iter = detector.affine_estimator.validate(M_iter, model_img.shape[:2], scene_img.shape[:2])\n",
    "\n",
    "        # Get inlier indices\n",
    "        inlier_indices = [iter_indices[i] for i in range(len(iter_indices)) if mask_iter[i]]\n",
    "\n",
    "        status = \"\"\n",
    "        if n_inliers_iter < Config.MIN_INLIERS:\n",
    "            status = \"REJECTED (too few inliers)\"\n",
    "        elif inlier_ratio_iter < Config.MIN_INLIERS_RATIO:\n",
    "            status = \"REJECTED (low inlier ratio)\"\n",
    "        elif not is_valid_iter:\n",
    "            status = \"REJECTED (invalid geometry)\"\n",
    "        else:\n",
    "            # Project corners for this detection\n",
    "            corners_iter = detector.affine_estimator.transform_points(M_iter, corners)\n",
    "            status = f\"✓ VALID at ({corners_iter[0,0]:.0f}, {corners_iter[0,1]:.0f})\"\n",
    "\n",
    "        print(f\"\\nIteration {iteration}:\")\n",
    "        print(f\"  Matches: {len(iter_matches)}\")\n",
    "        print(f\"  Inliers: {n_inliers_iter} ({inlier_ratio_iter*100:.1f}%)\")\n",
    "        print(f\"  Valid geometry: {is_valid_iter}\")\n",
    "        print(f\"  Status: {status}\")\n",
    "\n",
    "        # Mask inliers for next iteration\n",
    "        excluded.update(inlier_indices)\n",
    "\n",
    "        if \"REJECTED\" in status and n_inliers_iter < 3:\n",
    "            print(\"  → No more viable matches - STOP\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"END DIAGNOSTIC\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "def diagnose_quick(detector, scene_paths, model_paths, scene_idx, book_id):\n",
    "    \"\"\"\n",
    "    Quick diagnostic shortcut.\n",
    "\n",
    "    Usage:\n",
    "        diagnose_quick(detector, scene_paths, model_paths, scene_idx=19, book_id=6)\n",
    "    \"\"\"\n",
    "    diagnose_detection(detector, scene_paths[scene_idx], model_paths[book_id], book_id, scene_idx)\n",
    "\n",
    "\n",
    "def diagnose_all_failures(detector, scene_paths, model_paths, expected):\n",
    "    \"\"\"\n",
    "    Run diagnostics on all expected multi-instance cases.\n",
    "\n",
    "    Args:\n",
    "        expected: dict of {scene_idx: {book_id: expected_count}}\n",
    "\n",
    "    Example:\n",
    "        expected = {\n",
    "            10: {19: 4},  # scene_10 should have 4 copies of book_19\n",
    "            19: {6: 3},   # scene_19 should have 3 copies of book_6\n",
    "        }\n",
    "        diagnose_all_failures(detector, scene_paths, model_paths, expected)\n",
    "    \"\"\"\n",
    "    for scene_idx, books in expected.items():\n",
    "        for book_id, expected_count in books.items():\n",
    "            print(f\"\\n{'#'*70}\")\n",
    "            print(f\"Expected {expected_count} instances of Book {book_id} in Scene {scene_idx}\")\n",
    "            print(f\"{'#'*70}\")\n",
    "            diagnose_detection(detector, scene_paths[scene_idx], model_paths[book_id], book_id, scene_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiFubAxvnmQv"
   },
   "outputs": [],
   "source": [
    "diagnose_detection(detector, scene_paths[27], model_paths[2], book_id=2, scene_idx=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-IpwZ4x_hcY"
   },
   "source": [
    "## 15. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGSNVL-4_hcY"
   },
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "total_detections = 0\n",
    "detections_per_scene = []\n",
    "detections_per_book = defaultdict(int)\n",
    "\n",
    "for detections in all_results.values():\n",
    "    scene_total = sum(len(d.instances) for d in detections)\n",
    "    detections_per_scene.append(scene_total)\n",
    "    total_detections += scene_total\n",
    "\n",
    "    for d in detections:\n",
    "        if len(d.instances) > 0:\n",
    "            detections_per_book[d.book_id] += len(d.instances)\n",
    "\n",
    "print(\"DETECTION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total detections: {total_detections}\")\n",
    "print(f\"Average per scene: {np.mean(detections_per_scene):.2f}\")\n",
    "print(f\"Max in one scene: {max(detections_per_scene)}\")\n",
    "print(f\"\\nUnique books detected: {len(detections_per_book)}\")\n",
    "\n",
    "print(\"\\nTop 10 most detected books:\")\n",
    "for book_id, count in sorted(detections_per_book.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  Book {book_id}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueO6Y2q5_hcY"
   },
   "source": [
    "## 16. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "This pipeline implements book detection using traditional computer vision:\n",
    "\n",
    "1. **RootSIFT features** for robust descriptor matching\n",
    "2. **5NN matching** to support multi-instance detection\n",
    "3. **Affine transformation** (not homography) for geometric verification\n",
    "4. **Iterative detection with masking** for finding multiple copies\n",
    "\n",
    "### Design Rationale\n",
    "\n",
    "The key insight is that **simpler is better** for this dataset:\n",
    "\n",
    "- Books are upright → minimal rotation needed\n",
    "- Camera is frontal → minimal perspective distortion\n",
    "- Scale is consistent → no complex scale handling needed\n",
    "\n",
    "Using affine (6 DOF) instead of homography (8 DOF) provides more stable estimation with the limited number of matches per book instance (~6-10 matches).\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- May struggle with heavily occluded books\n",
    "- Performance depends on book cover texture (low-texture covers yield fewer features)\n",
    "- Adjacent identical books with overlapping features can be challenging"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
