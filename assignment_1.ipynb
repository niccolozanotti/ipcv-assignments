{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzfT6d2t_hcE"
      },
      "source": [
        "# **Product Recognition of Books**\n",
        "\n",
        "## Image Processing and Computer Vision - Assignment Module #1\n",
        "\n",
        "---\n",
        "\n",
        "## Approach Overview\n",
        "\n",
        "This solution implements a **traditional computer vision pipeline** for detecting books on shelves.\n",
        "\n",
        "### Key Design Decisions\n",
        "\n",
        "After extensive experimentation, we adopted a **simplified pipeline** based on the following observations about our dataset:\n",
        "\n",
        "1. **Books are nearly planar**: Shelf images have minimal perspective distortion\n",
        "2. **Books are upright or horizontal**: Spines can be vertical or horizontal (stacked shelves)\n",
        "3. **Scale is consistent**: Model and scene images have similar scale\n",
        "4. **Multiple copies are adjacent**: Same book appears side-by-side on shelves\n",
        "\n",
        "These characteristics led us to choose:\n",
        "\n",
        "| Component | Choice | Justification |\n",
        "|-----------|--------|---------------|\n",
        "| Features | RootSIFT | ~15-30% better matching than standard SIFT |\n",
        "| Matching | **Combined 5NN + BF ratio test** | Maximizes match recall for low-texture models |\n",
        "| Geometric model | **Similarity transform** (4 DOF) | Uniform scale + rotation + translation; strongest inductive bias for books |\n",
        "| Multi-instance | Iterative masking + **Orientation-aware template verification** | Handles both textured and low-texture books in any orientation |\n",
        "\n",
        "### Why Similarity Transform (4 DOF) Instead of Full Affine (6 DOF) or Homography (8 DOF)?\n",
        "\n",
        "| Property | Homography | Full Affine | Similarity (Partial Affine) |\n",
        "|----------|------------|-------------|----------------------------|\n",
        "| Degrees of freedom | 8 | 6 | **4** |\n",
        "| Minimum points | 4 | 3 | **2** |\n",
        "| Handles perspective | Yes | No | No |\n",
        "| Preserves aspect ratio | No | No | **Yes** |\n",
        "| Allows shear | Yes | Yes | **No** |\n",
        "| Stability with few points | Lowest | Medium | **Highest** |\n",
        "\n",
        "Books on shelves do not undergo perspective distortion, shear, or non-uniform scaling. The similarity transform (`cv2.estimateAffinePartial2D`) encodes exactly this **inductive bias**: it only allows uniform scaling, rotation, and translation. This makes RANSAC converge faster and more reliably, especially with the limited matches (~8-15) typical of narrow book spines.\n",
        "\n",
        "The resulting 2×3 matrix is converted to a 3×3 homography matrix (by appending `[0, 0, 1]`) for convenient corner projection via `cv2.perspectiveTransform`.\n",
        "\n",
        "### Two-Phase Multi-Instance Detection\n",
        "\n",
        "Some book spines (especially narrow ones with limited texture) yield too few feature matches to detect all copies via iterative RANSAC alone. To address this, we employ a **two-phase** strategy:\n",
        "\n",
        "1. **Phase 1 (Feature-based)**: Standard iterative similarity estimation with keypoint masking\n",
        "2. **Phase 2 (Template-based)**: If Phase 1 found at least one instance, extract the detected scene region as a template and use **Normalized Cross-Correlation (NCC)** to search for additional copies. The search direction is **orientation-aware**: it follows the axis along which copies are expected to be arranged (horizontal for vertical books, vertical for horizontal/stacked books).\n",
        "\n",
        "This combines the geometric robustness of feature matching with the appearance-level sensitivity of template matching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZzVgAL_hcK"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mak8OeZ1_hcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4a6612-8a92-4adf-f7ef-028bbdd3662a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenCV version: 4.13.0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"OpenCV version: {cv2.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r01GuR_3_hcO"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "All parameters are centralized here with justifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wj6NYy2n_hcO"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"\n",
        "    Configuration parameters for the detection pipeline.\n",
        "\n",
        "    Each parameter is justified based on dataset characteristics\n",
        "    and experimental results.\n",
        "    \"\"\"\n",
        "\n",
        "    # === PREPROCESSING ===\n",
        "    # CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "    # Normalizes lighting variations across shelf images\n",
        "    CLAHE_CLIP_LIMIT = 2.0   # Moderate contrast enhancement\n",
        "    CLAHE_GRID_SIZE = (4, 4) # 4x4 provides finer local adaptation than 8x8\n",
        "    CLAHE_FLAG = True  # able or disable\n",
        "    # Justification: Experiments showed fixed CLAHE outperformed adaptive selection\n",
        "\n",
        "    # === FEATURE DETECTION ===\n",
        "    # SIFT parameters (used via RootSIFT)\n",
        "    SIFT_FEATURES = 0                # 0 = detect all features\n",
        "    SIFT_CONTRAST_THRESHOLD = 0.01   # Lower than default 0.04 for more keypoints\n",
        "    SIFT_EDGE_THRESHOLD = 15         # Slightly higher than default 10\n",
        "\n",
        "    # === FEATURE MATCHING ===\n",
        "    # Three complementary matching strategies merged via union:\n",
        "    #\n",
        "    # 1) 5NN via FLANN: allows one model keypoint to match multiple scene\n",
        "    #    locations, essential for multi-instance detection.\n",
        "    # 2) BF 2NN ratio test: recovers matches that 5NN's group filtering misses.\n",
        "    # 3) BF 5NN consecutive ratio test (inspired by Conti & Preda):\n",
        "    #    for each model keypoint's 5 neighbors, keeps match[i] if\n",
        "    #    match[i].distance < ratio * match[i+1].distance. This captures\n",
        "    #    good matches at any rank, not just the best.\n",
        "    DISTANCE_THRESHOLD = 0.8  # 5NN: relative quality threshold\n",
        "    OUTLIER_RATIO = 1.5       # 5NN: filter matches far from the group\n",
        "    BF_RATIO = 0.85           # BF 2NN: Lowe's ratio test threshold\n",
        "    KNN_CONSECUTIVE_RATIO = 0.6  # BF 5NN: consecutive neighbor ratio test\n",
        "                                 # From reference implementation; stricter\n",
        "                                 # threshold compensated by checking all ranks\n",
        "\n",
        "    # === GEOMETRIC VERIFICATION ===\n",
        "    # Similarity transformation (4 DOF) with RANSAC\n",
        "    # Justification: Books are planar, so we inject inductive bias\n",
        "    # using estimateAffinePartial2D (uniform scale + rotation + translation).\n",
        "    # This is more constrained than full affine (6 DOF) or homography (8 DOF),\n",
        "    # providing more stable estimation with limited correspondences.\n",
        "    MIN_MATCH_COUNT = 5            # Minimum matches to attempt estimation\n",
        "    RANSAC_REPROJ_THRESHOLD = 3.5  # Pixels — allows small localization errors\n",
        "\n",
        "    # === DETECTION VALIDATION ===\n",
        "    MIN_INLIERS = 5            # Minimum inliers for valid detection\n",
        "    MIN_INLIERS_RATIO = 0.25   # At least 25% of matches should be inliers\n",
        "    MIN_AREA = 1000            # Minimum bounding box area (pixels)\n",
        "    MAX_AREA_RATIO = 10        # Max ratio: detected_area / model_area\n",
        "    MIN_AREA_RATIO = 0.15      # Min ratio: detected_area / model_area\n",
        "\n",
        "    # === MULTI-INSTANCE DETECTION ===\n",
        "    MAX_INSTANCES_PER_BOOK = 10  # Safety limit\n",
        "    IOU_THRESHOLD = 0.3          # Overlap threshold for duplicate removal\n",
        "\n",
        "    # === TEMPLATE VERIFICATION (Phase 2) ===\n",
        "    # When Phase 1 finds at least one instance, Phase 2 uses the detected\n",
        "    # scene region as a template to search for additional copies via NCC.\n",
        "    TEMPLATE_NCC_THRESHOLD = 0.48  # NCC score threshold (lowered from 0.50\n",
        "                                   # to catch the 4th copy in scene_10)\n",
        "    TEMPLATE_SEARCH_MARGIN = 20    # Pixels of margin around detected region\n",
        "    TEMPLATE_NMS_OVERLAP = 0.5     # Minimum template dimension fraction for NMS\n",
        "    MAX_PHASE2_ADDITIONS = 4       # Maximum new detections from Phase 2 per model\n",
        "    TEMPLATE_RELATIVE_SCORE = 0.6  # Phase 2 score must be >= 60% of best Phase 2 score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YWnhLAG_hcP"
      },
      "source": [
        "## 3. Data Classes\n",
        "\n",
        "Structured representation of detection results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FELmAZp0_hcP"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BoundingBox:\n",
        "    \"\"\"\n",
        "    Represents a detected book instance.\n",
        "\n",
        "    Stores the four corners of the bounding quadrilateral,\n",
        "    which may not be axis-aligned due to the affine transformation.\n",
        "    \"\"\"\n",
        "    top_left: Tuple[int, int]\n",
        "    top_right: Tuple[int, int]\n",
        "    bottom_right: Tuple[int, int]\n",
        "    bottom_left: Tuple[int, int]\n",
        "    area: int\n",
        "    n_inliers: int\n",
        "    inlier_ratio: float\n",
        "\n",
        "    def get_polygon(self) -> np.ndarray:\n",
        "        \"\"\"Return corners as numpy array for geometric operations.\"\"\"\n",
        "        return np.array([self.top_left, self.top_right,\n",
        "                        self.bottom_right, self.bottom_left], dtype=np.float32)\n",
        "\n",
        "@dataclass\n",
        "class BookDetection:\n",
        "    \"\"\"All detections for a single book model in a scene.\"\"\"\n",
        "    book_id: int\n",
        "    model_path: str\n",
        "    instances: List[BoundingBox]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPps9Xt4_hcQ"
      },
      "source": [
        "## 4. RootSIFT Feature Extractor\n",
        "\n",
        "### Why RootSIFT?\n",
        "\n",
        "Standard SIFT descriptors are compared using Euclidean distance. However, SIFT descriptors are histograms of gradient orientations, and the **Hellinger kernel** (Bhattacharyya distance) is more appropriate for histogram comparison.\n",
        "\n",
        "RootSIFT achieves this by:\n",
        "1. L1-normalizing the SIFT descriptor\n",
        "2. Taking the element-wise square root\n",
        "\n",
        "This allows Euclidean distance to implicitly compute Hellinger distance, providing **~15-30% better matching accuracy**.\n",
        "\n",
        "**Reference**: ArandjeloviÄ‡ & Zisserman, \"Three things everyone should know to improve object retrieval\" (CVPR 2012)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Sdy0nb3s_hcR"
      },
      "outputs": [],
      "source": [
        "class RootSIFT:\n",
        "    \"\"\"\n",
        "    RootSIFT feature extractor.\n",
        "\n",
        "    Enhances SIFT descriptors for better matching performance\n",
        "    by applying L1 normalization followed by square root.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sift = cv2.SIFT_create(\n",
        "            nfeatures=Config.SIFT_FEATURES,\n",
        "            contrastThreshold=Config.SIFT_CONTRAST_THRESHOLD,\n",
        "            edgeThreshold=Config.SIFT_EDGE_THRESHOLD\n",
        "        )\n",
        "\n",
        "    def detect_and_compute(self, image: np.ndarray):\n",
        "        \"\"\"\n",
        "        Detect keypoints and compute RootSIFT descriptors.\n",
        "\n",
        "        Args:\n",
        "            image: Grayscale input image\n",
        "\n",
        "        Returns:\n",
        "            keypoints: List of cv2.KeyPoint\n",
        "            descriptors: RootSIFT descriptors (Nx128 float32 array)\n",
        "        \"\"\"\n",
        "        keypoints, descriptors = self.sift.detectAndCompute(image, None)\n",
        "\n",
        "        if descriptors is None or len(descriptors) == 0:\n",
        "            return keypoints, None\n",
        "\n",
        "        # Convert to RootSIFT\n",
        "        eps = 1e-7\n",
        "\n",
        "        # Step 1: L1 normalize\n",
        "        descriptors = descriptors / (np.sum(descriptors, axis=1, keepdims=True) + eps)\n",
        "\n",
        "        # Step 2: Square root (Hellinger kernel)\n",
        "        descriptors = np.sqrt(descriptors)\n",
        "\n",
        "        return keypoints, descriptors.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhSgnmpL_hcS"
      },
      "source": [
        "## 5. Image Preprocessing\n",
        "\n",
        "### Why CLAHE?\n",
        "\n",
        "Shelf images have varying lighting conditions (shadows, reflections, uneven illumination). **CLAHE** (Contrast Limited Adaptive Histogram Equalization) normalizes local contrast while preventing over-amplification of noise.\n",
        "\n",
        "### Parameter Choices\n",
        "\n",
        "- **clipLimit = 2.0**: Moderate enhancement; higher values can amplify noise\n",
        "- **tileGridSize = (4, 4)**: Finer grid provides better local adaptation for book spines\n",
        "\n",
        "We use **fixed parameters** rather than adaptive selection because experiments showed that adaptive approaches (varying CLAHE based on image brightness) performed worse overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_aSWkBNo_hcS"
      },
      "outputs": [],
      "source": [
        "class ImagePreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing pipeline for lighting normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.clahe = cv2.createCLAHE(\n",
        "            clipLimit=Config.CLAHE_CLIP_LIMIT,\n",
        "            tileGridSize=Config.CLAHE_GRID_SIZE\n",
        "        )\n",
        "\n",
        "    def preprocess(self, image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert to grayscale and apply CLAHE.\n",
        "\n",
        "        Args:\n",
        "            image: BGR input image\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed grayscale image\n",
        "        \"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "        if Config.CLAHE_FLAG:\n",
        "            return self.clahe.apply(gray)\n",
        "        else:\n",
        "            return gray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XEy3ne9_hcT"
      },
      "source": [
        "## 6. Feature Matching with Combined Strategy\n",
        "\n",
        "### Three Complementary Matching Approaches\n",
        "\n",
        "We merge matches from three strategies via union (de-duplicated by `(queryIdx, trainIdx)` pair):\n",
        "\n",
        "**Strategy 1: 5NN via FLANN**\n",
        "For each model keypoint, find 5 nearest neighbors in the scene. Keep matches that are significantly better than the worst in their group. This allows one model keypoint to match multiple scene locations — essential for multi-instance detection.\n",
        "\n",
        "**Strategy 2: BF 2NN Ratio Test**\n",
        "Standard Lowe's ratio test (`best / second_best < 0.85`). Recovers matches that 5NN's group-based filtering misses, particularly for low-texture models.\n",
        "\n",
        "**Strategy 3: BF 5NN Consecutive Ratio Test** *(inspired by Conti & Preda)*\n",
        "For each model keypoint's 5 nearest neighbors, compare consecutive distances: keep `match[i]` if `match[i].distance < 0.6 * match[i+1].distance`. Unlike the standard ratio test that only checks rank 1 vs rank 2, this captures good matches at *any* rank position.\n",
        "\n",
        "### Why merge all three?\n",
        "\n",
        "Each strategy captures a different subset of correct matches. The union significantly improves recall — particularly critical for low-texture book spines where individual strategies may yield only 30-40 matches but the union reaches 80+.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B--b2RsJ_hcT"
      },
      "outputs": [],
      "source": [
        "class FeatureMatcher:\n",
        "    \"\"\"\n",
        "    Combined feature matcher merging three complementary strategies.\n",
        "\n",
        "    Strategies:\n",
        "    1. 5NN via FLANN (group-based filtering)\n",
        "    2. BF 2NN Lowe's ratio test\n",
        "    3. BF 5NN consecutive ratio test (Conti & Preda)\n",
        "\n",
        "    Matches are de-duplicated by (queryIdx, trainIdx) pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # FLANN matcher for efficient approximate nearest neighbor search\n",
        "        FLANN_INDEX_KDTREE = 1\n",
        "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "        search_params = dict(checks=100)\n",
        "        self.flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "        # Brute-force matcher for exact nearest neighbor\n",
        "        self.bf = cv2.BFMatcher()\n",
        "\n",
        "    def match(self, des_model: np.ndarray, des_scene: np.ndarray,\n",
        "              excluded_indices: Set[int] = None) -> List[cv2.DMatch]:\n",
        "        \"\"\"\n",
        "        Match model descriptors to scene descriptors using combined strategy.\n",
        "\n",
        "        Args:\n",
        "            des_model: Model descriptors\n",
        "            des_scene: Scene descriptors\n",
        "            excluded_indices: Scene keypoint indices to exclude (already matched)\n",
        "\n",
        "        Returns:\n",
        "            List of good matches (union of all strategies, de-duplicated)\n",
        "        \"\"\"\n",
        "        if des_model is None or des_scene is None:\n",
        "            return []\n",
        "        if len(des_model) < 2 or len(des_scene) < 5:\n",
        "            return []\n",
        "\n",
        "        if excluded_indices is None:\n",
        "            excluded_indices = set()\n",
        "\n",
        "        seen = set()       # Track (queryIdx, trainIdx) to avoid duplicates\n",
        "        combined = []      # Final merged match list\n",
        "\n",
        "        # --- Strategy 1: 5NN via FLANN ---\n",
        "        # Allows one model keypoint to match multiple scene locations\n",
        "        try:\n",
        "            matches = self.flann.knnMatch(des_model, des_scene, k=5)\n",
        "        except cv2.error:\n",
        "            matches = []\n",
        "\n",
        "        for match_group in matches:\n",
        "            valid = [m for m in match_group if m.trainIdx not in excluded_indices]\n",
        "\n",
        "            if len(valid) < 2:\n",
        "                continue\n",
        "\n",
        "            distances = [m.distance for m in valid]\n",
        "            min_dist = distances[0]\n",
        "            median_dist = distances[min(2, len(distances) - 1)]\n",
        "            max_dist = distances[-1]\n",
        "\n",
        "            if median_dist > 0 and min_dist > Config.DISTANCE_THRESHOLD * median_dist:\n",
        "                continue\n",
        "\n",
        "            threshold = max_dist / Config.OUTLIER_RATIO\n",
        "            for m in valid:\n",
        "                if m.distance <= threshold:\n",
        "                    key = (m.queryIdx, m.trainIdx)\n",
        "                    if key not in seen:\n",
        "                        seen.add(key)\n",
        "                        combined.append(m)\n",
        "\n",
        "        # --- Strategy 2: BF 2NN with Lowe's ratio test ---\n",
        "        try:\n",
        "            bf_matches_2nn = self.bf.knnMatch(des_model, des_scene, k=2)\n",
        "        except cv2.error:\n",
        "            bf_matches_2nn = []\n",
        "\n",
        "        for pair in bf_matches_2nn:\n",
        "            if len(pair) < 2:\n",
        "                continue\n",
        "            m, n = pair\n",
        "            if m.trainIdx in excluded_indices:\n",
        "                continue\n",
        "            if m.distance < Config.BF_RATIO * n.distance:\n",
        "                key = (m.queryIdx, m.trainIdx)\n",
        "                if key not in seen:\n",
        "                    seen.add(key)\n",
        "                    combined.append(m)\n",
        "\n",
        "        # --- Strategy 3: BF 5NN consecutive ratio test ---\n",
        "        # Inspired by Conti & Preda: for each model keypoint's k=5 neighbors,\n",
        "        # keep match[i] if its distance is significantly better than match[i+1].\n",
        "        # This captures good matches at any rank, not just rank 1.\n",
        "        try:\n",
        "            bf_matches_5nn = self.bf.knnMatch(des_model, des_scene, k=5)\n",
        "        except cv2.error:\n",
        "            bf_matches_5nn = []\n",
        "\n",
        "        for match_group in bf_matches_5nn:\n",
        "            for i in range(len(match_group) - 1):\n",
        "                m = match_group[i]\n",
        "                m_next = match_group[i + 1]\n",
        "                if m.trainIdx in excluded_indices:\n",
        "                    continue\n",
        "                if m.distance < Config.KNN_CONSECUTIVE_RATIO * m_next.distance:\n",
        "                    key = (m.queryIdx, m.trainIdx)\n",
        "                    if key not in seen:\n",
        "                        seen.add(key)\n",
        "                        combined.append(m)\n",
        "\n",
        "        return combined\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA27HZ93_hcU"
      },
      "source": [
        "## 7. Geometry Utilities\n",
        "\n",
        "Helper functions for geometric validation and IoU computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R_vFoEQH_hcU"
      },
      "outputs": [],
      "source": [
        "class GeometryUtils:\n",
        "    \"\"\"Geometric utility functions.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def polygon_area(points: np.ndarray) -> float:\n",
        "        \"\"\"Compute polygon area using Shoelace formula.\"\"\"\n",
        "        n = len(points)\n",
        "        area = 0.0\n",
        "        for i in range(n):\n",
        "            j = (i + 1) % n\n",
        "            area += points[i][0] * points[j][1]\n",
        "            area -= points[j][0] * points[i][1]\n",
        "        return abs(area) / 2.0\n",
        "\n",
        "    @staticmethod\n",
        "    def is_convex(points: np.ndarray) -> bool:\n",
        "        \"\"\"Check if quadrilateral is convex using cross product signs.\"\"\"\n",
        "        n = len(points)\n",
        "        if n != 4:\n",
        "            return False\n",
        "\n",
        "        sign = None\n",
        "        for i in range(n):\n",
        "            p1, p2, p3 = points[i], points[(i+1)%n], points[(i+2)%n]\n",
        "            cross = (p2[0]-p1[0]) * (p3[1]-p2[1]) - (p2[1]-p1[1]) * (p3[0]-p2[0])\n",
        "\n",
        "            if abs(cross) < 1e-6:\n",
        "                continue\n",
        "            if sign is None:\n",
        "                sign = cross > 0\n",
        "            elif (cross > 0) != sign:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def polygon_iou(poly1: np.ndarray, poly2: np.ndarray) -> float:\n",
        "        \"\"\"Compute Intersection over Union for two polygons.\"\"\"\n",
        "        # Find bounding region\n",
        "        all_pts = np.vstack([poly1, poly2])\n",
        "        x_min, y_min = np.floor(all_pts.min(axis=0)).astype(int) - 10\n",
        "        x_max, y_max = np.ceil(all_pts.max(axis=0)).astype(int) + 10\n",
        "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
        "\n",
        "        w, h = x_max - x_min, y_max - y_min\n",
        "        if w <= 0 or h <= 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Create masks\n",
        "        offset = np.array([x_min, y_min])\n",
        "        mask1 = np.zeros((h, w), dtype=np.uint8)\n",
        "        mask2 = np.zeros((h, w), dtype=np.uint8)\n",
        "        cv2.fillPoly(mask1, [(poly1 - offset).astype(np.int32)], 1)\n",
        "        cv2.fillPoly(mask2, [(poly2 - offset).astype(np.int32)], 1)\n",
        "\n",
        "        intersection = np.sum(mask1 & mask2)\n",
        "        union = np.sum(mask1 | mask2)\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqALpA4a_hcU"
      },
      "source": [
        "## 8. Similarity Transform Estimator\n",
        "\n",
        "### Why Similarity Transform (Partial Affine) Instead of Full Affine or Homography?\n",
        "\n",
        "Following the approach in [Conti & Preda, IPCV Assignment], we use `cv2.estimateAffinePartial2D` which estimates a **similarity transform** (4 DOF):\n",
        "\n",
        "$$\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} s \\cos\\theta & -s \\sin\\theta \\\\ s \\sin\\theta & s \\cos\\theta \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}$$\n",
        "\n",
        "This allows only:\n",
        "- **Uniform scaling** (single scale factor $s$)\n",
        "- **Rotation** (angle $\\theta$)\n",
        "- **Translation** ($t_x$, $t_y$)\n",
        "\n",
        "It does **not** allow shear or non-uniform scaling.\n",
        "\n",
        "### Why This Is Better for Books\n",
        "\n",
        "- **Books preserve aspect ratio**: A book spine is always the same width-to-height ratio regardless of viewing angle\n",
        "- **No shear in shelf images**: Books sit upright or lie flat, without trapezoidal distortion\n",
        "- **Fewer parameters = more robust**: With only 4 unknowns (vs 6 for full affine, 8 for homography), RANSAC converges faster and is less likely to overfit to noise when we have limited matches (~8-15 per book instance)\n",
        "- **Handles arbitrary rotation**: Unlike our previous validation that assumed upright books, the similarity transform naturally handles both vertical (0°) and horizontal (90°) book orientations\n",
        "\n",
        "### Homography Conversion\n",
        "\n",
        "The 2×3 affine matrix is converted to a 3×3 homography matrix by appending `[0, 0, 1]` as the third row. This allows using `cv2.perspectiveTransform` for corner projection, which is a standard OpenCV convention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JPmtYqfg_hcU"
      },
      "outputs": [],
      "source": [
        "class AffineEstimator:\n",
        "    \"\"\"\n",
        "    Similarity transform estimator using RANSAC.\n",
        "\n",
        "    Uses cv2.estimateAffinePartial2D (4 DOF: uniform scale + rotation +\n",
        "    translation) instead of full affine or homography. This provides the\n",
        "    strongest inductive bias for book detection: books do not undergo\n",
        "    shear or non-uniform scaling.\n",
        "\n",
        "    The 2x3 matrix is converted to 3x3 homography format for convenient\n",
        "    corner projection via cv2.perspectiveTransform.\n",
        "    \"\"\"\n",
        "\n",
        "    def estimate(self, src_pts: np.ndarray, dst_pts: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], int]:\n",
        "        \"\"\"\n",
        "        Estimate similarity transformation using RANSAC.\n",
        "\n",
        "        Args:\n",
        "            src_pts: Source points (model) - Nx2 array\n",
        "            dst_pts: Destination points (scene) - Nx2 array\n",
        "\n",
        "        Returns:\n",
        "            (homography_matrix, inlier_mask, n_inliers)\n",
        "            homography_matrix is 3x3 (similarity embedded in homography form)\n",
        "        \"\"\"\n",
        "        if len(src_pts) < 2:\n",
        "            return None, None, 0\n",
        "\n",
        "        try:\n",
        "            # estimateAffinePartial2D: 4 DOF similarity transform\n",
        "            # (uniform scale + rotation + translation)\n",
        "            M, inliers = cv2.estimateAffinePartial2D(\n",
        "                src_pts.reshape(-1, 1, 2),\n",
        "                dst_pts.reshape(-1, 1, 2),\n",
        "                method=cv2.RANSAC,\n",
        "                ransacReprojThreshold=Config.RANSAC_REPROJ_THRESHOLD,\n",
        "                maxIters=2000,\n",
        "                confidence=0.99\n",
        "            )\n",
        "\n",
        "            if M is None or inliers is None:\n",
        "                return None, None, 0\n",
        "\n",
        "            # Convert 2x3 affine to 3x3 homography matrix\n",
        "            H = np.vstack([M, [0, 0, 1]])\n",
        "\n",
        "            n_inliers = int(np.sum(inliers))\n",
        "            return H, inliers, n_inliers\n",
        "\n",
        "        except cv2.error:\n",
        "            return None, None, 0\n",
        "\n",
        "    def transform_corners(self, H: np.ndarray, model_shape: Tuple[int, int]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Project model corners to scene using the homography matrix.\n",
        "\n",
        "        Args:\n",
        "            H: 3x3 homography matrix\n",
        "            model_shape: (height, width) of model image\n",
        "\n",
        "        Returns:\n",
        "            Projected corners as 4x2 array: [TL, TR, BR, BL]\n",
        "        \"\"\"\n",
        "        h, w = model_shape\n",
        "        corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
        "        projected = cv2.perspectiveTransform(corners, H)\n",
        "        return projected.reshape(-1, 2)\n",
        "\n",
        "    def get_rotation_deg(self, H: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Extract rotation angle (degrees) from the similarity transform.\n",
        "\n",
        "        For a similarity matrix:\n",
        "            [s*cos(θ)  -s*sin(θ)  tx]\n",
        "            [s*sin(θ)   s*cos(θ)  ty]\n",
        "            [0          0          1]\n",
        "        \"\"\"\n",
        "        return np.arctan2(H[1, 0], H[0, 0]) * 180 / np.pi\n",
        "\n",
        "    def get_scale(self, H: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Extract uniform scale factor from the similarity transform.\n",
        "        \"\"\"\n",
        "        return np.sqrt(H[0, 0]**2 + H[1, 0]**2)\n",
        "\n",
        "    def validate(self, H: np.ndarray, model_shape: Tuple[int, int],\n",
        "                 scene_shape: Tuple[int, int]) -> bool:\n",
        "        \"\"\"\n",
        "        Validate if the similarity transform produces a reasonable bounding box.\n",
        "\n",
        "        Checks:\n",
        "        1. Result is convex quadrilateral\n",
        "        2. Projected corners are within scene bounds (with margin)\n",
        "        3. Area is within reasonable bounds\n",
        "        4. Scale factor is reasonable\n",
        "\n",
        "        Note: We do NOT check corner orientation (top_left must be above\n",
        "        bottom_left, etc.) because books can be horizontal in stacked shelves.\n",
        "        The similarity transform naturally handles arbitrary rotation.\n",
        "        \"\"\"\n",
        "        if H is None:\n",
        "            return False\n",
        "\n",
        "        # Project corners\n",
        "        projected = self.transform_corners(H, model_shape)\n",
        "\n",
        "        # === CHECK 1: Convexity ===\n",
        "        if not GeometryUtils.is_convex(projected):\n",
        "            return False\n",
        "\n",
        "        # === CHECK 2: Within scene bounds ===\n",
        "        scene_h, scene_w = scene_shape\n",
        "        margin = max(scene_h, scene_w) * 0.2\n",
        "        if (np.any(projected < -margin) or\n",
        "            np.any(projected[:, 0] > scene_w + margin) or\n",
        "            np.any(projected[:, 1] > scene_h + margin)):\n",
        "            return False\n",
        "\n",
        "        # === CHECK 3: Area bounds ===\n",
        "        h, w = model_shape\n",
        "        model_area = h * w\n",
        "        detected_area = GeometryUtils.polygon_area(projected)\n",
        "\n",
        "        if detected_area < Config.MIN_AREA:\n",
        "            return False\n",
        "\n",
        "        ratio = detected_area / model_area\n",
        "        if ratio < Config.MIN_AREA_RATIO or ratio > Config.MAX_AREA_RATIO:\n",
        "            return False\n",
        "\n",
        "        # === CHECK 4: Reasonable scale ===\n",
        "        # For similarity transform, scale is uniform, so we just check\n",
        "        # it's within a reasonable range\n",
        "        scale = self.get_scale(H)\n",
        "        if scale < 0.1 or scale > 5.0:\n",
        "            return False\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d21FqDVN_hcV"
      },
      "source": [
        "## 9. Book Detector\n",
        "\n",
        "### Multi-Instance Detection: Two-Phase Approach\n",
        "\n",
        "#### Phase 1: Iterative Feature-Based Detection with Keypoint Exclusion\n",
        "\n",
        "1. Match all model keypoints to scene\n",
        "2. Estimate similarity transformation (RANSAC)\n",
        "3. If valid: save detection, add inlier scene keypoint indices to exclusion set\n",
        "4. Re-match with excluded indices, repeat until no valid detections remain\n",
        "\n",
        "This approach is fast (features extracted once) and allows multiple RANSAC\n",
        "attempts even when match quality degrades, unlike scene masking which stops\n",
        "after the first failure.\n",
        "\n",
        "#### Phase 2: Orientation-Aware Template Search (NCC)\n",
        "\n",
        "For books with limited texture, Phase 1 may find only one instance.\n",
        "Phase 2 uses the first detection as a template and searches the **full scene**:\n",
        "- Detects **orientation** (vertical vs horizontal) from the similarity transform rotation\n",
        "- Applies NMS along the **correct axis** (X for side-by-side, Y for stacked)\n",
        "\n",
        "### Why Similarity Transform?\n",
        "\n",
        "We use `cv2.estimateAffinePartial2D` (4 DOF: uniform scale + rotation + translation)\n",
        "rather than full affine (6 DOF) or homography (8 DOF). Books don't undergo shear\n",
        "or non-uniform scaling, so this provides the strongest inductive bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IdhPuzsS_hcV"
      },
      "outputs": [],
      "source": [
        "class BookDetector:\n",
        "    \"\"\"\n",
        "    Main book detection pipeline.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Preprocess images (CLAHE)\n",
        "    2. Extract RootSIFT features\n",
        "    3. Match features (combined 5NN + BF + consecutive ratio)\n",
        "    4. Phase 1: Iterative detection with keypoint exclusion\n",
        "    5. Phase 2: Orientation-aware template search (NCC)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.preprocessor = ImagePreprocessor()\n",
        "        self.feature_extractor = RootSIFT()\n",
        "        self.matcher = FeatureMatcher()\n",
        "        self.affine_estimator = AffineEstimator()\n",
        "        self.model_cache = {}  # Cache model features\n",
        "\n",
        "    def load_model(self, model_path: str):\n",
        "        \"\"\"Load and cache model image features.\"\"\"\n",
        "        if model_path in self.model_cache:\n",
        "            return self.model_cache[model_path]\n",
        "\n",
        "        img = cv2.imread(model_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Could not load: {model_path}\")\n",
        "\n",
        "        gray = self.preprocessor.preprocess(img)\n",
        "        kp, des = self.feature_extractor.detect_and_compute(gray)\n",
        "\n",
        "        self.model_cache[model_path] = (img, kp, des)\n",
        "        return img, kp, des\n",
        "\n",
        "    def detect_in_scene(self, scene_path: str, model_paths: List[str],\n",
        "                        verbose: bool = False) -> Tuple[List[BookDetection], np.ndarray]:\n",
        "        \"\"\"\n",
        "        Detect all books in a scene image.\n",
        "        \"\"\"\n",
        "        scene_img = cv2.imread(scene_path)\n",
        "        if scene_img is None:\n",
        "            raise ValueError(f\"Could not load: {scene_path}\")\n",
        "\n",
        "        scene_gray = self.preprocessor.preprocess(scene_img)\n",
        "        scene_kp, scene_des = self.feature_extractor.detect_and_compute(scene_gray)\n",
        "        scene_gray_raw = cv2.cvtColor(scene_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Scene: {Path(scene_path).name} - {len(scene_kp)} keypoints\")\n",
        "\n",
        "        detections = []\n",
        "        result_img = scene_img.copy()\n",
        "\n",
        "        for book_id, model_path in enumerate(model_paths):\n",
        "            model_img, model_kp, model_des = self.load_model(model_path)\n",
        "\n",
        "            if model_des is None or len(model_kp) < Config.MIN_MATCH_COUNT:\n",
        "                detections.append(BookDetection(book_id, model_path, []))\n",
        "                continue\n",
        "\n",
        "            instances = self._detect_instances(\n",
        "                model_img, model_kp, model_des,\n",
        "                scene_img, scene_kp, scene_des,\n",
        "                scene_gray_raw\n",
        "            )\n",
        "\n",
        "            detection = BookDetection(book_id, model_path, instances)\n",
        "            detections.append(detection)\n",
        "            self._draw_detection(result_img, detection)\n",
        "\n",
        "            if verbose and len(instances) > 0:\n",
        "                print(f\"  Book {book_id}: {len(instances)} instance(s)\")\n",
        "\n",
        "        return detections, result_img\n",
        "\n",
        "    def _detect_instances(self, model_img, model_kp, model_des,\n",
        "                          scene_img, scene_kp, scene_des,\n",
        "                          scene_gray_raw) -> List[BoundingBox]:\n",
        "        \"\"\"\n",
        "        Detect all instances using two-phase approach.\n",
        "\n",
        "        Phase 1: Iterative similarity estimation + keypoint exclusion.\n",
        "        Phase 2: Orientation-aware NCC template search.\n",
        "        \"\"\"\n",
        "        instances = []\n",
        "        instance_transforms = []\n",
        "        excluded_indices: Set[int] = set()\n",
        "\n",
        "        for _ in range(Config.MAX_INSTANCES_PER_BOOK):\n",
        "            # Match with exclusion of previously used keypoints\n",
        "            matches = self.matcher.match(model_des, scene_des, excluded_indices)\n",
        "\n",
        "            if len(matches) < Config.MIN_MATCH_COUNT:\n",
        "                break\n",
        "\n",
        "            src_pts = np.float32([model_kp[m.queryIdx].pt for m in matches])\n",
        "            dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in matches])\n",
        "            match_indices = [m.trainIdx for m in matches]\n",
        "\n",
        "            # Estimate similarity transformation\n",
        "            H, inlier_mask, n_inliers = self.affine_estimator.estimate(src_pts, dst_pts)\n",
        "\n",
        "            if H is None:\n",
        "                break\n",
        "\n",
        "            inlier_ratio = n_inliers / len(matches)\n",
        "            if n_inliers < Config.MIN_INLIERS or inlier_ratio < Config.MIN_INLIERS_RATIO:\n",
        "                # Exclude these inliers and try again\n",
        "                inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
        "                                  if inlier_mask[i]]\n",
        "                excluded_indices.update(inlier_indices)\n",
        "                continue\n",
        "\n",
        "            if not self.affine_estimator.validate(H, model_img.shape[:2], scene_img.shape[:2]):\n",
        "                inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
        "                                  if inlier_mask[i]]\n",
        "                excluded_indices.update(inlier_indices)\n",
        "                continue\n",
        "\n",
        "            # Create bounding box\n",
        "            projected = self.affine_estimator.transform_corners(H, model_img.shape[:2])\n",
        "            area = GeometryUtils.polygon_area(projected)\n",
        "\n",
        "            bbox = BoundingBox(\n",
        "                top_left=tuple(map(int, projected[0])),\n",
        "                top_right=tuple(map(int, projected[1])),\n",
        "                bottom_right=tuple(map(int, projected[2])),\n",
        "                bottom_left=tuple(map(int, projected[3])),\n",
        "                area=int(area),\n",
        "                n_inliers=n_inliers,\n",
        "                inlier_ratio=inlier_ratio\n",
        "            )\n",
        "\n",
        "            # Check for duplicates (NMS)\n",
        "            is_duplicate = any(\n",
        "                GeometryUtils.polygon_iou(bbox.get_polygon(), ex.get_polygon())\n",
        "                > Config.IOU_THRESHOLD\n",
        "                for ex in instances\n",
        "            )\n",
        "\n",
        "            if is_duplicate:\n",
        "                inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
        "                                  if inlier_mask[i]]\n",
        "                excluded_indices.update(inlier_indices)\n",
        "                continue\n",
        "\n",
        "            # Valid new instance!\n",
        "            instances.append(bbox)\n",
        "            instance_transforms.append(H)\n",
        "\n",
        "            # Exclude inlier keypoints for next iteration\n",
        "            inlier_indices = [match_indices[i] for i in range(len(match_indices))\n",
        "                              if inlier_mask[i]]\n",
        "            excluded_indices.update(inlier_indices)\n",
        "\n",
        "        # === PHASE 2: Orientation-aware template search ===\n",
        "        if len(instances) >= 1 and len(instance_transforms) >= 1:\n",
        "            instances = self._template_search(\n",
        "                instances, instance_transforms[0],\n",
        "                scene_img, scene_gray_raw\n",
        "            )\n",
        "\n",
        "        return instances\n",
        "\n",
        "    def _template_search(self, instances: List[BoundingBox],\n",
        "                         H: np.ndarray,\n",
        "                         scene_img: np.ndarray,\n",
        "                         scene_gray_raw: np.ndarray) -> List[BoundingBox]:\n",
        "        \"\"\"\n",
        "        Phase 2: Orientation-aware template search using NCC.\n",
        "\n",
        "        Uses the same similarity transform from Phase 1. For each NCC peak, we\n",
        "        compute the translation offset from the reference detection and apply\n",
        "        the same rotation/scale to the model corners.\n",
        "\n",
        "        This ensures Phase 2 boxes have the same quality as Phase 1 boxes.\n",
        "        \"\"\"\n",
        "        ref = instances[0]\n",
        "        ref_poly = ref.get_polygon()\n",
        "\n",
        "        rotation_deg = self.affine_estimator.get_rotation_deg(H)\n",
        "        is_horizontal = 45 < abs(rotation_deg) < 135\n",
        "\n",
        "        scene_h, scene_w = scene_img.shape[:2]\n",
        "        x_min = max(0, int(np.min(ref_poly[:, 0])))\n",
        "        x_max = min(scene_w, int(np.max(ref_poly[:, 0])))\n",
        "        y_min = max(0, int(np.min(ref_poly[:, 1])))\n",
        "        y_max = min(scene_h, int(np.max(ref_poly[:, 1])))\n",
        "\n",
        "        template = scene_gray_raw[y_min:y_max, x_min:x_max]\n",
        "        t_h, t_w = template.shape[:2]\n",
        "\n",
        "        if t_w < 5 or t_h < 5:\n",
        "            return instances\n",
        "\n",
        "        if scene_gray_raw.shape[0] <= t_h or scene_gray_raw.shape[1] <= t_w:\n",
        "            return instances\n",
        "\n",
        "        result = cv2.matchTemplate(\n",
        "            scene_gray_raw, template, cv2.TM_CCOEFF_NORMED\n",
        "        )\n",
        "\n",
        "        # NMS distance along the separation axis\n",
        "        if is_horizontal:\n",
        "            nms_min_dist = t_h * Config.TEMPLATE_NMS_OVERLAP\n",
        "        else:\n",
        "            nms_min_dist = t_w * Config.TEMPLATE_NMS_OVERLAP\n",
        "\n",
        "        # Find local maxima above threshold\n",
        "        peaks = []\n",
        "        for x in range(result.shape[1]):\n",
        "            for y in range(result.shape[0]):\n",
        "                if result[y, x] >= Config.TEMPLATE_NCC_THRESHOLD:\n",
        "                    is_max = True\n",
        "                    for dx in range(-5, 6):\n",
        "                        for dy in range(-3, 4):\n",
        "                            nx, ny = x + dx, y + dy\n",
        "                            if (0 <= nx < result.shape[1] and\n",
        "                                0 <= ny < result.shape[0]):\n",
        "                                if result[ny, nx] > result[y, x]:\n",
        "                                    is_max = False\n",
        "                                    break\n",
        "                        if not is_max:\n",
        "                            break\n",
        "                    if is_max:\n",
        "                        peaks.append((x, y, result[y, x]))\n",
        "\n",
        "        # NMS along correct axis\n",
        "        peaks.sort(key=lambda p: -p[2])\n",
        "        kept_peaks = []\n",
        "        for px, py, score in peaks:\n",
        "            is_dup = False\n",
        "            for kx, ky, _ in kept_peaks:\n",
        "                if is_horizontal:\n",
        "                    if abs(py - ky) < nms_min_dist:\n",
        "                        is_dup = True\n",
        "                        break\n",
        "                else:\n",
        "                    if abs(px - kx) < nms_min_dist:\n",
        "                        is_dup = True\n",
        "                        break\n",
        "            if not is_dup:\n",
        "                kept_peaks.append((px, py, score))\n",
        "\n",
        "        # Reference detection's axis-aligned bounding box center\n",
        "        ref_center_x = (x_min + x_max) / 2.0\n",
        "        ref_center_y = (y_min + y_max) / 2.0\n",
        "\n",
        "        # Reference detection's ROTATED polygon center\n",
        "        ref_poly_center = np.mean(ref_poly, axis=0)\n",
        "\n",
        "        # Convert peaks to ROTATED bounding boxes by shifting the\n",
        "        # Phase 1 polygon. Each NCC peak gives us the top-left corner\n",
        "        # of the axis-aligned bounding box of the match. We compute\n",
        "        # the offset from the reference's axis-aligned box and apply\n",
        "        # it to the rotated polygon.\n",
        "        for px, py, score in kept_peaks:\n",
        "            peak_center_x = px + t_w / 2.0\n",
        "            peak_center_y = py + t_h / 2.0\n",
        "\n",
        "            # Offset from reference (in axis-aligned coords)\n",
        "            dx = peak_center_x - ref_center_x\n",
        "            dy = peak_center_y - ref_center_y\n",
        "\n",
        "            # Shift the rotated polygon by the same offset\n",
        "            shifted_poly = ref_poly + np.array([dx, dy])\n",
        "\n",
        "            area = GeometryUtils.polygon_area(shifted_poly)\n",
        "            bbox = BoundingBox(\n",
        "                top_left=tuple(map(int, shifted_poly[0])),\n",
        "                top_right=tuple(map(int, shifted_poly[1])),\n",
        "                bottom_right=tuple(map(int, shifted_poly[2])),\n",
        "                bottom_left=tuple(map(int, shifted_poly[3])),\n",
        "                area=int(area),\n",
        "                n_inliers=0,\n",
        "                inlier_ratio=score\n",
        "            )\n",
        "\n",
        "            # Reject if overlaps with existing detection\n",
        "            is_duplicate = any(\n",
        "                GeometryUtils.polygon_iou(\n",
        "                    bbox.get_polygon(), ex.get_polygon()\n",
        "                ) > Config.IOU_THRESHOLD\n",
        "                for ex in instances\n",
        "            )\n",
        "            if not is_duplicate:\n",
        "                instances.append(bbox)\n",
        "\n",
        "        return instances\n",
        "\n",
        "    def _draw_detection(self, img: np.ndarray, detection: BookDetection):\n",
        "        \"\"\"Draw bounding boxes on image.\"\"\"\n",
        "        colors = [\n",
        "            (0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0),\n",
        "            (255, 0, 255), (0, 255, 255), (128, 0, 255), (255, 128, 0)\n",
        "        ]\n",
        "        color = colors[detection.book_id % len(colors)]\n",
        "\n",
        "        for inst in detection.instances:\n",
        "            pts = np.array([inst.top_left, inst.top_right,\n",
        "                           inst.bottom_right, inst.bottom_left], dtype=np.int32)\n",
        "            cv2.polylines(img, [pts], True, color, 3)\n",
        "\n",
        "            label = f\"Book {detection.book_id}\"\n",
        "            pos = (inst.top_left[0], max(inst.top_left[1] - 10, 20))\n",
        "            cv2.putText(img, label, pos, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXPJLn4b_hcW"
      },
      "source": [
        "## 10. Output Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yClnTb1V_hcW"
      },
      "outputs": [],
      "source": [
        "def format_output(detections: List[BookDetection]) -> str:\n",
        "    \"\"\"\n",
        "    Format detection results as specified in the assignment.\n",
        "\n",
        "    Output format:\n",
        "    Book X - N instance(s) found:\n",
        "      Instance 1 {top_left: (x,y), top_right: (x,y), ...}\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "\n",
        "    for det in detections:\n",
        "        if len(det.instances) > 0:\n",
        "            lines.append(f\"Book {det.book_id} - {len(det.instances)} instance(s) found:\")\n",
        "\n",
        "            for i, inst in enumerate(det.instances, 1):\n",
        "                lines.append(\n",
        "                    f\"  Instance {i} {{\"\n",
        "                    f\"top_left: {inst.top_left}, \"\n",
        "                    f\"top_right: {inst.top_right}, \"\n",
        "                    f\"bottom_left: {inst.bottom_left}, \"\n",
        "                    f\"bottom_right: {inst.bottom_right}, \"\n",
        "                    f\"area: {inst.area}px}}\"\n",
        "                )\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7R9Mmaw_hcW"
      },
      "source": [
        "## 11. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "Y5nJwekg_hcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cff885e-1586-4221-d030-b5fb8fb27fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading repository at commit a455e8a...\n",
            "Extracting dataset folder...\n",
            "Dataset successfully downloaded and ready to use!\n",
            "Found 22 models and 29 scenes\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace this with the exact full 40-character commit hash you want to pin\n",
        "COMMIT_HASH = \"a455e8a87eb21bd2c649a20eb22e3273b025d185\"\n",
        "\n",
        "# GitHub automatically formats commit download URLs and folder names like this:\n",
        "repo_zip_url = f\"https://github.com/niccolozanotti/ipcv-assignments/archive/{COMMIT_HASH}.zip\"\n",
        "zip_path = \"temp_repo.zip\"\n",
        "extract_dir = f\"ipcv-assignments-{COMMIT_HASH}\"\n",
        "\n",
        "# 1. Download the specific commit zip\n",
        "print(f\"Downloading repository at commit {COMMIT_HASH[:7]}...\")\n",
        "urllib.request.urlretrieve(repo_zip_url, zip_path)\n",
        "\n",
        "# 2. Extract ONLY the dataset folder\n",
        "print(\"Extracting dataset folder...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    for file in zip_ref.namelist():\n",
        "        # Look specifically for the dataset folder inside the unzipped directory\n",
        "        if file.startswith(f\"{extract_dir}/dataset/\"):\n",
        "            zip_ref.extract(file, path=\".\")\n",
        "\n",
        "# 3. Move the dataset folder to the current directory and clean up\n",
        "if os.path.exists(\"dataset\"):\n",
        "    shutil.rmtree(\"dataset\") # Clean up if you are re-running the cell\n",
        "\n",
        "shutil.move(f\"{extract_dir}/dataset\", \"dataset\")\n",
        "shutil.rmtree(extract_dir)\n",
        "os.remove(zip_path)\n",
        "\n",
        "print(\"Dataset successfully downloaded and ready to use!\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Updated paths pointing to the locally downloaded folder\n",
        "MODELS_PATH = \"dataset/models\"\n",
        "SCENES_PATH = \"dataset/scenes\"\n",
        "\n",
        "# Load file lists\n",
        "# The sorting logic stays exactly the same to ensure model_10 comes after model_9\n",
        "model_files = sorted(Path(MODELS_PATH).glob(\"model_*.png\"), key=lambda x: int(x.stem.split('_')[1]))\n",
        "scene_files = sorted(Path(SCENES_PATH).glob(\"scene_*.jpg\"), key=lambda x: int(x.stem.split('_')[1]))\n",
        "\n",
        "model_paths = [str(f) for f in model_files]\n",
        "scene_paths = [str(f) for f in scene_files]\n",
        "\n",
        "print(f\"Found {len(model_paths)} models and {len(scene_paths)} scenes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LCqnDeh_hcX"
      },
      "source": [
        "## 12. Run Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qxp60LU4_hcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25529dd5-ae04-482b-85e4-2ca96034e21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing scene 0: scene_0.jpg\n",
            "  Found 0 instances of 0 books\n",
            "\n",
            "Processing scene 1: scene_1.jpg\n",
            "  Found 2 instances of 1 books\n",
            "\n",
            "Processing scene 2: scene_2.jpg\n",
            "  Found 2 instances of 1 books\n",
            "\n",
            "Processing scene 3: scene_3.jpg\n",
            "  Found 11 instances of 1 books\n",
            "\n",
            "Processing scene 4: scene_4.jpg\n",
            "  Found 4 instances of 2 books\n",
            "\n",
            "Processing scene 5: scene_5.jpg\n",
            "  Found 1 instances of 1 books\n",
            "\n",
            "Processing scene 6: scene_6.jpg\n",
            "  Found 1 instances of 1 books\n",
            "\n",
            "Processing scene 7: scene_7.jpg\n",
            "  Found 2 instances of 1 books\n",
            "\n",
            "Processing scene 8: scene_8.jpg\n",
            "  Found 0 instances of 0 books\n",
            "\n",
            "Processing scene 9: scene_9.jpg\n",
            "  Found 4 instances of 1 books\n",
            "\n",
            "Processing scene 10: scene_10.jpg\n"
          ]
        }
      ],
      "source": [
        "# Initialize detector\n",
        "detector = BookDetector()\n",
        "\n",
        "# Process all scenes\n",
        "all_results = {}\n",
        "all_images = {}\n",
        "\n",
        "for idx, scene_path in enumerate(scene_paths):\n",
        "    print(f\"\\nProcessing scene {idx}: {Path(scene_path).name}\")\n",
        "\n",
        "    detections, result_img = detector.detect_in_scene(scene_path, model_paths, verbose=False)\n",
        "\n",
        "    all_results[idx] = detections\n",
        "    all_images[idx] = result_img\n",
        "\n",
        "    total = sum(len(d.instances) for d in detections)\n",
        "    books = sum(1 for d in detections if len(d.instances) > 0)\n",
        "    print(f\"  Found {total} instances of {books} books\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROCESSING COMPLETE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPe8xovu_hcX"
      },
      "source": [
        "## 13. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d0JU6HLN_hcX"
      },
      "outputs": [],
      "source": [
        "# Print formatted results\n",
        "for idx, detections in all_results.items():\n",
        "    detected = [d for d in detections if len(d.instances) > 0]\n",
        "    if detected:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"SCENE: {Path(scene_paths[idx]).name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(format_output(detected))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uZrTZlL_hcX"
      },
      "source": [
        "## 14. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BT6XPnhp_hcX"
      },
      "outputs": [],
      "source": [
        "# Visualize all results\n",
        "for idx in range(len(scene_paths)):\n",
        "    if idx not in all_images:\n",
        "        continue\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(cv2.cvtColor(all_images[idx], cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Scene {idx}: {Path(scene_paths[idx]).name}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-FCtZPI0lG3"
      },
      "source": [
        "## Full Pipeline Diagnostic\n",
        "\n",
        "The function below traces **every intermediate step** when detecting a specific book in a specific scene.\n",
        "It reports:\n",
        "- Preprocessing and feature extraction statistics\n",
        "- Match counts from both 5NN and BF strategies (with overlap analysis)\n",
        "- Each RANSAC iteration: affine matrix, scale/rotation, all 7 validation checks\n",
        "- Phase 2 template matching: NCC scores, peaks, NMS, duplicate rejection\n",
        "- Final detection count and visualization\n",
        "\n",
        "Usage:\n",
        "```python\n",
        "diagnose_full(detector, scene_paths, model_paths, scene_idx=10, book_id=19)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-IpwZ4x_hcY"
      },
      "source": [
        "## 15. Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGSNVL-4_hcY"
      },
      "outputs": [],
      "source": [
        "# Compute statistics\n",
        "total_detections = 0\n",
        "detections_per_scene = []\n",
        "detections_per_book = defaultdict(int)\n",
        "\n",
        "for detections in all_results.values():\n",
        "    scene_total = sum(len(d.instances) for d in detections)\n",
        "    detections_per_scene.append(scene_total)\n",
        "    total_detections += scene_total\n",
        "\n",
        "    for d in detections:\n",
        "        if len(d.instances) > 0:\n",
        "            detections_per_book[d.book_id] += len(d.instances)\n",
        "\n",
        "print(\"DETECTION STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total detections: {total_detections}\")\n",
        "print(f\"Average per scene: {np.mean(detections_per_scene):.2f}\")\n",
        "print(f\"Max in one scene: {max(detections_per_scene)}\")\n",
        "print(f\"\\nUnique books detected: {len(detections_per_book)}\")\n",
        "\n",
        "print(\"\\nTop 10 most detected books:\")\n",
        "for book_id, count in sorted(detections_per_book.items(), key=lambda x: -x[1])[:10]:\n",
        "    print(f\"  Book {book_id}: {count} instances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueO6Y2q5_hcY"
      },
      "source": [
        "## 16. Conclusion\n",
        "\n",
        "### Summary\n",
        "\n",
        "This pipeline implements book detection using traditional computer vision:\n",
        "\n",
        "1. **RootSIFT features** for robust descriptor matching\n",
        "2. **Three-strategy matching** (5NN + BF ratio + consecutive ratio) for maximum recall\n",
        "3. **Similarity transform** (4 DOF via `estimateAffinePartial2D`) for geometric verification\n",
        "4. **Iterative keypoint exclusion** for multi-instance detection\n",
        "5. **Orientation-aware template search** (NCC) for recovering copies with limited features\n",
        "\n",
        "### Key Design Decisions\n",
        "\n",
        "| Decision | Rationale |\n",
        "|----------|-----------|\n",
        "| Similarity (4 DOF) over homography (8 DOF) | Books don't undergo perspective distortion; stronger inductive bias |\n",
        "| Keypoint exclusion over scene masking | Allows multiple RANSAC attempts even with degraded match quality |\n",
        "| Full-scene template search | Handles both vertical and horizontal book arrangements |\n",
        "| NMS along orientation axis | Prevents collapsing stacked/adjacent copies |\n",
        "| Three matching strategies | Each captures different correct matches; union improves recall |\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- May struggle with heavily occluded books\n",
        "- Performance depends on book cover texture (low-texture covers yield fewer features)\n",
        "- Phase 2 template matching produces axis-aligned boxes (not rotated to match book orientation)\n",
        "- FLANN non-determinism can cause slight variation between runs\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}